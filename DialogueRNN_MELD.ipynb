{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DialogueRNN_MELD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPypT3do0fAx9BfAeQyi2M6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anitayadav3/EmotionRecognitionInConversation/blob/master/DialogueRNN_MELD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3dlTvio9hhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b3afcde-0da4-40ed-c8ab-8964c13baf74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UsgFGQc9wkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "063e79d6-aff8-4247-a3f9-2a1952a68ac9"
      },
      "source": [
        "%cd /content/drive/My Drive/DialogueRNN"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DialogueRNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YE0NEuV-DK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "8a111d27-5944-49a0-9a61-9da9109f52ac"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2AApaY92XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f15c225-556d-4a98-8a5a-98a3577d5b2b"
      },
      "source": [
        "!python train_MELD.py --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on GPU\n",
            "Running on the multimodal features........\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch 1 train_loss 1.5887 train_acc 46.23 train_fscore 29.67 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4979 test_acc 48.12 test_fscore 31.27 time 17.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4812    1.0000    0.6498    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.0000    0.0000    0.0000     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.0000    0.0000    0.0000     345.0\n",
            "\n",
            "    accuracy                         0.4812    2610.0\n",
            "   macro avg     0.0687    0.1429    0.0928    2610.0\n",
            "weighted avg     0.2316    0.4812    0.3127    2610.0\n",
            "\n",
            "epoch 2 train_loss 1.4193 train_acc 49.26 train_fscore 38.02 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3829 test_acc 54.56 test_fscore 47.33 time 17.58\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6650    0.8710    0.7542    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.3258    0.4652    0.3832     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3657    0.4145    0.3886     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.1938    0.2501    0.2180    2610.0\n",
            "weighted avg     0.4186    0.5456    0.4733    2610.0\n",
            "\n",
            "epoch 3 train_loss 1.2165 train_acc 56.55 train_fscore 49.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.417 test_acc 49.39 test_fscore 45.23 time 17.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7294    0.7126    0.7209    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.3126    0.4254    0.3604     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.2667    0.6464    0.3776     345.0\n",
            "\n",
            "    accuracy                         0.4939    2610.0\n",
            "   macro avg     0.1870    0.2549    0.2084    2610.0\n",
            "weighted avg     0.4344    0.4939    0.4523    2610.0\n",
            "\n",
            "epoch 4 train_loss 1.1759 train_acc 58.68 train_fscore 52.42 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3218 test_acc 57.55 test_fscore 50.85 time 17.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6783    0.8949    0.7717    1256.0\n",
            "           1     0.2727    0.0427    0.0738     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.0288    0.0531     208.0\n",
            "           4     0.3883    0.5622    0.4593     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4337    0.3884    0.4098     345.0\n",
            "\n",
            "    accuracy                         0.5755    2610.0\n",
            "   macro avg     0.3009    0.2739    0.2525    2610.0\n",
            "weighted avg     0.4995    0.5755    0.5085    2610.0\n",
            "\n",
            "epoch 5 train_loss 1.1381 train_acc 60.54 train_fscore 55.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3129 test_acc 56.51 test_fscore 50.61 time 17.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7028    0.8455    0.7676    1256.0\n",
            "           1     0.3529    0.0214    0.0403     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3704    0.0481    0.0851     208.0\n",
            "           4     0.3691    0.5995    0.4569     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3881    0.4522    0.4177     345.0\n",
            "\n",
            "    accuracy                         0.5651    2610.0\n",
            "   macro avg     0.3119    0.2809    0.2525    2610.0\n",
            "weighted avg     0.5139    0.5651    0.5061    2610.0\n",
            "\n",
            "epoch 6 train_loss 1.1122 train_acc 61.69 train_fscore 57.89 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2907 test_acc 57.51 test_fscore 53.65 time 17.29\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6965    0.8424    0.7625    1256.0\n",
            "           1     0.3176    0.2633    0.2879     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2632    0.0721    0.1132     208.0\n",
            "           4     0.4332    0.5647    0.4903     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4585    0.3681    0.4084     345.0\n",
            "\n",
            "    accuracy                         0.5751    2610.0\n",
            "   macro avg     0.3099    0.3015    0.2946    2610.0\n",
            "weighted avg     0.5177    0.5751    0.5365    2610.0\n",
            "\n",
            "epoch 7 train_loss 1.0806 train_acc 63.58 train_fscore 60.12 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2651 test_acc 58.58 test_fscore 55.71 time 17.22\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7025    0.8272    0.7598    1256.0\n",
            "           1     0.5031    0.2847    0.3636     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2119    0.1538    0.1783     208.0\n",
            "           4     0.4930    0.5249    0.5084     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4249    0.4841    0.4526     345.0\n",
            "\n",
            "    accuracy                         0.5858    2610.0\n",
            "   macro avg     0.3336    0.3250    0.3232    2610.0\n",
            "weighted avg     0.5412    0.5858    0.5571    2610.0\n",
            "\n",
            "epoch 8 train_loss 1.0255 train_acc 66.47 train_fscore 63.55 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2496 test_acc 59.27 test_fscore 55.82 time 17.38\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7119    0.8264    0.7649    1256.0\n",
            "           1     0.3737    0.6263    0.4681     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3611    0.0625    0.1066     208.0\n",
            "           4     0.5543    0.4950    0.5230     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4231    0.3507    0.3835     345.0\n",
            "\n",
            "    accuracy                         0.5927    2610.0\n",
            "   macro avg     0.3463    0.3373    0.3209    2610.0\n",
            "weighted avg     0.5529    0.5927    0.5582    2610.0\n",
            "\n",
            "epoch 9 train_loss 0.9949 train_acc 67.63 train_fscore 64.47 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2327 test_acc 57.89 test_fscore 55.85 time 17.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7295    0.7667    0.7477    1256.0\n",
            "           1     0.4685    0.4769    0.4727     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2621    0.1298    0.1736     208.0\n",
            "           4     0.4485    0.5746    0.5038     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4041    0.4522    0.4268     345.0\n",
            "\n",
            "    accuracy                         0.5789    2610.0\n",
            "   macro avg     0.3304    0.3429    0.3321    2610.0\n",
            "weighted avg     0.5449    0.5789    0.5585    2610.0\n",
            "\n",
            "epoch 10 train_loss 0.98 train_acc 68.18 train_fscore 65.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.248 test_acc 58.43 test_fscore 55.74 time 17.73\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7317    0.7643    0.7477    1256.0\n",
            "           1     0.4457    0.5409    0.4887     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.5652    0.0625    0.1126     208.0\n",
            "           4     0.4544    0.5821    0.5104     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3962    0.4812    0.4346     345.0\n",
            "\n",
            "    accuracy                         0.5843    2610.0\n",
            "   macro avg     0.3705    0.3473    0.3277    2610.0\n",
            "weighted avg     0.5675    0.5843    0.5574    2610.0\n",
            "\n",
            "epoch 11 train_loss 0.9743 train_acc 68.44 train_fscore 65.77 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2093 test_acc 59.66 test_fscore 57.18 time 17.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7149    0.8065    0.7579    1256.0\n",
            "           1     0.4512    0.5267    0.4860     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3182    0.1683    0.2201     208.0\n",
            "           4     0.5141    0.5448    0.5290     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4316    0.4116    0.4214     345.0\n",
            "\n",
            "    accuracy                         0.5966    2610.0\n",
            "   macro avg     0.3471    0.3511    0.3449    2610.0\n",
            "weighted avg     0.5542    0.5966    0.5718    2610.0\n",
            "\n",
            "epoch 12 train_loss 0.9594 train_acc 68.86 train_fscore 66.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2161 test_acc 60.0 test_fscore 56.68 time 17.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7114    0.8225    0.7629    1256.0\n",
            "           1     0.4698    0.4698    0.4698     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4286    0.0721    0.1235     208.0\n",
            "           4     0.5034    0.5547    0.5278     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4085    0.4725    0.4382     345.0\n",
            "\n",
            "    accuracy                         0.6000    2610.0\n",
            "   macro avg     0.3602    0.3416    0.3317    2610.0\n",
            "weighted avg     0.5586    0.6000    0.5668    2610.0\n",
            "\n",
            "epoch 13 train_loss 0.956 train_acc 68.55 train_fscore 66.06 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2245 test_acc 58.01 test_fscore 56.39 time 17.28\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7401    0.7436    0.7419    1256.0\n",
            "           1     0.4429    0.5516    0.4913     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3148    0.1635    0.2152     208.0\n",
            "           4     0.4808    0.5597    0.5172     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3934    0.4812    0.4329     345.0\n",
            "\n",
            "    accuracy                         0.5801    2610.0\n",
            "   macro avg     0.3388    0.3571    0.3426    2610.0\n",
            "weighted avg     0.5550    0.5801    0.5639    2610.0\n",
            "\n",
            "epoch 14 train_loss 0.9472 train_acc 69.17 train_fscore 66.77 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2242 test_acc 59.62 test_fscore 56.54 time 17.54\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7045    0.8352    0.7643    1256.0\n",
            "           1     0.4448    0.4875    0.4652     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3467    0.1250    0.1837     208.0\n",
            "           4     0.5880    0.3905    0.4694     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3970    0.5420    0.4583     345.0\n",
            "\n",
            "    accuracy                         0.5962    2610.0\n",
            "   macro avg     0.3544    0.3400    0.3344    2610.0\n",
            "weighted avg     0.5576    0.5962    0.5654    2610.0\n",
            "\n",
            "epoch 15 train_loss 0.9443 train_acc 69.32 train_fscore 66.83 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2218 test_acc 59.46 test_fscore 56.64 time 17.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7110    0.8169    0.7603    1256.0\n",
            "           1     0.4634    0.4733    0.4683     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4000    0.1154    0.1791     208.0\n",
            "           4     0.5393    0.4776    0.5066     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3815    0.5130    0.4376     345.0\n",
            "\n",
            "    accuracy                         0.5946    2610.0\n",
            "   macro avg     0.3565    0.3423    0.3360    2610.0\n",
            "weighted avg     0.5574    0.5946    0.5664    2610.0\n",
            "\n",
            "epoch 16 train_loss 0.9306 train_acc 69.71 train_fscore 67.26 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2173 test_acc 59.39 test_fscore 57.45 time 17.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7253    0.7946    0.7584    1256.0\n",
            "           1     0.4537    0.5053    0.4781     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2706    0.2212    0.2434     208.0\n",
            "           4     0.5140    0.5473    0.5301     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4458    0.4174    0.4311     345.0\n",
            "\n",
            "    accuracy                         0.5939    2610.0\n",
            "   macro avg     0.3442    0.3551    0.3487    2610.0\n",
            "weighted avg     0.5575    0.5939    0.5745    2610.0\n",
            "\n",
            "epoch 17 train_loss 0.93 train_acc 69.19 train_fscore 66.83 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2258 test_acc 57.74 test_fscore 56.82 time 17.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7428    0.7357    0.7392    1256.0\n",
            "           1     0.4676    0.4875    0.4774     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2523    0.2692    0.2605     208.0\n",
            "           4     0.5180    0.5373    0.5275     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4009    0.5043    0.4467     345.0\n",
            "\n",
            "    accuracy                         0.5774    2610.0\n",
            "   macro avg     0.3402    0.3620    0.3502    2610.0\n",
            "weighted avg     0.5607    0.5774    0.5682    2610.0\n",
            "\n",
            "epoch 18 train_loss 0.9305 train_acc 69.32 train_fscore 66.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2336 test_acc 59.0 test_fscore 56.63 time 17.49\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7275    0.7906    0.7577    1256.0\n",
            "           1     0.4608    0.5231    0.4900     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3934    0.1154    0.1784     208.0\n",
            "           4     0.5343    0.4652    0.4973     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3670    0.5478    0.4395     345.0\n",
            "\n",
            "    accuracy                         0.5900    2610.0\n",
            "   macro avg     0.3547    0.3489    0.3376    2610.0\n",
            "weighted avg     0.5618    0.5900    0.5663    2610.0\n",
            "\n",
            "epoch 19 train_loss 0.9239 train_acc 69.58 train_fscore 67.24 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2202 test_acc 59.23 test_fscore 56.72 time 17.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7260    0.7994    0.7609    1256.0\n",
            "           1     0.4417    0.5801    0.5015     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4194    0.1250    0.1926     208.0\n",
            "           4     0.5699    0.4055    0.4738     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3725    0.5507    0.4444     345.0\n",
            "\n",
            "    accuracy                         0.5923    2610.0\n",
            "   macro avg     0.3614    0.3515    0.3390    2610.0\n",
            "weighted avg     0.5674    0.5923    0.5672    2610.0\n",
            "\n",
            "epoch 20 train_loss 0.9234 train_acc 69.65 train_fscore 67.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2079 test_acc 58.66 test_fscore 57.26 time 17.37\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7443    0.7556    0.7499    1256.0\n",
            "           1     0.4673    0.5089    0.4872     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2938    0.2500    0.2701     208.0\n",
            "           4     0.4713    0.5920    0.5248     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4294    0.4319    0.4306     345.0\n",
            "\n",
            "    accuracy                         0.5866    2610.0\n",
            "   macro avg     0.3437    0.3626    0.3518    2610.0\n",
            "weighted avg     0.5613    0.5866    0.5726    2610.0\n",
            "\n",
            "epoch 21 train_loss 0.9127 train_acc 70.01 train_fscore 67.6 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2068 test_acc 58.58 test_fscore 57.24 time 17.68\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7398    0.7540    0.7468    1256.0\n",
            "           1     0.4581    0.5445    0.4976     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2890    0.2404    0.2625     208.0\n",
            "           4     0.5085    0.5224    0.5153     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4122    0.4899    0.4477     345.0\n",
            "\n",
            "    accuracy                         0.5858    2610.0\n",
            "   macro avg     0.3439    0.3644    0.3528    2610.0\n",
            "weighted avg     0.5612    0.5858    0.5724    2610.0\n",
            "\n",
            "epoch 22 train_loss 0.9127 train_acc 69.9 train_fscore 67.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2146 test_acc 58.97 test_fscore 57.23 time 17.36\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7319    0.7739    0.7523    1256.0\n",
            "           1     0.4386    0.5338    0.4815     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3500    0.2019    0.2561     208.0\n",
            "           4     0.5391    0.4975    0.5175     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3898    0.5072    0.4408     345.0\n",
            "\n",
            "    accuracy                         0.5897    2610.0\n",
            "   macro avg     0.3499    0.3592    0.3497    2610.0\n",
            "weighted avg     0.5619    0.5897    0.5723    2610.0\n",
            "\n",
            "epoch 23 train_loss 0.9096 train_acc 70.08 train_fscore 67.85 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2302 test_acc 57.05 test_fscore 55.87 time 17.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7439    0.7309    0.7373    1256.0\n",
            "           1     0.4960    0.4413    0.4670     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3254    0.1971    0.2455     208.0\n",
            "           4     0.4831    0.4975    0.4902     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3515    0.5971    0.4425     345.0\n",
            "\n",
            "    accuracy                         0.5705    2610.0\n",
            "   macro avg     0.3428    0.3520    0.3404    2610.0\n",
            "weighted avg     0.5582    0.5705    0.5587    2610.0\n",
            "\n",
            "epoch 24 train_loss 0.904 train_acc 70.19 train_fscore 67.89 valid_loss nan valid_acc nan val_fscore nan test_loss 1.222 test_acc 59.04 test_fscore 57.23 time 17.6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7273    0.7922    0.7584    1256.0\n",
            "           1     0.5158    0.4057    0.4542     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3226    0.2404    0.2755     208.0\n",
            "           4     0.5559    0.4453    0.4945     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3732    0.5884    0.4567     345.0\n",
            "\n",
            "    accuracy                         0.5904    2610.0\n",
            "   macro avg     0.3564    0.3531    0.3485    2610.0\n",
            "weighted avg     0.5662    0.5904    0.5723    2610.0\n",
            "\n",
            "epoch 25 train_loss 0.9024 train_acc 70.17 train_fscore 67.9 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2263 test_acc 59.5 test_fscore 56.89 time 17.6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7164    0.8025    0.7570    1256.0\n",
            "           1     0.4551    0.4875    0.4708     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3684    0.1346    0.1972     208.0\n",
            "           4     0.4956    0.5547    0.5235     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4176    0.4551    0.4355     345.0\n",
            "\n",
            "    accuracy                         0.5950    2610.0\n",
            "   macro avg     0.3504    0.3478    0.3406    2610.0\n",
            "weighted avg     0.5546    0.5950    0.5689    2610.0\n",
            "\n",
            "epoch 26 train_loss 0.8938 train_acc 70.93 train_fscore 68.51 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2349 test_acc 57.89 test_fscore 56.23 time 17.54\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7280    0.7691    0.7480    1256.0\n",
            "           1     0.5166    0.3879    0.4431     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3385    0.2115    0.2604     208.0\n",
            "           4     0.5108    0.4701    0.4896     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3549    0.5884    0.4427     345.0\n",
            "\n",
            "    accuracy                         0.5789    2610.0\n",
            "   macro avg     0.3498    0.3467    0.3405    2610.0\n",
            "weighted avg     0.5585    0.5789    0.5623    2610.0\n",
            "\n",
            "epoch 27 train_loss 0.8945 train_acc 70.59 train_fscore 68.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.228 test_acc 59.54 test_fscore 56.7 time 17.63\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7177    0.8057    0.7592    1256.0\n",
            "           1     0.4859    0.4306    0.4566     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4200    0.1010    0.1628     208.0\n",
            "           4     0.5012    0.5299    0.5151     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3929    0.5420    0.4555     345.0\n",
            "\n",
            "    accuracy                         0.5954    2610.0\n",
            "   macro avg     0.3597    0.3442    0.3356    2610.0\n",
            "weighted avg     0.5603    0.5954    0.5670    2610.0\n",
            "\n",
            "epoch 28 train_loss 0.8901 train_acc 70.33 train_fscore 68.07 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2344 test_acc 57.59 test_fscore 55.89 time 17.69\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7322    0.7532    0.7425    1256.0\n",
            "           1     0.4618    0.4733    0.4675     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3636    0.1538    0.2162     208.0\n",
            "           4     0.5242    0.4577    0.4887     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3519    0.6029    0.4444     345.0\n",
            "\n",
            "    accuracy                         0.5759    2610.0\n",
            "   macro avg     0.3477    0.3487    0.3371    2610.0\n",
            "weighted avg     0.5583    0.5759    0.5589    2610.0\n",
            "\n",
            "epoch 29 train_loss 0.8863 train_acc 70.5 train_fscore 68.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2575 test_acc 58.66 test_fscore 56.67 time 17.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7275    0.7803    0.7530    1256.0\n",
            "           1     0.4621    0.4555    0.4588     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3636    0.1731    0.2345     208.0\n",
            "           4     0.5193    0.4677    0.4921     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3790    0.5768    0.4575     345.0\n",
            "\n",
            "    accuracy                         0.5866    2610.0\n",
            "   macro avg     0.3502    0.3505    0.3423    2610.0\n",
            "weighted avg     0.5589    0.5866    0.5667    2610.0\n",
            "\n",
            "epoch 30 train_loss 0.8857 train_acc 70.32 train_fscore 68.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2313 test_acc 58.85 test_fscore 56.68 time 17.68\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7259    0.7779    0.7510    1256.0\n",
            "           1     0.4455    0.5231    0.4812     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3896    0.1442    0.2105     208.0\n",
            "           4     0.5000    0.5274    0.5133     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3926    0.4928    0.4370     345.0\n",
            "\n",
            "    accuracy                         0.5885    2610.0\n",
            "   macro avg     0.3505    0.3522    0.3419    2610.0\n",
            "weighted avg     0.5572    0.5885    0.5668    2610.0\n",
            "\n",
            "epoch 31 train_loss 0.8753 train_acc 70.84 train_fscore 68.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2237 test_acc 59.16 test_fscore 56.75 time 17.63\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7197    0.7994    0.7575    1256.0\n",
            "           1     0.4355    0.5409    0.4825     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3750    0.1442    0.2083     208.0\n",
            "           4     0.5267    0.4900    0.5077     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3908    0.4667    0.4254     345.0\n",
            "\n",
            "    accuracy                         0.5916    2610.0\n",
            "   macro avg     0.3497    0.3487    0.3402    2610.0\n",
            "weighted avg     0.5559    0.5916    0.5675    2610.0\n",
            "\n",
            "epoch 32 train_loss 0.8794 train_acc 71.1 train_fscore 68.86 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2333 test_acc 59.46 test_fscore 57.22 time 17.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7264    0.7906    0.7571    1256.0\n",
            "           1     0.4567    0.5445    0.4968     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3780    0.1490    0.2138     208.0\n",
            "           4     0.5313    0.4851    0.5072     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3922    0.5217    0.4478     345.0\n",
            "\n",
            "    accuracy                         0.5946    2610.0\n",
            "   macro avg     0.3550    0.3558    0.3461    2610.0\n",
            "weighted avg     0.5625    0.5946    0.5722    2610.0\n",
            "\n",
            "epoch 33 train_loss 0.8653 train_acc 71.38 train_fscore 69.13 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2351 test_acc 58.01 test_fscore 56.84 time 17.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7401    0.7460    0.7431    1256.0\n",
            "           1     0.4476    0.5623    0.4984     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2827    0.2596    0.2707     208.0\n",
            "           4     0.5375    0.4453    0.4871     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3983    0.5391    0.4581     345.0\n",
            "\n",
            "    accuracy                         0.5801    2610.0\n",
            "   macro avg     0.3438    0.3646    0.3511    2610.0\n",
            "weighted avg     0.5623    0.5801    0.5684    2610.0\n",
            "\n",
            "epoch 34 train_loss 0.8685 train_acc 71.12 train_fscore 68.9 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2516 test_acc 58.89 test_fscore 56.47 time 17.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7264    0.7842    0.7542    1256.0\n",
            "           1     0.4146    0.6050    0.4920     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4151    0.1058    0.1686     208.0\n",
            "           4     0.5342    0.5050    0.5192     402.0\n",
            "           5     1.0000    0.0147    0.0290      68.0\n",
            "           6     0.3805    0.4522    0.4132     345.0\n",
            "\n",
            "    accuracy                         0.5889    2610.0\n",
            "   macro avg     0.4958    0.3524    0.3395    2610.0\n",
            "weighted avg     0.5859    0.5889    0.5647    2610.0\n",
            "\n",
            "epoch 35 train_loss 0.8637 train_acc 71.23 train_fscore 69.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2568 test_acc 57.78 test_fscore 56.55 time 17.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7279    0.7667    0.7468    1256.0\n",
            "           1     0.4852    0.4662    0.4755     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2479    0.2837    0.2646     208.0\n",
            "           4     0.5432    0.4378    0.4848     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3934    0.5188    0.4475     345.0\n",
            "\n",
            "    accuracy                         0.5778    2610.0\n",
            "   macro avg     0.3425    0.3533    0.3456    2610.0\n",
            "weighted avg     0.5579    0.5778    0.5655    2610.0\n",
            "\n",
            "epoch 36 train_loss 0.852 train_acc 71.27 train_fscore 69.09 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2501 test_acc 57.7 test_fscore 55.96 time 17.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7297    0.7588    0.7440    1256.0\n",
            "           1     0.4837    0.4235    0.4516     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3571    0.1683    0.2288     208.0\n",
            "           4     0.5312    0.4453    0.4844     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3531    0.6377    0.4545     345.0\n",
            "\n",
            "    accuracy                         0.5770    2610.0\n",
            "   macro avg     0.3507    0.3476    0.3376    2610.0\n",
            "weighted avg     0.5602    0.5770    0.5596    2610.0\n",
            "\n",
            "epoch 37 train_loss 0.8522 train_acc 71.66 train_fscore 69.46 valid_loss nan valid_acc nan val_fscore nan test_loss 1.228 test_acc 57.85 test_fscore 56.27 time 17.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7340    0.7492    0.7415    1256.0\n",
            "           1     0.4271    0.5730    0.4894     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3700    0.1779    0.2403     208.0\n",
            "           4     0.5335    0.4552    0.4913     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3708    0.5449    0.4413     345.0\n",
            "\n",
            "    accuracy                         0.5785    2610.0\n",
            "   macro avg     0.3479    0.3572    0.3434    2610.0\n",
            "weighted avg     0.5599    0.5785    0.5627    2610.0\n",
            "\n",
            "epoch 38 train_loss 0.8541 train_acc 71.36 train_fscore 69.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2822 test_acc 58.2 test_fscore 55.31 time 17.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7148    0.7922    0.7515    1256.0\n",
            "           1     0.4715    0.4128    0.4402     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4516    0.0673    0.1172     208.0\n",
            "           4     0.5322    0.4527    0.4892     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3539    0.6145    0.4492     345.0\n",
            "\n",
            "    accuracy                         0.5820    2610.0\n",
            "   macro avg     0.3606    0.3342    0.3210    2610.0\n",
            "weighted avg     0.5595    0.5820    0.5531    2610.0\n",
            "\n",
            "epoch 39 train_loss 0.8464 train_acc 71.87 train_fscore 69.6 valid_loss nan valid_acc nan val_fscore nan test_loss 1.253 test_acc 58.08 test_fscore 56.59 time 17.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7269    0.7651    0.7455    1256.0\n",
            "           1     0.4454    0.5374    0.4871     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2983    0.2596    0.2776     208.0\n",
            "           4     0.5051    0.4975    0.5013     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4043    0.4348    0.4190     345.0\n",
            "\n",
            "    accuracy                         0.5808    2610.0\n",
            "   macro avg     0.3400    0.3563    0.3472    2610.0\n",
            "weighted avg     0.5528    0.5808    0.5659    2610.0\n",
            "\n",
            "epoch 40 train_loss 0.8531 train_acc 71.34 train_fscore 69.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2887 test_acc 58.74 test_fscore 55.74 time 17.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7140    0.7970    0.7532    1256.0\n",
            "           1     0.4467    0.5516    0.4936     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4688    0.0721    0.1250     208.0\n",
            "           4     0.5507    0.4055    0.4670     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3734    0.5768    0.4533     345.0\n",
            "\n",
            "    accuracy                         0.5874    2610.0\n",
            "   macro avg     0.3648    0.3433    0.3275    2610.0\n",
            "weighted avg     0.5632    0.5874    0.5574    2610.0\n",
            "\n",
            "epoch 41 train_loss 0.8384 train_acc 72.06 train_fscore 69.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.293 test_acc 58.93 test_fscore 56.24 time 17.72\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7175    0.8049    0.7587    1256.0\n",
            "           1     0.4591    0.5196    0.4875     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3594    0.1106    0.1691     208.0\n",
            "           4     0.5485    0.4080    0.4679     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3731    0.5623    0.4486     345.0\n",
            "\n",
            "    accuracy                         0.5893    2610.0\n",
            "   macro avg     0.3511    0.3436    0.3331    2610.0\n",
            "weighted avg     0.5572    0.5893    0.5624    2610.0\n",
            "\n",
            "epoch 42 train_loss 0.8331 train_acc 72.05 train_fscore 69.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2665 test_acc 58.54 test_fscore 56.66 time 17.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7258    0.7755    0.7498    1256.0\n",
            "           1     0.5153    0.3594    0.4235     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3413    0.2067    0.2575     208.0\n",
            "           4     0.5166    0.5025    0.5095     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3755    0.6029    0.4627     345.0\n",
            "\n",
            "    accuracy                         0.5854    2610.0\n",
            "   macro avg     0.3535    0.3496    0.3433    2610.0\n",
            "weighted avg     0.5611    0.5854    0.5666    2610.0\n",
            "\n",
            "epoch 43 train_loss 0.8336 train_acc 72.11 train_fscore 70.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2751 test_acc 58.28 test_fscore 56.23 time 17.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7289    0.7643    0.7462    1256.0\n",
            "           1     0.4531    0.4982    0.4746     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4464    0.1202    0.1894     208.0\n",
            "           4     0.5364    0.4577    0.4940     402.0\n",
            "           5     0.3333    0.0147    0.0282      68.0\n",
            "           6     0.3625    0.6116    0.4552     345.0\n",
            "\n",
            "    accuracy                         0.5828    2610.0\n",
            "   macro avg     0.4087    0.3524    0.3411    2610.0\n",
            "weighted avg     0.5744    0.5828    0.5623    2610.0\n",
            "\n",
            "epoch 44 train_loss 0.8151 train_acc 72.68 train_fscore 70.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3193 test_acc 57.2 test_fscore 55.74 time 17.62\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7336    0.7476    0.7405    1256.0\n",
            "           1     0.4871    0.4021    0.4405     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3361    0.1923    0.2446     208.0\n",
            "           4     0.5531    0.4279    0.4825     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3433    0.6638    0.4526     345.0\n",
            "\n",
            "    accuracy                         0.5720    2610.0\n",
            "   macro avg     0.3505    0.3477    0.3373    2610.0\n",
            "weighted avg     0.5628    0.5720    0.5574    2610.0\n",
            "\n",
            "epoch 45 train_loss 0.8201 train_acc 72.52 train_fscore 70.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2679 test_acc 58.31 test_fscore 56.62 time 17.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7318    0.7667    0.7488    1256.0\n",
            "           1     0.4613    0.4448    0.4529     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3451    0.1875    0.2430     208.0\n",
            "           4     0.5328    0.4851    0.5078     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3683    0.5797    0.4505     345.0\n",
            "\n",
            "    accuracy                         0.5831    2610.0\n",
            "   macro avg     0.3485    0.3520    0.3433    2610.0\n",
            "weighted avg     0.5601    0.5831    0.5662    2610.0\n",
            "\n",
            "epoch 46 train_loss 0.8115 train_acc 72.97 train_fscore 70.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.297 test_acc 56.51 test_fscore 55.19 time 17.54\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7303    0.7436    0.7369    1256.0\n",
            "           1     0.4272    0.4591    0.4425     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3134    0.2019    0.2456     208.0\n",
            "           4     0.5849    0.3856    0.4648     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3435    0.6232    0.4428     345.0\n",
            "\n",
            "    accuracy                         0.5651    2610.0\n",
            "   macro avg     0.3427    0.3448    0.3332    2610.0\n",
            "weighted avg     0.5579    0.5651    0.5519    2610.0\n",
            "\n",
            "epoch 47 train_loss 0.811 train_acc 72.41 train_fscore 70.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2963 test_acc 56.97 test_fscore 55.71 time 17.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7425    0.7301    0.7363    1256.0\n",
            "           1     0.5071    0.3808    0.4350     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3750    0.1875    0.2500     208.0\n",
            "           4     0.5130    0.4900    0.5013     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3363    0.6580    0.4451     345.0\n",
            "\n",
            "    accuracy                         0.5697    2610.0\n",
            "   macro avg     0.3534    0.3495    0.3382    2610.0\n",
            "weighted avg     0.5653    0.5697    0.5571    2610.0\n",
            "\n",
            "epoch 48 train_loss 0.7993 train_acc 73.27 train_fscore 71.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2915 test_acc 56.74 test_fscore 56.14 time 17.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7534    0.7030    0.7273    1256.0\n",
            "           1     0.4551    0.5409    0.4943     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2951    0.2596    0.2762     208.0\n",
            "           4     0.5545    0.4552    0.5000     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3548    0.6058    0.4475     345.0\n",
            "\n",
            "    accuracy                         0.5674    2610.0\n",
            "   macro avg     0.3447    0.3664    0.3493    2610.0\n",
            "weighted avg     0.5674    0.5674    0.5614    2610.0\n",
            "\n",
            "epoch 49 train_loss 0.7933 train_acc 73.24 train_fscore 71.32 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2887 test_acc 57.66 test_fscore 56.39 time 17.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7285    0.7540    0.7410    1256.0\n",
            "           1     0.4407    0.5160    0.4754     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3063    0.2356    0.2663     208.0\n",
            "           4     0.5252    0.4403    0.4790     402.0\n",
            "           5     0.0714    0.0147    0.0244      68.0\n",
            "           6     0.3974    0.5391    0.4576     345.0\n",
            "\n",
            "    accuracy                         0.5766    2610.0\n",
            "   macro avg     0.3528    0.3571    0.3491    2610.0\n",
            "weighted avg     0.5577    0.5766    0.5639    2610.0\n",
            "\n",
            "epoch 50 train_loss 0.795 train_acc 73.45 train_fscore 71.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2796 test_acc 57.43 test_fscore 56.26 time 17.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7398    0.7333    0.7365    1256.0\n",
            "           1     0.4286    0.5231    0.4712     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3594    0.2212    0.2738     208.0\n",
            "           4     0.5222    0.4677    0.4934     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3724    0.5710    0.4508     345.0\n",
            "\n",
            "    accuracy                         0.5743    2610.0\n",
            "   macro avg     0.3460    0.3595    0.3465    2610.0\n",
            "weighted avg     0.5604    0.5743    0.5626    2610.0\n",
            "\n",
            "epoch 51 train_loss 0.7851 train_acc 73.44 train_fscore 71.7 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3116 test_acc 58.93 test_fscore 56.83 time 17.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7262    0.7771    0.7508    1256.0\n",
            "           1     0.4176    0.5231    0.4645     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4103    0.1538    0.2238     208.0\n",
            "           4     0.5071    0.5299    0.5182     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4126    0.4928    0.4491     345.0\n",
            "\n",
            "    accuracy                         0.5893    2610.0\n",
            "   macro avg     0.3534    0.3538    0.3438    2610.0\n",
            "weighted avg     0.5598    0.5893    0.5683    2610.0\n",
            "\n",
            "epoch 52 train_loss 0.7796 train_acc 73.56 train_fscore 71.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2902 test_acc 58.62 test_fscore 56.81 time 17.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7230    0.7771    0.7490    1256.0\n",
            "           1     0.4618    0.4306    0.4457     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3978    0.1779    0.2458     208.0\n",
            "           4     0.5011    0.5498    0.5243     402.0\n",
            "           5     0.0417    0.0147    0.0217      68.0\n",
            "           6     0.3973    0.5043    0.4444     345.0\n",
            "\n",
            "    accuracy                         0.5862    2610.0\n",
            "   macro avg     0.3604    0.3506    0.3473    2610.0\n",
            "weighted avg     0.5601    0.5862    0.5681    2610.0\n",
            "\n",
            "epoch 53 train_loss 0.7694 train_acc 74.05 train_fscore 72.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3138 test_acc 58.43 test_fscore 56.05 time 17.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7110    0.8049    0.7550    1256.0\n",
            "           1     0.4218    0.5374    0.4726     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3684    0.1346    0.1972     208.0\n",
            "           4     0.5196    0.4279    0.4693     402.0\n",
            "           5     0.0400    0.0147    0.0215      68.0\n",
            "           6     0.4081    0.4696    0.4367     345.0\n",
            "\n",
            "    accuracy                         0.5843    2610.0\n",
            "   macro avg     0.3527    0.3413    0.3360    2610.0\n",
            "weighted avg     0.5519    0.5843    0.5605    2610.0\n",
            "\n",
            "epoch 54 train_loss 0.7619 train_acc 74.05 train_fscore 72.47 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2953 test_acc 57.78 test_fscore 56.44 time 17.69\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7309    0.7373    0.7340    1256.0\n",
            "           1     0.4623    0.4804    0.4712     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3431    0.2260    0.2725     208.0\n",
            "           4     0.4714    0.5746    0.5179     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4092    0.4899    0.4459     345.0\n",
            "\n",
            "    accuracy                         0.5778    2610.0\n",
            "   macro avg     0.3453    0.3583    0.3488    2610.0\n",
            "weighted avg     0.5555    0.5778    0.5644    2610.0\n",
            "\n",
            "epoch 55 train_loss 0.7631 train_acc 74.15 train_fscore 72.54 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3214 test_acc 57.09 test_fscore 55.63 time 17.58\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.7548    0.7392    1256.0\n",
            "           1     0.5380    0.3523    0.4258     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3151    0.2212    0.2599     208.0\n",
            "           4     0.5345    0.4428    0.4844     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3482    0.6348    0.4497     345.0\n",
            "\n",
            "    accuracy                         0.5709    2610.0\n",
            "   macro avg     0.3514    0.3437    0.3370    2610.0\n",
            "weighted avg     0.5599    0.5709    0.5563    2610.0\n",
            "\n",
            "epoch 56 train_loss 0.7699 train_acc 73.94 train_fscore 72.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2993 test_acc 58.12 test_fscore 56.48 time 17.63\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7313    0.7540    0.7425    1256.0\n",
            "           1     0.4391    0.4875    0.4621     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3763    0.1683    0.2326     208.0\n",
            "           4     0.4944    0.5473    0.5195     402.0\n",
            "           5     0.1000    0.0147    0.0256      68.0\n",
            "           6     0.3899    0.5130    0.4431     345.0\n",
            "\n",
            "    accuracy                         0.5812    2610.0\n",
            "   macro avg     0.3616    0.3550    0.3465    2610.0\n",
            "weighted avg     0.5595    0.5812    0.5648    2610.0\n",
            "\n",
            "epoch 57 train_loss 0.741 train_acc 75.06 train_fscore 73.56 valid_loss nan valid_acc nan val_fscore nan test_loss 1.337 test_acc 56.13 test_fscore 54.87 time 17.76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7283    0.7365    0.7324    1256.0\n",
            "           1     0.4467    0.4626    0.4545     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3302    0.1683    0.2229     208.0\n",
            "           4     0.5041    0.4627    0.4825     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3381    0.5478    0.4181     345.0\n",
            "\n",
            "    accuracy                         0.5613    2610.0\n",
            "   macro avg     0.3353    0.3397    0.3301    2610.0\n",
            "weighted avg     0.5472    0.5613    0.5487    2610.0\n",
            "\n",
            "epoch 58 train_loss 0.7431 train_acc 74.44 train_fscore 72.94 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3569 test_acc 58.28 test_fscore 56.31 time 17.63\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7143    0.7922    0.7512    1256.0\n",
            "           1     0.4308    0.4982    0.4620     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3535    0.1683    0.2280     208.0\n",
            "           4     0.4798    0.5323    0.5047     402.0\n",
            "           5     0.0645    0.0294    0.0404      68.0\n",
            "           6     0.4426    0.3913    0.4154     345.0\n",
            "\n",
            "    accuracy                         0.5828    2610.0\n",
            "   macro avg     0.3551    0.3445    0.3431    2610.0\n",
            "weighted avg     0.5524    0.5828    0.5631    2610.0\n",
            "\n",
            "epoch 59 train_loss 0.7331 train_acc 74.63 train_fscore 73.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3673 test_acc 56.97 test_fscore 55.29 time 17.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7152    0.7699    0.7416    1256.0\n",
            "           1     0.4383    0.4804    0.4584     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3043    0.2019    0.2428     208.0\n",
            "           4     0.5396    0.3557    0.4288     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3861    0.5797    0.4635     345.0\n",
            "\n",
            "    accuracy                         0.5697    2610.0\n",
            "   macro avg     0.3405    0.3411    0.3336    2610.0\n",
            "weighted avg     0.5498    0.5697    0.5529    2610.0\n",
            "\n",
            "epoch 60 train_loss 0.7384 train_acc 74.61 train_fscore 73.22 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3967 test_acc 58.2 test_fscore 55.56 time 17.72\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7120    0.7954    0.7514    1256.0\n",
            "           1     0.4300    0.4698    0.4490     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3750    0.0865    0.1406     208.0\n",
            "           4     0.5182    0.4602    0.4875     402.0\n",
            "           5     0.1000    0.0147    0.0256      68.0\n",
            "           6     0.3810    0.5333    0.4444     345.0\n",
            "\n",
            "    accuracy                         0.5820    2610.0\n",
            "   macro avg     0.3595    0.3371    0.3284    2610.0\n",
            "weighted avg     0.5516    0.5820    0.5556    2610.0\n",
            "\n",
            "epoch 61 train_loss 0.7299 train_acc 75.41 train_fscore 73.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3638 test_acc 57.47 test_fscore 56.07 time 17.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7345    0.7468    0.7406    1256.0\n",
            "           1     0.4524    0.4733    0.4626     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3956    0.1731    0.2408     208.0\n",
            "           4     0.5013    0.4876    0.4943     402.0\n",
            "           5     0.1111    0.0147    0.0260      68.0\n",
            "           6     0.3623    0.5681    0.4424     345.0\n",
            "\n",
            "    accuracy                         0.5747    2610.0\n",
            "   macro avg     0.3653    0.3519    0.3438    2610.0\n",
            "weighted avg     0.5617    0.5747    0.5607    2610.0\n",
            "\n",
            "epoch 62 train_loss 0.7154 train_acc 75.69 train_fscore 74.4 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3588 test_acc 58.31 test_fscore 56.77 time 17.38\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7269    0.7715    0.7486    1256.0\n",
            "           1     0.4854    0.4128    0.4462     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.2212    0.2659     208.0\n",
            "           4     0.5148    0.5199    0.5173     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3824    0.5275    0.4434     345.0\n",
            "\n",
            "    accuracy                         0.5831    2610.0\n",
            "   macro avg     0.3490    0.3504    0.3459    2610.0\n",
            "weighted avg     0.5585    0.5831    0.5677    2610.0\n",
            "\n",
            "epoch 63 train_loss 0.7054 train_acc 76.18 train_fscore 74.95 valid_loss nan valid_acc nan val_fscore nan test_loss 1.394 test_acc 58.05 test_fscore 56.44 time 17.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7312    0.7643    0.7474    1256.0\n",
            "           1     0.4345    0.5196    0.4733     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3540    0.1923    0.2492     208.0\n",
            "           4     0.5093    0.4776    0.4929     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3839    0.5130    0.4392     345.0\n",
            "\n",
            "    accuracy                         0.5805    2610.0\n",
            "   macro avg     0.3447    0.3524    0.3431    2610.0\n",
            "weighted avg     0.5560    0.5805    0.5644    2610.0\n",
            "\n",
            "epoch 64 train_loss 0.7049 train_acc 75.92 train_fscore 74.69 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3712 test_acc 56.82 test_fscore 55.7 time 17.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7318    0.7428    0.7373    1256.0\n",
            "           1     0.4356    0.5053    0.4679     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2975    0.2260    0.2568     208.0\n",
            "           4     0.5053    0.4701    0.4871     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3723    0.4986    0.4263     345.0\n",
            "\n",
            "    accuracy                         0.5682    2610.0\n",
            "   macro avg     0.3346    0.3490    0.3393    2610.0\n",
            "weighted avg     0.5498    0.5682    0.5570    2610.0\n",
            "\n",
            "epoch 65 train_loss 0.6945 train_acc 76.43 train_fscore 75.16 valid_loss nan valid_acc nan val_fscore nan test_loss 1.429 test_acc 55.98 test_fscore 54.77 time 17.54\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7336    0.7301    0.7318    1256.0\n",
            "           1     0.4797    0.4199    0.4478     281.0\n",
            "           2     0.1250    0.0200    0.0345      50.0\n",
            "           3     0.3690    0.1490    0.2123     208.0\n",
            "           4     0.5043    0.4328    0.4659     402.0\n",
            "           5     0.0909    0.0147    0.0253      68.0\n",
            "           6     0.3288    0.6348    0.4332     345.0\n",
            "\n",
            "    accuracy                         0.5598    2610.0\n",
            "   macro avg     0.3759    0.3431    0.3358    2610.0\n",
            "weighted avg     0.5600    0.5598    0.5477    2610.0\n",
            "\n",
            "epoch 66 train_loss 0.6777 train_acc 76.59 train_fscore 75.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3956 test_acc 57.32 test_fscore 55.75 time 17.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7211    0.7699    0.7447    1256.0\n",
            "           1     0.4389    0.4733    0.4555     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3206    0.2019    0.2478     208.0\n",
            "           4     0.5340    0.4303    0.4766     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3657    0.5246    0.4310     345.0\n",
            "\n",
            "    accuracy                         0.5732    2610.0\n",
            "   macro avg     0.3400    0.3429    0.3365    2610.0\n",
            "weighted avg     0.5504    0.5732    0.5575    2610.0\n",
            "\n",
            "epoch 67 train_loss 0.6817 train_acc 76.64 train_fscore 75.59 valid_loss nan valid_acc nan val_fscore nan test_loss 1.423 test_acc 56.36 test_fscore 55.42 time 17.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7359    0.7277    0.7318    1256.0\n",
            "           1     0.4351    0.4769    0.4550     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.2067    0.2552     208.0\n",
            "           4     0.4935    0.4751    0.4842     402.0\n",
            "           5     0.0500    0.0147    0.0227      68.0\n",
            "           6     0.3622    0.5449    0.4352     345.0\n",
            "\n",
            "    accuracy                         0.5636    2610.0\n",
            "   macro avg     0.3443    0.3494    0.3406    2610.0\n",
            "weighted avg     0.5527    0.5636    0.5542    2610.0\n",
            "\n",
            "epoch 68 train_loss 0.6803 train_acc 76.85 train_fscore 75.78 valid_loss nan valid_acc nan val_fscore nan test_loss 1.435 test_acc 56.21 test_fscore 55.42 time 17.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7475    0.7094    0.7279    1256.0\n",
            "           1     0.4189    0.5053    0.4581     281.0\n",
            "           2     0.0357    0.0200    0.0256      50.0\n",
            "           3     0.3426    0.1779    0.2342     208.0\n",
            "           4     0.4692    0.5498    0.5063     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3796    0.5072    0.4342     345.0\n",
            "\n",
            "    accuracy                         0.5621    2610.0\n",
            "   macro avg     0.3419    0.3528    0.3409    2610.0\n",
            "weighted avg     0.5552    0.5621    0.5542    2610.0\n",
            "\n",
            "epoch 69 train_loss 0.673 train_acc 77.04 train_fscore 76.0 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4485 test_acc 57.01 test_fscore 56.07 time 17.41\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7420    0.7213    0.7315    1256.0\n",
            "           1     0.4591    0.4591    0.4591     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3440    0.2067    0.2583     208.0\n",
            "           4     0.5036    0.5199    0.5116     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3708    0.5826    0.4532     345.0\n",
            "\n",
            "    accuracy                         0.5701    2610.0\n",
            "   macro avg     0.3457    0.3557    0.3448    2610.0\n",
            "weighted avg     0.5605    0.5701    0.5607    2610.0\n",
            "\n",
            "epoch 70 train_loss 0.6673 train_acc 77.27 train_fscore 76.28 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4518 test_acc 56.28 test_fscore 54.92 time 17.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7240    0.7436    0.7337    1256.0\n",
            "           1     0.4057    0.5587    0.4701     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3398    0.1683    0.2251     208.0\n",
            "           4     0.4806    0.4627    0.4715     402.0\n",
            "           5     0.0526    0.0147    0.0230      68.0\n",
            "           6     0.3768    0.4522    0.4111     345.0\n",
            "\n",
            "    accuracy                         0.5628    2610.0\n",
            "   macro avg     0.3399    0.3429    0.3335    2610.0\n",
            "weighted avg     0.5444    0.5628    0.5492    2610.0\n",
            "\n",
            "epoch 71 train_loss 0.6447 train_acc 77.78 train_fscore 76.81 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4617 test_acc 55.1 test_fscore 54.06 time 17.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7248    0.7277    0.7263    1256.0\n",
            "           1     0.4239    0.5053    0.4610     281.0\n",
            "           2     0.0250    0.0200    0.0222      50.0\n",
            "           3     0.2920    0.1587    0.2056     208.0\n",
            "           4     0.5238    0.3557    0.4237     402.0\n",
            "           5     0.0625    0.0147    0.0238      68.0\n",
            "           6     0.3566    0.5913    0.4449     345.0\n",
            "\n",
            "    accuracy                         0.5510    2610.0\n",
            "   macro avg     0.3441    0.3391    0.3297    2610.0\n",
            "weighted avg     0.5476    0.5510    0.5406    2610.0\n",
            "\n",
            "epoch 72 train_loss 0.6458 train_acc 78.25 train_fscore 77.34 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5134 test_acc 54.33 test_fscore 53.49 time 17.54\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7316    0.7054    0.7183    1256.0\n",
            "           1     0.4582    0.4093    0.4323     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3474    0.1587    0.2178     208.0\n",
            "           4     0.5149    0.3881    0.4426     402.0\n",
            "           5     0.0385    0.0147    0.0213      68.0\n",
            "           6     0.3175    0.6580    0.4283     345.0\n",
            "\n",
            "    accuracy                         0.5433    2610.0\n",
            "   macro avg     0.3440    0.3334    0.3229    2610.0\n",
            "weighted avg     0.5514    0.5433    0.5349    2610.0\n",
            "\n",
            "epoch 73 train_loss 0.6449 train_acc 77.79 train_fscore 76.86 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4403 test_acc 55.67 test_fscore 55.11 time 17.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7238    0.7428    0.7332    1256.0\n",
            "           1     0.4294    0.4982    0.4613     281.0\n",
            "           2     0.0189    0.0200    0.0194      50.0\n",
            "           3     0.2366    0.2548    0.2454     208.0\n",
            "           4     0.5082    0.4627    0.4844     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4130    0.4058    0.4094     345.0\n",
            "\n",
            "    accuracy                         0.5567    2610.0\n",
            "   macro avg     0.3328    0.3406    0.3361    2610.0\n",
            "weighted avg     0.5466    0.5567    0.5511    2610.0\n",
            "\n",
            "epoch 74 train_loss 0.6303 train_acc 78.01 train_fscore 77.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4881 test_acc 56.9 test_fscore 54.85 time 17.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7095    0.7818    0.7439    1256.0\n",
            "           1     0.4327    0.4804    0.4553     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3913    0.1731    0.2400     208.0\n",
            "           4     0.4932    0.3607    0.4167     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3703    0.5420    0.4400     345.0\n",
            "\n",
            "    accuracy                         0.5690    2610.0\n",
            "   macro avg     0.3424    0.3340    0.3280    2610.0\n",
            "weighted avg     0.5441    0.5690    0.5485    2610.0\n",
            "\n",
            "epoch 75 train_loss 0.6329 train_acc 77.89 train_fscore 77.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4374 test_acc 56.25 test_fscore 55.46 time 17.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7265    0.7444    0.7354    1256.0\n",
            "           1     0.4295    0.4875    0.4567     281.0\n",
            "           2     0.0200    0.0200    0.0200      50.0\n",
            "           3     0.2810    0.2067    0.2382     208.0\n",
            "           4     0.5125    0.4602    0.4849     402.0\n",
            "           5     0.0714    0.0147    0.0244      68.0\n",
            "           6     0.3897    0.4812    0.4306     345.0\n",
            "\n",
            "    accuracy                         0.5625    2610.0\n",
            "   macro avg     0.3472    0.3450    0.3415    2610.0\n",
            "weighted avg     0.5509    0.5625    0.5546    2610.0\n",
            "\n",
            "epoch 76 train_loss 0.6415 train_acc 77.79 train_fscore 76.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4789 test_acc 57.09 test_fscore 55.58 time 17.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7261    0.7619    0.7436    1256.0\n",
            "           1     0.4444    0.4698    0.4567     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3506    0.1298    0.1895     208.0\n",
            "           4     0.4707    0.5398    0.5029     402.0\n",
            "           5     0.0645    0.0294    0.0404      68.0\n",
            "           6     0.3894    0.4493    0.4172     345.0\n",
            "\n",
            "    accuracy                         0.5709    2610.0\n",
            "   macro avg     0.3494    0.3400    0.3358    2610.0\n",
            "weighted avg     0.5509    0.5709    0.5558    2610.0\n",
            "\n",
            "epoch 77 train_loss 0.6209 train_acc 78.37 train_fscore 77.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5243 test_acc 56.44 test_fscore 54.95 time 17.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7233    0.7556    0.7391    1256.0\n",
            "           1     0.4362    0.4626    0.4491     281.0\n",
            "           2     0.0769    0.0200    0.0317      50.0\n",
            "           3     0.3789    0.1731    0.2376     208.0\n",
            "           4     0.5186    0.3806    0.4390     402.0\n",
            "           5     0.0833    0.0147    0.0250      68.0\n",
            "           6     0.3470    0.5884    0.4366     345.0\n",
            "\n",
            "    accuracy                         0.5644    2610.0\n",
            "   macro avg     0.3663    0.3421    0.3369    2610.0\n",
            "weighted avg     0.5546    0.5644    0.5495    2610.0\n",
            "\n",
            "epoch 78 train_loss 0.6043 train_acc 79.12 train_fscore 78.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5662 test_acc 55.82 test_fscore 55.27 time 17.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7473    0.7038    0.7249    1256.0\n",
            "           1     0.4385    0.4057    0.4214     281.0\n",
            "           2     0.0385    0.0200    0.0263      50.0\n",
            "           3     0.3805    0.2067    0.2679     208.0\n",
            "           4     0.4660    0.5448    0.5023     402.0\n",
            "           5     0.0400    0.0147    0.0215      68.0\n",
            "           6     0.3659    0.5652    0.4442     345.0\n",
            "\n",
            "    accuracy                         0.5582    2610.0\n",
            "   macro avg     0.3538    0.3516    0.3441    2610.0\n",
            "weighted avg     0.5590    0.5582    0.5527    2610.0\n",
            "\n",
            "epoch 79 train_loss 0.593 train_acc 79.48 train_fscore 78.74 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5379 test_acc 56.48 test_fscore 55.05 time 17.29\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7224    0.7564    0.7390    1256.0\n",
            "           1     0.4413    0.4413    0.4413     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3298    0.1490    0.2053     208.0\n",
            "           4     0.4973    0.4652    0.4807     402.0\n",
            "           5     0.0294    0.0147    0.0196      68.0\n",
            "           6     0.3606    0.5246    0.4274     345.0\n",
            "\n",
            "    accuracy                         0.5648    2610.0\n",
            "   macro avg     0.3401    0.3359    0.3305    2610.0\n",
            "weighted avg     0.5465    0.5648    0.5505    2610.0\n",
            "\n",
            "epoch 80 train_loss 0.5965 train_acc 79.53 train_fscore 78.94 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5583 test_acc 56.32 test_fscore 54.91 time 17.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7172    0.7532    0.7348    1256.0\n",
            "           1     0.4143    0.5160    0.4596     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3673    0.1731    0.2353     208.0\n",
            "           4     0.4862    0.3955    0.4362     402.0\n",
            "           5     0.0789    0.0441    0.0566      68.0\n",
            "           6     0.3843    0.5246    0.4436     345.0\n",
            "\n",
            "    accuracy                         0.5632    2610.0\n",
            "   macro avg     0.3498    0.3438    0.3380    2610.0\n",
            "weighted avg     0.5468    0.5632    0.5491    2610.0\n",
            "\n",
            "epoch 81 train_loss 0.587 train_acc 79.67 train_fscore 78.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5223 test_acc 57.13 test_fscore 55.36 time 17.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7068    0.7811    0.7421    1256.0\n",
            "           1     0.4257    0.4591    0.4418     281.0\n",
            "           2     0.0588    0.0200    0.0299      50.0\n",
            "           3     0.3617    0.1635    0.2252     208.0\n",
            "           4     0.4715    0.4527    0.4619     402.0\n",
            "           5     0.0625    0.0294    0.0400      68.0\n",
            "           6     0.4154    0.4696    0.4408     345.0\n",
            "\n",
            "    accuracy                         0.5713    2610.0\n",
            "   macro avg     0.3575    0.3393    0.3402    2610.0\n",
            "weighted avg     0.5451    0.5713    0.5536    2610.0\n",
            "\n",
            "epoch 82 train_loss 0.575 train_acc 79.89 train_fscore 79.17 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6127 test_acc 56.63 test_fscore 55.26 time 17.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7142    0.7699    0.7410    1256.0\n",
            "           1     0.4265    0.4128    0.4195     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3000    0.2163    0.2514     208.0\n",
            "           4     0.4960    0.4577    0.4761     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3971    0.4812    0.4351     345.0\n",
            "\n",
            "    accuracy                         0.5663    2610.0\n",
            "   macro avg     0.3334    0.3340    0.3319    2610.0\n",
            "weighted avg     0.5424    0.5663    0.5526    2610.0\n",
            "\n",
            "epoch 83 train_loss 0.5663 train_acc 80.16 train_fscore 79.54 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5731 test_acc 56.55 test_fscore 55.07 time 17.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7170    0.7604    0.7380    1256.0\n",
            "           1     0.4224    0.4555    0.4384     281.0\n",
            "           2     0.0385    0.0200    0.0263      50.0\n",
            "           3     0.3297    0.1442    0.2007     208.0\n",
            "           4     0.4842    0.4577    0.4706     402.0\n",
            "           5     0.0370    0.0147    0.0211      68.0\n",
            "           6     0.3925    0.5130    0.4447     345.0\n",
            "\n",
            "    accuracy                         0.5655    2610.0\n",
            "   macro avg     0.3459    0.3379    0.3342    2610.0\n",
            "weighted avg     0.5449    0.5655    0.5507    2610.0\n",
            "\n",
            "epoch 84 train_loss 0.5631 train_acc 80.46 train_fscore 79.94 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6327 test_acc 54.67 test_fscore 53.94 time 17.69\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7329    0.7014    0.7168    1256.0\n",
            "           1     0.4349    0.4875    0.4597     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3143    0.1587    0.2109     208.0\n",
            "           4     0.5236    0.3856    0.4441     402.0\n",
            "           5     0.0556    0.0147    0.0233      68.0\n",
            "           6     0.3443    0.6377    0.4472     345.0\n",
            "\n",
            "    accuracy                         0.5467    2610.0\n",
            "   macro avg     0.3437    0.3408    0.3289    2610.0\n",
            "weighted avg     0.5522    0.5467    0.5394    2610.0\n",
            "\n",
            "epoch 85 train_loss 0.5789 train_acc 79.84 train_fscore 79.28 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5827 test_acc 54.33 test_fscore 54.71 time 17.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7460    0.6990    0.7217    1256.0\n",
            "           1     0.4243    0.5089    0.4628     281.0\n",
            "           2     0.0175    0.0400    0.0244      50.0\n",
            "           3     0.3038    0.2308    0.2623     208.0\n",
            "           4     0.4733    0.4851    0.4791     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3907    0.4406    0.4142     345.0\n",
            "\n",
            "    accuracy                         0.5433    2610.0\n",
            "   macro avg     0.3365    0.3435    0.3378    2610.0\n",
            "weighted avg     0.5538    0.5433    0.5471    2610.0\n",
            "\n",
            "epoch 86 train_loss 0.5424 train_acc 80.82 train_fscore 80.31 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5844 test_acc 56.05 test_fscore 54.87 time 17.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7243    0.7404    0.7323    1256.0\n",
            "           1     0.4448    0.4448    0.4448     281.0\n",
            "           2     0.0323    0.0200    0.0247      50.0\n",
            "           3     0.3469    0.1635    0.2222     208.0\n",
            "           4     0.4877    0.4453    0.4655     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3653    0.5623    0.4429     345.0\n",
            "\n",
            "    accuracy                         0.5605    2610.0\n",
            "   macro avg     0.3431    0.3395    0.3332    2610.0\n",
            "weighted avg     0.5481    0.5605    0.5487    2610.0\n",
            "\n",
            "epoch 87 train_loss 0.5328 train_acc 81.68 train_fscore 81.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6288 test_acc 53.83 test_fscore 53.26 time 17.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7333    0.6919    0.7120    1256.0\n",
            "           1     0.3870    0.5302    0.4474     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3467    0.1250    0.1837     208.0\n",
            "           4     0.4407    0.4900    0.4641     402.0\n",
            "           5     0.0545    0.0441    0.0488      68.0\n",
            "           6     0.3684    0.4667    0.4118     345.0\n",
            "\n",
            "    accuracy                         0.5383    2610.0\n",
            "   macro avg     0.3330    0.3354    0.3240    2610.0\n",
            "weighted avg     0.5402    0.5383    0.5326    2610.0\n",
            "\n",
            "epoch 88 train_loss 0.5199 train_acc 81.95 train_fscore 81.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6236 test_acc 54.56 test_fscore 54.37 time 17.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7351    0.7158    0.7253    1256.0\n",
            "           1     0.4313    0.4911    0.4592     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2520    0.3077    0.2771     208.0\n",
            "           4     0.4792    0.4005    0.4363     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3857    0.4696    0.4235     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.3262    0.3407    0.3316    2610.0\n",
            "weighted avg     0.5450    0.5456    0.5437    2610.0\n",
            "\n",
            "epoch 89 train_loss 0.5315 train_acc 81.74 train_fscore 81.32 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6527 test_acc 54.21 test_fscore 54.09 time 17.44\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7372    0.6990    0.7176    1256.0\n",
            "           1     0.4387    0.5089    0.4712     281.0\n",
            "           2     0.0159    0.0200    0.0177      50.0\n",
            "           3     0.2913    0.1442    0.1929     208.0\n",
            "           4     0.4819    0.4627    0.4721     402.0\n",
            "           5     0.0577    0.0441    0.0500      68.0\n",
            "           6     0.3558    0.5043    0.4173     345.0\n",
            "\n",
            "    accuracy                         0.5421    2610.0\n",
            "   macro avg     0.3398    0.3405    0.3341    2610.0\n",
            "weighted avg     0.5483    0.5421    0.5409    2610.0\n",
            "\n",
            "epoch 90 train_loss 0.5398 train_acc 81.1 train_fscore 80.69 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6456 test_acc 56.28 test_fscore 54.37 time 17.62\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7092    0.7747    0.7405    1256.0\n",
            "           1     0.4125    0.4947    0.4498     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3871    0.1154    0.1778     208.0\n",
            "           4     0.4909    0.4005    0.4411     402.0\n",
            "           5     0.0526    0.0294    0.0377      68.0\n",
            "           6     0.3696    0.4928    0.4224     345.0\n",
            "\n",
            "    accuracy                         0.5628    2610.0\n",
            "   macro avg     0.3460    0.3296    0.3242    2610.0\n",
            "weighted avg     0.5424    0.5628    0.5437    2610.0\n",
            "\n",
            "epoch 91 train_loss 0.5203 train_acc 81.96 train_fscore 81.58 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6417 test_acc 55.17 test_fscore 54.42 time 17.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7344    0.7110    0.7225    1256.0\n",
            "           1     0.4187    0.4947    0.4535     281.0\n",
            "           2     0.0357    0.0200    0.0256      50.0\n",
            "           3     0.3469    0.1635    0.2222     208.0\n",
            "           4     0.4306    0.5174    0.4701     402.0\n",
            "           5     0.0690    0.0294    0.0412      68.0\n",
            "           6     0.3844    0.4725    0.4239     345.0\n",
            "\n",
            "    accuracy                         0.5517    2610.0\n",
            "   macro avg     0.3457    0.3441    0.3370    2610.0\n",
            "weighted avg     0.5458    0.5517    0.5442    2610.0\n",
            "\n",
            "epoch 92 train_loss 0.5016 train_acc 82.26 train_fscore 81.9 valid_loss nan valid_acc nan val_fscore nan test_loss 1.671 test_acc 55.36 test_fscore 54.3 time 17.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7213    0.7396    0.7303    1256.0\n",
            "           1     0.4567    0.4128    0.4336     281.0\n",
            "           2     0.0256    0.0200    0.0225      50.0\n",
            "           3     0.3130    0.1731    0.2229     208.0\n",
            "           4     0.4707    0.4403    0.4550     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3550    0.5391    0.4281     345.0\n",
            "\n",
            "    accuracy                         0.5536    2610.0\n",
            "   macro avg     0.3346    0.3321    0.3275    2610.0\n",
            "weighted avg     0.5411    0.5536    0.5430    2610.0\n",
            "\n",
            "epoch 93 train_loss 0.4991 train_acc 82.41 train_fscore 81.96 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7164 test_acc 54.56 test_fscore 53.61 time 17.58\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7167    0.7293    0.7230    1256.0\n",
            "           1     0.4041    0.4875    0.4419     281.0\n",
            "           2     0.0175    0.0200    0.0187      50.0\n",
            "           3     0.3402    0.1587    0.2164     208.0\n",
            "           4     0.4982    0.3483    0.4100     402.0\n",
            "           5     0.0333    0.0147    0.0204      68.0\n",
            "           6     0.3712    0.5681    0.4490     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.3402    0.3324    0.3256    2610.0\n",
            "weighted avg     0.5425    0.5456    0.5361    2610.0\n",
            "\n",
            "epoch 94 train_loss 0.48 train_acc 82.9 train_fscore 82.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7193 test_acc 55.67 test_fscore 53.93 time 17.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7008    0.7850    0.7405    1256.0\n",
            "           1     0.4407    0.4626    0.4514     281.0\n",
            "           2     0.0278    0.0200    0.0233      50.0\n",
            "           3     0.3017    0.1683    0.2160     208.0\n",
            "           4     0.4740    0.3632    0.4113     402.0\n",
            "           5     0.0476    0.0147    0.0225      68.0\n",
            "           6     0.3607    0.4464    0.3990     345.0\n",
            "\n",
            "    accuracy                         0.5567    2610.0\n",
            "   macro avg     0.3362    0.3229    0.3234    2610.0\n",
            "weighted avg     0.5312    0.5567    0.5393    2610.0\n",
            "\n",
            "epoch 95 train_loss 0.4884 train_acc 82.97 train_fscore 82.64 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6309 test_acc 55.67 test_fscore 54.22 time 17.57\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6996    0.7675    0.7320    1256.0\n",
            "           1     0.4197    0.4555    0.4369     281.0\n",
            "           2     0.0500    0.0200    0.0286      50.0\n",
            "           3     0.2727    0.1875    0.2222     208.0\n",
            "           4     0.4675    0.3930    0.4270     402.0\n",
            "           5     0.0263    0.0147    0.0189      68.0\n",
            "           6     0.4175    0.4696    0.4420     345.0\n",
            "\n",
            "    accuracy                         0.5567    2610.0\n",
            "   macro avg     0.3362    0.3297    0.3296    2610.0\n",
            "weighted avg     0.5324    0.5567    0.5422    2610.0\n",
            "\n",
            "epoch 96 train_loss 0.4961 train_acc 82.36 train_fscore 82.04 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7014 test_acc 54.33 test_fscore 53.49 time 17.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7197    0.7197    0.7197    1256.0\n",
            "           1     0.4205    0.4235    0.4220     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2654    0.2067    0.2324     208.0\n",
            "           4     0.5121    0.3682    0.4284     402.0\n",
            "           5     0.0400    0.0147    0.0215      68.0\n",
            "           6     0.3500    0.5884    0.4389     345.0\n",
            "\n",
            "    accuracy                         0.5433    2610.0\n",
            "   macro avg     0.3297    0.3316    0.3233    2610.0\n",
            "weighted avg     0.5390    0.5433    0.5349    2610.0\n",
            "\n",
            "epoch 97 train_loss 0.4756 train_acc 83.54 train_fscore 83.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7833 test_acc 54.06 test_fscore 53.4 time 17.44\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7228    0.7078    0.7152    1256.0\n",
            "           1     0.4296    0.4235    0.4265     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2857    0.1635    0.2080     208.0\n",
            "           4     0.4508    0.4328    0.4416     402.0\n",
            "           5     0.0606    0.0294    0.0396      68.0\n",
            "           6     0.3642    0.5594    0.4411     345.0\n",
            "\n",
            "    accuracy                         0.5406    2610.0\n",
            "   macro avg     0.3305    0.3309    0.3246    2610.0\n",
            "weighted avg     0.5360    0.5406    0.5340    2610.0\n",
            "\n",
            "epoch 98 train_loss 0.4669 train_acc 83.69 train_fscore 83.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7252 test_acc 54.56 test_fscore 54.09 time 17.56\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7281    0.7078    0.7178    1256.0\n",
            "           1     0.4255    0.4270    0.4263     281.0\n",
            "           2     0.0294    0.0200    0.0238      50.0\n",
            "           3     0.2950    0.1971    0.2363     208.0\n",
            "           4     0.4571    0.4776    0.4672     402.0\n",
            "           5     0.0270    0.0147    0.0190      68.0\n",
            "           6     0.3774    0.5217    0.4380     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.3342    0.3380    0.3326    2610.0\n",
            "weighted avg     0.5413    0.5456    0.5409    2610.0\n",
            "\n",
            "epoch 99 train_loss 0.4448 train_acc 84.51 train_fscore 84.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7578 test_acc 53.64 test_fscore 53.56 time 17.64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7326    0.7046    0.7183    1256.0\n",
            "           1     0.3869    0.5053    0.4383     281.0\n",
            "           2     0.0303    0.0400    0.0345      50.0\n",
            "           3     0.2903    0.2596    0.2741     208.0\n",
            "           4     0.4716    0.3930    0.4288     402.0\n",
            "           5     0.0833    0.0147    0.0250      68.0\n",
            "           6     0.3624    0.4580    0.4046     345.0\n",
            "\n",
            "    accuracy                         0.5364    2610.0\n",
            "   macro avg     0.3368    0.3393    0.3319    2610.0\n",
            "weighted avg     0.5406    0.5364    0.5356    2610.0\n",
            "\n",
            "epoch 100 train_loss 0.4449 train_acc 84.37 train_fscore 84.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.753 test_acc 55.36 test_fscore 54.12 time 17.4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7140    0.7572    0.7349    1256.0\n",
            "           1     0.4180    0.3808    0.3985     281.0\n",
            "           2     0.0256    0.0200    0.0225      50.0\n",
            "           3     0.2979    0.2019    0.2407     208.0\n",
            "           4     0.4344    0.4527    0.4434     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3951    0.4696    0.4291     345.0\n",
            "\n",
            "    accuracy                         0.5536    2610.0\n",
            "   macro avg     0.3264    0.3260    0.3242    2610.0\n",
            "weighted avg     0.5319    0.5536    0.5412    2610.0\n",
            "\n",
            "Test performance..\n",
            "Fscore 57.45 accuracy 59.39\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7253    0.7946    0.7584    1256.0\n",
            "           1     0.4537    0.5053    0.4781     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2706    0.2212    0.2434     208.0\n",
            "           4     0.5140    0.5473    0.5301     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4458    0.4174    0.4311     345.0\n",
            "\n",
            "    accuracy                         0.5939    2610.0\n",
            "   macro avg     0.3442    0.3551    0.3487    2610.0\n",
            "weighted avg     0.5575    0.5939    0.5745    2610.0\n",
            "\n",
            "[[998.  56.   0.  77.  84.   0.  41.]\n",
            " [ 66. 142.   0.   4.  38.   0.  31.]\n",
            " [ 25.   6.   0.   3.   6.   0.  10.]\n",
            " [ 92.  19.   0.  46.  14.   0.  37.]\n",
            " [ 95.  37.   0.  12. 220.   0.  38.]\n",
            " [ 27.   9.   0.   6.   4.   0.  22.]\n",
            " [ 73.  44.   0.  22.  62.   0. 144.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
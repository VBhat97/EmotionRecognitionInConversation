{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa_on_ERC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNa/Hj3C7hpPjPid40fcSqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anitayadav3/EmotionRecognitionInConversation/blob/master/RoBERTa_on_ERC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olmz5sdiAERJ",
        "outputId": "32c7a03b-7c35-4795-edbd-fb74d636a806"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 22.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 42.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETWOWe6W1lz9",
        "outputId": "99c15745-f824-4d55-8454-3aa688cd8cb5"
      },
      "source": [
        "import fastai\r\n",
        "import transformers\r\n",
        "print(fastai.__version__)\r\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.61\n",
            "4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFznjbKeATIO"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "from pathlib import Path \r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import random \r\n",
        "\r\n",
        "# fastai\r\n",
        "from fastai import *\r\n",
        "from fastai.text import *\r\n",
        "from fastai.callbacks import *\r\n",
        "\r\n",
        "# transformers\r\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\r\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM6Dn9KbAWt7"
      },
      "source": [
        "MODEL_CLASSES = {\r\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\r\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zde8T1PFFPH"
      },
      "source": [
        "# Parameters\r\n",
        "seed = 42\r\n",
        "use_fp16 = False\r\n",
        "bs = 16\r\n",
        "\r\n",
        "model_type = 'roberta'\r\n",
        "pretrained_model_name = 'roberta-base'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vph3koySFHqD"
      },
      "source": [
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMqVH9hkGCI1"
      },
      "source": [
        "def seed_all(seed_value):\r\n",
        "    random.seed(seed_value) # Python\r\n",
        "    np.random.seed(seed_value) # cpu vars\r\n",
        "    torch.manual_seed(seed_value) # cpu  vars\r\n",
        "    \r\n",
        "    if torch.cuda.is_available(): \r\n",
        "        torch.cuda.manual_seed(seed_value)\r\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\r\n",
        "        torch.backends.cudnn.deterministic = True  #needed\r\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr0ae-odGDCB"
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\r\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\r\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\r\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\r\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\r\n",
        "        self.model_type = model_type\r\n",
        "\r\n",
        "    def __call__(self, *args, **kwargs): \r\n",
        "        return self\r\n",
        "\r\n",
        "    def tokenizer(self, t:str) -> List[str]:\r\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\r\n",
        "        CLS = self._pretrained_tokenizer.cls_token\r\n",
        "        SEP = self._pretrained_tokenizer.sep_token\r\n",
        "        if self.model_type in ['roberta']:\r\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\r\n",
        "            tokens = [CLS] + tokens + [SEP]\r\n",
        "        else:\r\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\r\n",
        "            if self.model_type in ['xlnet']:\r\n",
        "                tokens = tokens + [SEP] +  [CLS]\r\n",
        "            else:\r\n",
        "                tokens = [CLS] + tokens + [SEP]\r\n",
        "        return tokens"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "TOZiPq9IHLpI",
        "outputId": "9a05096a-9f02-4640-d05e-ecbdff7a9476"
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\r\n",
        "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\r\n",
        "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3bbd84eb67c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtransformer_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformer_base_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformersBaseTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfastai_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_base_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_rules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_rules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-a40919ef1031>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained_tokenizer, model_type, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_tokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pretrained_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RobertaTokenizer' object has no attribute 'max_len'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaOw2g8MR8xR"
      },
      "source": [
        "class TransformersVocab(Vocab):\r\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\r\n",
        "        super(TransformersVocab, self).__init__(itos = [])\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "    \r\n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\r\n",
        "        \"Convert a list of tokens `t` to their ids.\"\r\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\r\n",
        "        #return self.tokenizer.encode(t)\r\n",
        "\r\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\r\n",
        "        \"Convert a list of `nums` to their tokens.\"\r\n",
        "        nums = np.array(nums).tolist()\r\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\r\n",
        "    \r\n",
        "    def __getstate__(self):\r\n",
        "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\r\n",
        "\r\n",
        "    def __setstate__(self, state:dict):\r\n",
        "        self.itos = state['itos']\r\n",
        "        self.tokenizer = state['tokenizer']\r\n",
        "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "QjvbTxknSDdn",
        "outputId": "ddca6fd6-4708-46f9-ce96-3b21634f4444"
      },
      "source": [
        "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\r\n",
        "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\r\n",
        "\r\n",
        "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\r\n",
        "\r\n",
        "transformer_processor = [tokenize_processor, numericalize_processor]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-495240ff95d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnumericalize_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumericalizeProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenize_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizeProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastai_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtransformer_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumericalize_processor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fastai_tokenizer' is not defined"
          ]
        }
      ]
    }
  ]
}
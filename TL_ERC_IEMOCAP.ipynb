{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TL-ERC_IEMOCAP.ipynb",
      "provenance": [],
      "mount_file_id": "1hBfUI0BOqqpzPVNBKidsV51KZNOxdTz9",
      "authorship_tag": "ABX9TyMwYoiQ0knrXXyDJB4PWSgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anitayadav3/EmotionRecognitionInConversation/blob/master/TL_ERC_IEMOCAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pETrP0_ivwDl",
        "outputId": "7594003a-a55e-4a73-db13-e339875891f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/TL-ERC"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVH4fWXowX40",
        "outputId": "a270024e-617d-4334-ba53-8404d2297f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd bert_model/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx3mnH6rwlns",
        "outputId": "922edf19-b08d-47f8-ef31-8a90c2a0c719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDXbobLawvGN",
        "outputId": "eed0bf1b-45b4-4510-c708-856b90b207bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.63)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.63)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.63->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0BfakGoweV3",
        "outputId": "996fd446-c71b-489d-c2c2-cf52489c76ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/cornell_weights.pkl --data=iemocap --n_epoch=50"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.6 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.9266560077667236\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 1 loss average: 1.762\n",
            "train\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0431    0.0133    0.0203       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2500    0.0444    0.0755      1080\n",
            "           3     0.1285    0.0614    0.0831       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2551    0.8504    0.3925      1210\n",
            "\n",
            "    accuracy                         0.2401      4699\n",
            "   macro avg     0.1128    0.1616    0.0952      4699\n",
            "weighted avg     0.1471    0.2401    0.1333      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2844    0.8620    0.4276       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3813    0.4593    0.4167       381\n",
            "\n",
            "    accuracy                         0.3118      1623\n",
            "   macro avg     0.1109    0.2202    0.1407      1623\n",
            "weighted avg     0.1568    0.3118    0.1990      1623\n",
            "\n",
            "1.7615495522816975 0.13329484937679448 0.1721575470803111 0.19899387200466165\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 2.415884494781494\n",
            "100%|███████████████████████████████████████████| 48/48 [00:18<00:00,  2.64it/s]\n",
            "Epoch 2 loss average: 1.706\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.6222    0.0366    0.0692       764\n",
            "           2     0.2861    0.1778    0.2193      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.1818    0.0038    0.0075       520\n",
            "           5     0.2774    0.9107    0.4253      1210\n",
            "\n",
            "    accuracy                         0.2818      4699\n",
            "   macro avg     0.2279    0.1882    0.1202      4699\n",
            "weighted avg     0.2585    0.2818    0.1720      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.2586    0.1224    0.1662       245\n",
            "           2     0.2914    0.7422    0.4185       384\n",
            "           3     1.0000    0.0529    0.1006       170\n",
            "           4     0.7273    0.0268    0.0516       299\n",
            "           5     0.5069    0.6772    0.5798       381\n",
            "\n",
            "    accuracy                         0.3635      1623\n",
            "   macro avg     0.4640    0.2702    0.2194      1623\n",
            "weighted avg     0.4657    0.3635    0.2803      1623\n",
            "\n",
            "1.7056300789117813 0.172012124366948 0.20964543002781746 0.2802505727514808\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.2602224349975586\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.81it/s]\n",
            "Epoch 3 loss average: 1.357\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0080    0.0158       376\n",
            "           1     0.5443    0.5223    0.5331       764\n",
            "           2     0.3598    0.4028    0.3801      1080\n",
            "           3     0.6698    0.1923    0.2988       749\n",
            "           4     0.3987    0.4769    0.4343       520\n",
            "           5     0.4564    0.7231    0.5596      1210\n",
            "\n",
            "    accuracy                         0.4478      4699\n",
            "   macro avg     0.5715    0.3876    0.3703      4699\n",
            "weighted avg     0.5196    0.4478    0.4151      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0278    0.0541       144\n",
            "           1     0.6402    0.6898    0.6640       245\n",
            "           2     0.4401    0.7552    0.5561       384\n",
            "           3     0.7177    0.5235    0.6054       170\n",
            "           4     0.5520    0.6923    0.6142       299\n",
            "           5     0.6548    0.3386    0.4464       381\n",
            "\n",
            "    accuracy                         0.5471      1623\n",
            "   macro avg     0.6675    0.5045    0.4900      1623\n",
            "weighted avg     0.6201    0.5471    0.5180      1623\n",
            "\n",
            "1.3570280447602272 0.4150848254575492 0.4084963626250872 0.5179684997492073\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 0.7699130177497864\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.82it/s]\n",
            "Epoch 4 loss average: 1.105\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4388    0.2766    0.3393       376\n",
            "           1     0.6578    0.7801    0.7138       764\n",
            "           2     0.4595    0.4306    0.4446      1080\n",
            "           3     0.6290    0.5047    0.5600       749\n",
            "           4     0.5896    0.6077    0.5985       520\n",
            "           5     0.5302    0.6165    0.5701      1210\n",
            "\n",
            "    accuracy                         0.5544      4699\n",
            "   macro avg     0.5508    0.5360    0.5377      4699\n",
            "weighted avg     0.5497    0.5544    0.5477      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0069    0.0138       144\n",
            "           1     0.5747    0.8163    0.6745       245\n",
            "           2     0.6166    0.5234    0.5662       384\n",
            "           3     0.6266    0.5824    0.6037       170\n",
            "           4     0.5763    0.7960    0.6685       299\n",
            "           5     0.5703    0.5643    0.5673       381\n",
            "\n",
            "    accuracy                         0.5878      1623\n",
            "   macro avg     0.6607    0.5482    0.5157      1623\n",
            "weighted avg     0.6270    0.5878    0.5566      1623\n",
            "\n",
            "1.1046356111764908 0.5476728481470314 0.4263244887264115 0.5565724475021653\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.0886491537094116\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 5 loss average: 0.954\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4450    0.2261    0.2998       376\n",
            "           1     0.7084    0.8521    0.7736       764\n",
            "           2     0.5848    0.5333    0.5579      1080\n",
            "           3     0.6739    0.5354    0.5967       749\n",
            "           4     0.5551    0.7077    0.6221       520\n",
            "           5     0.5817    0.6471    0.6127      1210\n",
            "\n",
            "    accuracy                         0.6095      4699\n",
            "   macro avg     0.5915    0.5836    0.5771      4699\n",
            "weighted avg     0.6038    0.6095    0.5997      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8000    0.1111    0.1951       144\n",
            "           1     0.4492    0.7755    0.5689       245\n",
            "           2     0.5056    0.5911    0.5450       384\n",
            "           3     0.8507    0.3353    0.4810       170\n",
            "           4     0.7097    0.3679    0.4846       299\n",
            "           5     0.5344    0.7139    0.6112       381\n",
            "\n",
            "    accuracy                         0.5373      1623\n",
            "   macro avg     0.6416    0.4825    0.4810      1623\n",
            "weighted avg     0.6037    0.5373    0.5153      1623\n",
            "\n",
            "0.9542043805122375 0.5997189757491568 0.3906604890918207 0.5152795143826829\n",
            "Patience counter: 1\n",
            "Epoch: 6, iter 0: loss = 1.0842384099960327\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 6 loss average: 0.811\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7130    0.4229    0.5309       376\n",
            "           1     0.8055    0.8508    0.8275       764\n",
            "           2     0.6441    0.6333    0.6387      1080\n",
            "           3     0.7025    0.6368    0.6681       749\n",
            "           4     0.6484    0.7942    0.7139       520\n",
            "           5     0.6476    0.6909    0.6685      1210\n",
            "\n",
            "    accuracy                         0.6850      4699\n",
            "   macro avg     0.6935    0.6715    0.6746      4699\n",
            "weighted avg     0.6865    0.6850    0.6814      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5217    0.3333    0.4068       144\n",
            "           1     0.7833    0.6490    0.7098       245\n",
            "           2     0.4803    0.6667    0.5583       384\n",
            "           3     0.6273    0.5941    0.6103       170\n",
            "           4     0.6931    0.4682    0.5589       299\n",
            "           5     0.5486    0.6220    0.5830       381\n",
            "\n",
            "    accuracy                         0.5798      1623\n",
            "   macro avg     0.6091    0.5556    0.5712      1623\n",
            "weighted avg     0.6003    0.5798    0.5791      1623\n",
            "\n",
            "0.81084804640462 0.6814454372019211 0.49581016351338897 0.5790948023062804\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.8196878433227539\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 7 loss average: 0.696\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7152    0.5878    0.6453       376\n",
            "           1     0.8557    0.8770    0.8662       764\n",
            "           2     0.6769    0.6944    0.6856      1080\n",
            "           3     0.7654    0.6796    0.7199       749\n",
            "           4     0.7282    0.8038    0.7642       520\n",
            "           5     0.6976    0.7264    0.7117      1210\n",
            "\n",
            "    accuracy                         0.7336      4699\n",
            "   macro avg     0.7398    0.7282    0.7321      4699\n",
            "weighted avg     0.7342    0.7336    0.7326      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4667    0.4375    0.4516       144\n",
            "           1     0.6820    0.7265    0.7036       245\n",
            "           2     0.5464    0.5521    0.5492       384\n",
            "           3     0.6092    0.6235    0.6163       170\n",
            "           4     0.6593    0.4983    0.5676       299\n",
            "           5     0.5581    0.6430    0.5976       381\n",
            "\n",
            "    accuracy                         0.5872      1623\n",
            "   macro avg     0.5869    0.5802    0.5810      1623\n",
            "weighted avg     0.5899    0.5872    0.5856      1623\n",
            "\n",
            "0.6957518470784029 0.7326241716421829 0.4982244556329001 0.5856202235221903\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.5859885215759277\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 8 loss average: 0.610\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7341    0.6463    0.6874       376\n",
            "           1     0.8589    0.8927    0.8755       764\n",
            "           2     0.7430    0.7333    0.7381      1080\n",
            "           3     0.8167    0.7557    0.7850       749\n",
            "           4     0.7561    0.8346    0.7934       520\n",
            "           5     0.7558    0.7752    0.7654      1210\n",
            "\n",
            "    accuracy                         0.7778      4699\n",
            "   macro avg     0.7775    0.7730    0.7741      4699\n",
            "weighted avg     0.7776    0.7778    0.7770      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4874    0.4028    0.4411       144\n",
            "           1     0.7461    0.5878    0.6575       245\n",
            "           2     0.4848    0.7057    0.5748       384\n",
            "           3     0.5517    0.6588    0.6005       170\n",
            "           4     0.7386    0.4348    0.5474       299\n",
            "           5     0.5898    0.5774    0.5836       381\n",
            "\n",
            "    accuracy                         0.5761      1623\n",
            "   macro avg     0.5997    0.5612    0.5675      1623\n",
            "weighted avg     0.6029    0.5761    0.5751      1623\n",
            "\n",
            "0.6098968476677934 0.777015516518608 0.49006996437933936 0.575111586032464\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.46887487173080444\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 9 loss average: 0.526\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7500    0.6702    0.7079       376\n",
            "           1     0.8873    0.8966    0.8919       764\n",
            "           2     0.7914    0.7972    0.7943      1080\n",
            "           3     0.8388    0.8198    0.8292       749\n",
            "           4     0.7780    0.8423    0.8089       520\n",
            "           5     0.8005    0.7992    0.7998      1210\n",
            "\n",
            "    accuracy                         0.8123      4699\n",
            "   macro avg     0.8077    0.8042    0.8053      4699\n",
            "weighted avg     0.8121    0.8123    0.8118      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6596    0.2153    0.3246       144\n",
            "           1     0.7028    0.7143    0.7085       245\n",
            "           2     0.4822    0.6719    0.5615       384\n",
            "           3     0.6139    0.5706    0.5915       170\n",
            "           4     0.6421    0.5819    0.6105       299\n",
            "           5     0.6061    0.5774    0.5914       381\n",
            "\n",
            "    accuracy                         0.5884      1623\n",
            "   macro avg     0.6178    0.5552    0.5647      1623\n",
            "weighted avg     0.6036    0.5884    0.5819      1623\n",
            "\n",
            "0.5259834388270974 0.8118471306030008 0.48486827566236596 0.5818567164888232\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.2632659375667572\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 10 loss average: 0.434\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7892    0.7367    0.7620       376\n",
            "           1     0.9185    0.8848    0.9013       764\n",
            "           2     0.8123    0.8537    0.8325      1080\n",
            "           3     0.8554    0.8451    0.8502       749\n",
            "           4     0.8271    0.8558    0.8412       520\n",
            "           5     0.8374    0.8298    0.8335      1210\n",
            "\n",
            "    accuracy                         0.8421      4699\n",
            "   macro avg     0.8400    0.8343    0.8368      4699\n",
            "weighted avg     0.8427    0.8421    0.8421      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.3611    0.4194       144\n",
            "           1     0.6047    0.6367    0.6203       245\n",
            "           2     0.4901    0.5156    0.5025       384\n",
            "           3     0.5638    0.6235    0.5922       170\n",
            "           4     0.7079    0.4214    0.5283       299\n",
            "           5     0.5173    0.6667    0.5826       381\n",
            "\n",
            "    accuracy                         0.5496      1623\n",
            "   macro avg     0.5640    0.5375    0.5409      1623\n",
            "weighted avg     0.5625    0.5496    0.5459      1623\n",
            "\n",
            "0.43417767466356355 0.8421131842288474 0.48125972760243974 0.5458541452416086\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.12613831460475922\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 11 loss average: 0.383\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8378    0.8245    0.8311       376\n",
            "           1     0.9241    0.9241    0.9241       764\n",
            "           2     0.8435    0.8481    0.8458      1080\n",
            "           3     0.8598    0.8598    0.8598       749\n",
            "           4     0.8757    0.8673    0.8715       520\n",
            "           5     0.8469    0.8504    0.8487      1210\n",
            "\n",
            "    accuracy                         0.8632      4699\n",
            "   macro avg     0.8646    0.8624    0.8635      4699\n",
            "weighted avg     0.8632    0.8632    0.8632      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5325    0.2847    0.3710       144\n",
            "           1     0.6642    0.7265    0.6940       245\n",
            "           2     0.4785    0.6953    0.5669       384\n",
            "           3     0.5500    0.5824    0.5657       170\n",
            "           4     0.6641    0.5686    0.6126       299\n",
            "           5     0.6162    0.4593    0.5263       381\n",
            "\n",
            "    accuracy                         0.5730      1623\n",
            "   macro avg     0.5842    0.5528    0.5561      1623\n",
            "weighted avg     0.5853    0.5730    0.5675      1623\n",
            "\n",
            "0.383211284254988 0.8631651406558979 0.5227629800134705 0.5674675302681919\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.3413642346858978\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 12 loss average: 0.372\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8820    0.7154    0.7900       376\n",
            "           1     0.9281    0.9463    0.9371       764\n",
            "           2     0.8400    0.8509    0.8454      1080\n",
            "           3     0.8813    0.8625    0.8718       749\n",
            "           4     0.8372    0.9096    0.8719       520\n",
            "           5     0.8585    0.8678    0.8631      1210\n",
            "\n",
            "    accuracy                         0.8683      4699\n",
            "   macro avg     0.8712    0.8588    0.8632      4699\n",
            "weighted avg     0.8687    0.8683    0.8676      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5294    0.3125    0.3930       144\n",
            "           1     0.6770    0.6245    0.6497       245\n",
            "           2     0.4909    0.6354    0.5539       384\n",
            "           3     0.5450    0.6059    0.5738       170\n",
            "           4     0.6443    0.5452    0.5906       299\n",
            "           5     0.5550    0.5433    0.5491       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5736    0.5445    0.5517      1623\n",
            "weighted avg     0.5714    0.5638    0.5618      1623\n",
            "\n",
            "0.3716821308868627 0.8675985043646266 0.5104367635391005 0.5617978885103119\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.15541917085647583\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 13 loss average: 0.296\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8333    0.8245    0.8289       376\n",
            "           1     0.9186    0.9450    0.9316       764\n",
            "           2     0.9172    0.8926    0.9047      1080\n",
            "           3     0.8861    0.9039    0.8949       749\n",
            "           4     0.8848    0.8865    0.8857       520\n",
            "           5     0.8921    0.8884    0.8903      1210\n",
            "\n",
            "    accuracy                         0.8957      4699\n",
            "   macro avg     0.8887    0.8902    0.8893      4699\n",
            "weighted avg     0.8957    0.8957    0.8956      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4419    0.3958    0.4176       144\n",
            "           1     0.7268    0.5755    0.6424       245\n",
            "           2     0.4573    0.6693    0.5433       384\n",
            "           3     0.4676    0.5941    0.5233       170\n",
            "           4     0.7010    0.4548    0.5517       299\n",
            "           5     0.5701    0.4908    0.5275       381\n",
            "\n",
            "    accuracy                         0.5416      1623\n",
            "   macro avg     0.5608    0.5301    0.5343      1623\n",
            "weighted avg     0.5691    0.5416    0.5429      1623\n",
            "\n",
            "0.2958442255233725 0.8956373131171474 0.4993420678032239 0.5428607982369191\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.21971046924591064\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 14 loss average: 0.252\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8862    0.8697    0.8779       376\n",
            "           1     0.9567    0.9555    0.9561       764\n",
            "           2     0.8997    0.9139    0.9068      1080\n",
            "           3     0.8952    0.9012    0.8982       749\n",
            "           4     0.9251    0.9269    0.9260       520\n",
            "           5     0.9038    0.8926    0.8981      1210\n",
            "\n",
            "    accuracy                         0.9110      4699\n",
            "   macro avg     0.9111    0.9100    0.9105      4699\n",
            "weighted avg     0.9110    0.9110    0.9110      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4351    0.3958    0.4145       144\n",
            "           1     0.6735    0.6735    0.6735       245\n",
            "           2     0.5054    0.6094    0.5525       384\n",
            "           3     0.5363    0.5647    0.5501       170\n",
            "           4     0.6303    0.5017    0.5587       299\n",
            "           5     0.5613    0.5407    0.5508       381\n",
            "\n",
            "    accuracy                         0.5595      1623\n",
            "   macro avg     0.5570    0.5476    0.5500      1623\n",
            "weighted avg     0.5639    0.5595    0.5590      1623\n",
            "\n",
            "0.2519776332192123 0.9110173634188499 0.5059737183308096 0.5590193199201916\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.10890638828277588\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 15 loss average: 0.227\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9136    0.8723    0.8925       376\n",
            "           1     0.9514    0.9490    0.9502       764\n",
            "           2     0.9028    0.9120    0.9074      1080\n",
            "           3     0.9128    0.9359    0.9242       749\n",
            "           4     0.9305    0.9519    0.9411       520\n",
            "           5     0.9217    0.9041    0.9128      1210\n",
            "\n",
            "    accuracy                         0.9210      4699\n",
            "   macro avg     0.9221    0.9209    0.9214      4699\n",
            "weighted avg     0.9211    0.9210    0.9210      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4884    0.2917    0.3652       144\n",
            "           1     0.6608    0.6122    0.6356       245\n",
            "           2     0.4487    0.6380    0.5269       384\n",
            "           3     0.4928    0.6059    0.5435       170\n",
            "           4     0.6322    0.5117    0.5656       299\n",
            "           5     0.5687    0.4672    0.5130       381\n",
            "\n",
            "    accuracy                         0.5367      1623\n",
            "   macro avg     0.5486    0.5211    0.5250      1623\n",
            "weighted avg     0.5508    0.5367    0.5346      1623\n",
            "\n",
            "0.22653083293698728 0.9209654980485165 0.4803072542468023 0.5345633699627343\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.28400763869285583\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 16 loss average: 0.204\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9103    0.9176    0.9139       376\n",
            "           1     0.9657    0.9581    0.9619       764\n",
            "           2     0.9220    0.9306    0.9263      1080\n",
            "           3     0.9160    0.9172    0.9166       749\n",
            "           4     0.9276    0.9365    0.9321       520\n",
            "           5     0.9215    0.9116    0.9165      1210\n",
            "\n",
            "    accuracy                         0.9276      4699\n",
            "   macro avg     0.9272    0.9286    0.9279      4699\n",
            "weighted avg     0.9277    0.9276    0.9277      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5333    0.2778    0.3653       144\n",
            "           1     0.6311    0.6286    0.6299       245\n",
            "           2     0.4988    0.5469    0.5217       384\n",
            "           3     0.5172    0.6176    0.5630       170\n",
            "           4     0.6496    0.5084    0.5704       299\n",
            "           5     0.5247    0.6142    0.5659       381\n",
            "\n",
            "    accuracy                         0.5514      1623\n",
            "   macro avg     0.5591    0.5322    0.5360      1623\n",
            "weighted avg     0.5576    0.5514    0.5478      1623\n",
            "\n",
            "0.20417747095537683 0.9276552965468545 0.48683233637382856 0.5478254823531742\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.0984831377863884\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 17 loss average: 0.186\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9155    0.8644    0.8892       376\n",
            "           1     0.9552    0.9764    0.9657       764\n",
            "           2     0.9326    0.9352    0.9339      1080\n",
            "           3     0.9253    0.9266    0.9260       749\n",
            "           4     0.9286    0.9500    0.9392       520\n",
            "           5     0.9374    0.9281    0.9327      1210\n",
            "\n",
            "    accuracy                         0.9347      4699\n",
            "   macro avg     0.9324    0.9301    0.9311      4699\n",
            "weighted avg     0.9345    0.9347    0.9345      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3918    0.5278    0.4497       144\n",
            "           1     0.6105    0.6653    0.6367       245\n",
            "           2     0.5179    0.5286    0.5232       384\n",
            "           3     0.5526    0.6176    0.5833       170\n",
            "           4     0.6359    0.3913    0.4845       299\n",
            "           5     0.5631    0.5853    0.5740       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5453    0.5527    0.5419      1623\n",
            "weighted avg     0.5567    0.5465    0.5449      1623\n",
            "\n",
            "0.18554297826873759 0.9345021638859893 0.4561448509985625 0.5449041878702032\n",
            "Patience counter: 11\n",
            "Done! It took 4e+02 secs\n",
            "\n",
            "Current RUN: 1\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0596952699124813\n",
            "Best test f1 weighted\n",
            "0.5790948023062804\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.3 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8893332481384277\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 1 loss average: 1.780\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.2351    0.3455    0.2798       764\n",
            "           2     0.2528    0.2074    0.2279      1080\n",
            "           3     0.1429    0.0027    0.0052       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2398    0.5298    0.3302      1210\n",
            "\n",
            "    accuracy                         0.2407      4699\n",
            "   macro avg     0.1451    0.1809    0.1405      4699\n",
            "weighted avg     0.1809    0.2407    0.1837      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3565    0.1068    0.1643       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2467    0.9764    0.3939       381\n",
            "\n",
            "    accuracy                         0.2545      1623\n",
            "   macro avg     0.1005    0.1805    0.0930      1623\n",
            "weighted avg     0.1423    0.2545    0.1313      1623\n",
            "\n",
            "1.7804701551795006 0.18371881236673412 0.11896631528902969 0.1313386036968761\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.9706416130065918\n",
            "100%|███████████████████████████████████████████| 48/48 [00:18<00:00,  2.66it/s]\n",
            "Epoch 2 loss average: 1.706\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.3129    0.3454    0.3283      1080\n",
            "           3     1.0000    0.0013    0.0027       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2792    0.8083    0.4150      1210\n",
            "\n",
            "    accuracy                         0.2877      4699\n",
            "   macro avg     0.2654    0.1925    0.1243      4699\n",
            "weighted avg     0.3032    0.2877    0.1828      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.4103    0.0653    0.1127       245\n",
            "           2     0.3151    0.5391    0.3977       384\n",
            "           3     1.0000    0.0118    0.0233       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3449    0.8373    0.4885       381\n",
            "\n",
            "    accuracy                         0.3352      1623\n",
            "   macro avg     0.3450    0.2422    0.1704      1623\n",
            "weighted avg     0.3222    0.3352    0.2282      1623\n",
            "\n",
            "1.7059542884429295 0.18275951622508546 0.18294797810773286 0.2282180298584534\n",
            "Patience counter: 1\n",
            "Epoch: 3, iter 0: loss = 1.5582935810089111\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 3 loss average: 1.533\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.6875    0.1584    0.2574       764\n",
            "           2     0.3137    0.5315    0.3945      1080\n",
            "           3     0.6606    0.2417    0.3539       749\n",
            "           4     0.3659    0.1442    0.2069       520\n",
            "           5     0.4002    0.7322    0.5175      1210\n",
            "\n",
            "    accuracy                         0.3909      4699\n",
            "   macro avg     0.4046    0.3013    0.2884      4699\n",
            "weighted avg     0.4327    0.3909    0.3451      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2143    0.1250    0.1579       144\n",
            "           1     0.3266    0.5959    0.4220       245\n",
            "           2     0.4644    0.5260    0.4933       384\n",
            "           3     0.8077    0.3706    0.5081       170\n",
            "           4     0.6667    0.0201    0.0390       299\n",
            "           5     0.4772    0.7139    0.5720       381\n",
            "\n",
            "    accuracy                         0.4356      1623\n",
            "   macro avg     0.4928    0.3919    0.3654      1623\n",
            "weighted avg     0.4976    0.4356    0.3891      1623\n",
            "\n",
            "1.532564473648866 0.3450910254378381 0.3337749809574017 0.38909619622525154\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.1547706127166748\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 4 loss average: 1.224\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4247    0.2473    0.3126       376\n",
            "           1     0.6534    0.6466    0.6500       764\n",
            "           2     0.4711    0.4222    0.4453      1080\n",
            "           3     0.6128    0.4713    0.5328       749\n",
            "           4     0.5573    0.5615    0.5594       520\n",
            "           5     0.5163    0.7066    0.5967      1210\n",
            "\n",
            "    accuracy                         0.5412      4699\n",
            "   macro avg     0.5393    0.5093    0.5161      4699\n",
            "weighted avg     0.5408    0.5412    0.5335      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.8209    0.4490    0.5805       245\n",
            "           2     0.3920    0.7422    0.5131       384\n",
            "           3     0.7905    0.4882    0.6036       170\n",
            "           4     0.5985    0.5284    0.5613       299\n",
            "           5     0.5649    0.5827    0.5736       381\n",
            "\n",
            "    accuracy                         0.5287      1623\n",
            "   macro avg     0.5278    0.4651    0.4720      1623\n",
            "weighted avg     0.5423    0.5287    0.5103      1623\n",
            "\n",
            "1.2237929503122966 0.5335167051677041 0.40865004345248446 0.5103060760744329\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.8666936159133911\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 5 loss average: 1.014\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5820    0.1888    0.2851       376\n",
            "           1     0.6977    0.7552    0.7253       764\n",
            "           2     0.5067    0.5639    0.5337      1080\n",
            "           3     0.7248    0.4887    0.5837       749\n",
            "           4     0.5440    0.5346    0.5393       520\n",
            "           5     0.5796    0.7339    0.6477      1210\n",
            "\n",
            "    accuracy                         0.5935      4699\n",
            "   macro avg     0.6058    0.5442    0.5525      4699\n",
            "weighted avg     0.6014    0.5935    0.5829      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4079    0.4306    0.4189       144\n",
            "           1     0.7384    0.7143    0.7261       245\n",
            "           2     0.5291    0.5911    0.5584       384\n",
            "           3     0.4902    0.7353    0.5882       170\n",
            "           4     0.6183    0.5418    0.5775       299\n",
            "           5     0.6181    0.4672    0.5321       381\n",
            "\n",
            "    accuracy                         0.5724      1623\n",
            "   macro avg     0.5670    0.5800    0.5669      1623\n",
            "weighted avg     0.5832    0.5724    0.5718      1623\n",
            "\n",
            "1.0141345386703808 0.5829258711203497 0.44851750149258734 0.571838077382233\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.8439006209373474\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 6 loss average: 0.875\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5881    0.5239    0.5541       376\n",
            "           1     0.7625    0.8194    0.7899       764\n",
            "           2     0.6321    0.5694    0.5991      1080\n",
            "           3     0.6784    0.6195    0.6476       749\n",
            "           4     0.6246    0.6942    0.6576       520\n",
            "           5     0.6422    0.6942    0.6672      1210\n",
            "\n",
            "    accuracy                         0.6604      4699\n",
            "   macro avg     0.6546    0.6534    0.6526      4699\n",
            "weighted avg     0.6589    0.6604    0.6583      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3850    0.5347    0.4477       144\n",
            "           1     0.5850    0.8571    0.6954       245\n",
            "           2     0.5345    0.6458    0.5849       384\n",
            "           3     0.7007    0.6059    0.6498       170\n",
            "           4     0.6471    0.3311    0.4381       299\n",
            "           5     0.6567    0.5171    0.5786       381\n",
            "\n",
            "    accuracy                         0.5755      1623\n",
            "   macro avg     0.5848    0.5820    0.5657      1623\n",
            "weighted avg     0.5957    0.5755    0.5677      1623\n",
            "\n",
            "0.8751781079918146 0.6582646885658149 0.46811551892435016 0.5676622973273653\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.7737496495246887\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 7 loss average: 0.764\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7352    0.5612    0.6365       376\n",
            "           1     0.8065    0.8455    0.8256       764\n",
            "           2     0.6566    0.6833    0.6697      1080\n",
            "           3     0.7800    0.6155    0.6881       749\n",
            "           4     0.6872    0.7942    0.7368       520\n",
            "           5     0.6772    0.7248    0.7002      1210\n",
            "\n",
            "    accuracy                         0.7121      4699\n",
            "   macro avg     0.7238    0.7041    0.7095      4699\n",
            "weighted avg     0.7156    0.7121    0.7106      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4941    0.2917    0.3668       144\n",
            "           1     0.6814    0.6286    0.6539       245\n",
            "           2     0.4846    0.5729    0.5251       384\n",
            "           3     0.6135    0.5882    0.6006       170\n",
            "           4     0.6667    0.4348    0.5263       299\n",
            "           5     0.5220    0.6850    0.5925       381\n",
            "\n",
            "    accuracy                         0.5588      1623\n",
            "   macro avg     0.5770    0.5335    0.5442      1623\n",
            "weighted avg     0.5710    0.5588    0.5545      1623\n",
            "\n",
            "0.7637604586780071 0.710592542219349 0.4558888511425614 0.5544500636144875\n",
            "Patience counter: 2\n",
            "Epoch: 8, iter 0: loss = 0.5838457345962524\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 8 loss average: 0.646\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7330    0.6862    0.7088       376\n",
            "           1     0.8094    0.8835    0.8448       764\n",
            "           2     0.7413    0.7139    0.7274      1080\n",
            "           3     0.8038    0.7383    0.7697       749\n",
            "           4     0.7339    0.7904    0.7611       520\n",
            "           5     0.7527    0.7620    0.7573      1210\n",
            "\n",
            "    accuracy                         0.7640      4699\n",
            "   macro avg     0.7623    0.7624    0.7615      4699\n",
            "weighted avg     0.7638    0.7640    0.7632      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5333    0.3889    0.4498       144\n",
            "           1     0.7079    0.5837    0.6398       245\n",
            "           2     0.4553    0.5573    0.5012       384\n",
            "           3     0.5803    0.6588    0.6171       170\n",
            "           4     0.7417    0.3746    0.4978       299\n",
            "           5     0.5239    0.6903    0.5957       381\n",
            "\n",
            "    accuracy                         0.5545      1623\n",
            "   macro avg     0.5904    0.5423    0.5502      1623\n",
            "weighted avg     0.5823    0.5545    0.5512      1623\n",
            "\n",
            "0.6458762443313996 0.7631532581899724 0.47660171049123906 0.5512485424511718\n",
            "Patience counter: 3\n",
            "Epoch: 9, iter 0: loss = 0.46895357966423035\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 9 loss average: 0.564\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7486    0.6888    0.7175       376\n",
            "           1     0.8676    0.8665    0.8671       764\n",
            "           2     0.7704    0.7861    0.7782      1080\n",
            "           3     0.8106    0.7770    0.7935       749\n",
            "           4     0.7974    0.8173    0.8072       520\n",
            "           5     0.7833    0.8008    0.7920      1210\n",
            "\n",
            "    accuracy                         0.7972      4699\n",
            "   macro avg     0.7963    0.7894    0.7926      4699\n",
            "weighted avg     0.7972    0.7972    0.7970      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5556    0.2431    0.3382       144\n",
            "           1     0.5546    0.7878    0.6509       245\n",
            "           2     0.5129    0.5182    0.5155       384\n",
            "           3     0.4846    0.6471    0.5542       170\n",
            "           4     0.6066    0.5518    0.5779       299\n",
            "           5     0.5877    0.5013    0.5411       381\n",
            "\n",
            "    accuracy                         0.5502      1623\n",
            "   macro avg     0.5503    0.5415    0.5296      1623\n",
            "weighted avg     0.5548    0.5502    0.5418      1623\n",
            "\n",
            "0.5642925233890613 0.7969770277336672 0.45444943879261945 0.5417748557178005\n",
            "Patience counter: 4\n",
            "Epoch: 10, iter 0: loss = 0.3577384054660797\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 10 loss average: 0.468\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7723    0.7846    0.7784       376\n",
            "           1     0.8759    0.9149    0.8950       764\n",
            "           2     0.8228    0.8083    0.8155      1080\n",
            "           3     0.8508    0.8224    0.8364       749\n",
            "           4     0.8382    0.8269    0.8325       520\n",
            "           5     0.8215    0.8289    0.8252      1210\n",
            "\n",
            "    accuracy                         0.8334      4699\n",
            "   macro avg     0.8302    0.8310    0.8305      4699\n",
            "weighted avg     0.8332    0.8334    0.8332      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4568    0.5139    0.4837       144\n",
            "           1     0.6137    0.6939    0.6513       245\n",
            "           2     0.4897    0.5547    0.5201       384\n",
            "           3     0.6121    0.5941    0.6030       170\n",
            "           4     0.6122    0.4013    0.4848       299\n",
            "           5     0.5902    0.6010    0.5956       381\n",
            "\n",
            "    accuracy                         0.5588      1623\n",
            "   macro avg     0.5625    0.5598    0.5564      1623\n",
            "weighted avg     0.5645    0.5588    0.5566      1623\n",
            "\n",
            "0.46827804017812014 0.8331618107750826 0.48069253799548955 0.5565955027231995\n",
            "Patience counter: 5\n",
            "Epoch: 11, iter 0: loss = 0.29935482144355774\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 11 loss average: 0.431\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8039    0.7633    0.7831       376\n",
            "           1     0.8947    0.9123    0.9034       764\n",
            "           2     0.8546    0.8324    0.8433      1080\n",
            "           3     0.8486    0.8304    0.8394       749\n",
            "           4     0.8287    0.8654    0.8467       520\n",
            "           5     0.8389    0.8562    0.8474      1210\n",
            "\n",
            "    accuracy                         0.8493      4699\n",
            "   macro avg     0.8449    0.8433    0.8439      4699\n",
            "weighted avg     0.8492    0.8493    0.8491      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4031    0.5347    0.4597       144\n",
            "           1     0.5803    0.7224    0.6436       245\n",
            "           2     0.4737    0.6562    0.5502       384\n",
            "           3     0.5357    0.6176    0.5738       170\n",
            "           4     0.7059    0.2408    0.3591       299\n",
            "           5     0.6296    0.4908    0.5516       381\n",
            "\n",
            "    accuracy                         0.5360      1623\n",
            "   macro avg     0.5547    0.5438    0.5230      1623\n",
            "weighted avg     0.5694    0.5360    0.5239      1623\n",
            "\n",
            "0.4306879037370284 0.8490861654370214 0.46120083219547253 0.5238770567361916\n",
            "Patience counter: 6\n",
            "Epoch: 12, iter 0: loss = 0.32095766067504883\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 12 loss average: 0.354\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8387    0.8298    0.8342       376\n",
            "           1     0.9144    0.9228    0.9186       764\n",
            "           2     0.8653    0.8685    0.8669      1080\n",
            "           3     0.8806    0.8665    0.8735       749\n",
            "           4     0.8752    0.8500    0.8624       520\n",
            "           5     0.8683    0.8826    0.8754      1210\n",
            "\n",
            "    accuracy                         0.8755      4699\n",
            "   macro avg     0.8738    0.8700    0.8718      4699\n",
            "weighted avg     0.8755    0.8755    0.8754      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4044    0.3819    0.3929       144\n",
            "           1     0.6959    0.5510    0.6150       245\n",
            "           2     0.4481    0.6745    0.5385       384\n",
            "           3     0.5157    0.6765    0.5852       170\n",
            "           4     0.6645    0.3445    0.4537       299\n",
            "           5     0.5697    0.5039    0.5348       381\n",
            "\n",
            "    accuracy                         0.5293      1623\n",
            "   macro avg     0.5497    0.5221    0.5200      1623\n",
            "weighted avg     0.5571    0.5293    0.5255      1623\n",
            "\n",
            "0.35390351867924136 0.8754362095696321 0.4689255256778711 0.52553958411828\n",
            "Patience counter: 7\n",
            "Epoch: 13, iter 0: loss = 0.5009366869926453\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 13 loss average: 0.309\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8482    0.8617    0.8549       376\n",
            "           1     0.9251    0.9372    0.9311       764\n",
            "           2     0.8883    0.8759    0.8821      1080\n",
            "           3     0.8797    0.8692    0.8744       749\n",
            "           4     0.8987    0.9038    0.9012       520\n",
            "           5     0.8815    0.8851    0.8833      1210\n",
            "\n",
            "    accuracy                         0.8891      4699\n",
            "   macro avg     0.8869    0.8888    0.8878      4699\n",
            "weighted avg     0.8891    0.8891    0.8891      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4264    0.3819    0.4029       144\n",
            "           1     0.6379    0.6327    0.6352       245\n",
            "           2     0.4658    0.6198    0.5318       384\n",
            "           3     0.5469    0.6176    0.5801       170\n",
            "           4     0.6330    0.3980    0.4887       299\n",
            "           5     0.5806    0.5486    0.5641       381\n",
            "\n",
            "    accuracy                         0.5428      1623\n",
            "   macro avg     0.5484    0.5331    0.5338      1623\n",
            "weighted avg     0.5545    0.5428    0.5407      1623\n",
            "\n",
            "0.3088008346967399 0.889076416849358 0.47609589540574176 0.5406963780797263\n",
            "Patience counter: 8\n",
            "Epoch: 14, iter 0: loss = 0.06949431449174881\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 14 loss average: 0.277\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8811    0.8670    0.8740       376\n",
            "           1     0.9152    0.9463    0.9305       764\n",
            "           2     0.8828    0.9065    0.8945      1080\n",
            "           3     0.9193    0.8825    0.9005       749\n",
            "           4     0.9201    0.9077    0.9138       520\n",
            "           5     0.8982    0.8893    0.8937      1210\n",
            "\n",
            "    accuracy                         0.9017      4699\n",
            "   macro avg     0.9028    0.8999    0.9012      4699\n",
            "weighted avg     0.9018    0.9017    0.9016      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3789    0.5000    0.4311       144\n",
            "           1     0.5986    0.6816    0.6374       245\n",
            "           2     0.5090    0.5130    0.5110       384\n",
            "           3     0.5261    0.6529    0.5827       170\n",
            "           4     0.6018    0.4448    0.5115       299\n",
            "           5     0.6000    0.5276    0.5615       381\n",
            "\n",
            "    accuracy                         0.5428      1623\n",
            "   macro avg     0.5357    0.5533    0.5392      1623\n",
            "weighted avg     0.5512    0.5428    0.5425      1623\n",
            "\n",
            "0.2770962839325269 0.9016012686353443 0.47325823488558205 0.5424522140722234\n",
            "Patience counter: 9\n",
            "Epoch: 15, iter 0: loss = 0.2161492109298706\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 15 loss average: 0.240\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8846    0.9176    0.9008       376\n",
            "           1     0.9357    0.9529    0.9442       764\n",
            "           2     0.9213    0.9102    0.9157      1080\n",
            "           3     0.8992    0.9172    0.9081       749\n",
            "           4     0.9521    0.9173    0.9344       520\n",
            "           5     0.9116    0.9033    0.9074      1210\n",
            "\n",
            "    accuracy                         0.9179      4699\n",
            "   macro avg     0.9174    0.9197    0.9184      4699\n",
            "weighted avg     0.9181    0.9179    0.9179      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4730    0.4861    0.4795       144\n",
            "           1     0.5360    0.7592    0.6284       245\n",
            "           2     0.4874    0.5052    0.4962       384\n",
            "           3     0.4913    0.6647    0.5650       170\n",
            "           4     0.7079    0.4214    0.5283       299\n",
            "           5     0.5745    0.4856    0.5263       381\n",
            "\n",
            "    accuracy                         0.5385      1623\n",
            "   macro avg     0.5450    0.5537    0.5373      1623\n",
            "weighted avg     0.5549    0.5385    0.5348      1623\n",
            "\n",
            "0.23969724820926785 0.9178747462665595 0.4699280989381602 0.5348485722449042\n",
            "Patience counter: 10\n",
            "Epoch: 16, iter 0: loss = 0.06434452533721924\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 16 loss average: 0.214\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9176    0.9176    0.9176       376\n",
            "           1     0.9516    0.9516    0.9516       764\n",
            "           2     0.9260    0.9148    0.9204      1080\n",
            "           3     0.9092    0.9226    0.9158       749\n",
            "           4     0.9319    0.9481    0.9399       520\n",
            "           5     0.9252    0.9198    0.9225      1210\n",
            "\n",
            "    accuracy                         0.9272      4699\n",
            "   macro avg     0.9269    0.9291    0.9280      4699\n",
            "weighted avg     0.9272    0.9272    0.9272      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4744    0.2569    0.3333       144\n",
            "           1     0.6022    0.6612    0.6304       245\n",
            "           2     0.4377    0.6771    0.5317       384\n",
            "           3     0.5944    0.5000    0.5431       170\n",
            "           4     0.6170    0.4849    0.5431       299\n",
            "           5     0.5888    0.4698    0.5226       381\n",
            "\n",
            "    accuracy                         0.5348      1623\n",
            "   macro avg     0.5524    0.5083    0.5174      1623\n",
            "weighted avg     0.5507    0.5348    0.5302      1623\n",
            "\n",
            "0.21363310112307468 0.9272066840779317 0.45745150676769014 0.5301535984065624\n",
            "Patience counter: 11\n",
            "Done! It took 3.7e+02 secs\n",
            "\n",
            "Current RUN: 2\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0728933215141296\n",
            "Best test f1 weighted\n",
            "0.571838077382233\n",
            "Best epoch\n",
            "5\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.3 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.732494831085205\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 1 loss average: 1.750\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1093    0.1217    0.1152       764\n",
            "           2     0.2189    0.1843    0.2001      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2491    0.6050    0.3529      1210\n",
            "\n",
            "    accuracy                         0.2179      4699\n",
            "   macro avg     0.0962    0.1518    0.1114      4699\n",
            "weighted avg     0.1322    0.2179    0.1556      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2699    0.8464    0.4093       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3962    0.4357    0.4150       381\n",
            "\n",
            "    accuracy                         0.3025      1623\n",
            "   macro avg     0.1110    0.2137    0.1374      1623\n",
            "weighted avg     0.1569    0.3025    0.1943      1623\n",
            "\n",
            "1.749924769004186 0.15557662974049513 0.17512511298289787 0.19426607597647796\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.5662716627120972\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.70it/s]\n",
            "Epoch 2 loss average: 1.704\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.3128    0.3157    0.3143      1080\n",
            "           3     0.5428    0.1949    0.2868       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2977    0.8215    0.4370      1210\n",
            "\n",
            "    accuracy                         0.3152      4699\n",
            "   macro avg     0.1922    0.2220    0.1730      4699\n",
            "weighted avg     0.2351    0.3152    0.2305      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.8667    0.0531    0.1000       245\n",
            "           2     0.2636    0.3021    0.2816       384\n",
            "           3     0.3333    0.7471    0.4610       170\n",
            "           4     0.7308    0.0635    0.1169       299\n",
            "           5     0.3417    0.6824    0.4553       381\n",
            "\n",
            "    accuracy                         0.3296      1623\n",
            "   macro avg     0.4227    0.3080    0.2358      1623\n",
            "weighted avg     0.4429    0.3296    0.2584      1623\n",
            "\n",
            "1.7038492088516553 0.23048788619246588 0.2364230269253358 0.2584277417500581\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.4664177894592285\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 3 loss average: 1.470\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.7515    0.1662    0.2722       764\n",
            "           2     0.3279    0.4444    0.3774      1080\n",
            "           3     0.4781    0.4806    0.4794       749\n",
            "           4     0.3268    0.3865    0.3542       520\n",
            "           5     0.4231    0.5934    0.4940      1210\n",
            "\n",
            "    accuracy                         0.4014      4699\n",
            "   macro avg     0.3846    0.3452    0.3295      4699\n",
            "weighted avg     0.4189    0.4014    0.3738      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.3083    0.8531    0.4529       245\n",
            "           2     0.6730    0.3698    0.4773       384\n",
            "           3     0.8161    0.4176    0.5525       170\n",
            "           4     0.6143    0.4582    0.5249       299\n",
            "           5     0.5377    0.5984    0.5665       381\n",
            "\n",
            "    accuracy                         0.4849      1623\n",
            "   macro avg     0.4916    0.4495    0.4290      1623\n",
            "weighted avg     0.5307    0.4849    0.4688      1623\n",
            "\n",
            "1.4698462386926014 0.37379696602317836 0.3321758894812295 0.4688467325924554\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.3687162399291992\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 4 loss average: 1.155\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8537    0.0931    0.1679       376\n",
            "           1     0.6194    0.7435    0.6758       764\n",
            "           2     0.4971    0.3972    0.4416      1080\n",
            "           3     0.6259    0.4646    0.5333       749\n",
            "           4     0.5592    0.8173    0.6641       520\n",
            "           5     0.4936    0.6372    0.5563      1210\n",
            "\n",
            "    accuracy                         0.5482      4699\n",
            "   macro avg     0.6081    0.5255    0.5065      4699\n",
            "weighted avg     0.5720    0.5482    0.5265      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8333    0.0347    0.0667       144\n",
            "           1     0.8121    0.5469    0.6537       245\n",
            "           2     0.4807    0.5521    0.5139       384\n",
            "           3     0.8229    0.4647    0.5940       170\n",
            "           4     0.5768    0.7157    0.6388       299\n",
            "           5     0.5110    0.7297    0.6011       381\n",
            "\n",
            "    accuracy                         0.5681      1623\n",
            "   macro avg     0.6728    0.5073    0.5114      1623\n",
            "weighted avg     0.6227    0.5681    0.5472      1623\n",
            "\n",
            "1.1546994832654793 0.5265388600964485 0.4322025570002871 0.5471912441846477\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.8011040091514587\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.75it/s]\n",
            "Epoch 5 loss average: 0.936\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6320    0.3883    0.4811       376\n",
            "           1     0.7568    0.7984    0.7771       764\n",
            "           2     0.5765    0.5306    0.5526      1080\n",
            "           3     0.7212    0.5594    0.6301       749\n",
            "           4     0.6292    0.7212    0.6720       520\n",
            "           5     0.5842    0.7198    0.6449      1210\n",
            "\n",
            "    accuracy                         0.6372      4699\n",
            "   macro avg     0.6500    0.6196    0.6263      4699\n",
            "weighted avg     0.6411    0.6372    0.6327      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5636    0.2153    0.3116       144\n",
            "           1     0.7864    0.6612    0.7184       245\n",
            "           2     0.5031    0.6354    0.5616       384\n",
            "           3     0.7400    0.6529    0.6937       170\n",
            "           4     0.6215    0.6589    0.6396       299\n",
            "           5     0.5854    0.6299    0.6068       381\n",
            "\n",
            "    accuracy                         0.6069      1623\n",
            "   macro avg     0.6333    0.5756    0.5886      1623\n",
            "weighted avg     0.6172    0.6069    0.6019      1623\n",
            "\n",
            "0.9359548725187778 0.6327072948323786 0.4713831440674185 0.6019076865603565\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.976327121257782\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 6 loss average: 0.815\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7328    0.4814    0.5811       376\n",
            "           1     0.8002    0.8390    0.8192       764\n",
            "           2     0.6453    0.6046    0.6243      1080\n",
            "           3     0.7708    0.5928    0.6702       749\n",
            "           4     0.6646    0.8115    0.7307       520\n",
            "           5     0.6345    0.7488    0.6869      1210\n",
            "\n",
            "    accuracy                         0.6910      4699\n",
            "   macro avg     0.7080    0.6797    0.6854      4699\n",
            "weighted avg     0.6968    0.6910    0.6877      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4315    0.4375    0.4345       144\n",
            "           1     0.7739    0.6286    0.6937       245\n",
            "           2     0.4754    0.7057    0.5681       384\n",
            "           3     0.7786    0.6000    0.6777       170\n",
            "           4     0.6927    0.4749    0.5635       299\n",
            "           5     0.6129    0.5984    0.6056       381\n",
            "\n",
            "    accuracy                         0.5915      1623\n",
            "   macro avg     0.6275    0.5742    0.5905      1623\n",
            "weighted avg     0.6206    0.5915    0.5946      1623\n",
            "\n",
            "0.8149547049154838 0.6877278075956603 0.4965054580331798 0.5946452049528572\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.9192394018173218\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 7 loss average: 0.719\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6242    0.5080    0.5601       376\n",
            "           1     0.8153    0.8901    0.8511       764\n",
            "           2     0.7053    0.7046    0.7050      1080\n",
            "           3     0.7227    0.6716    0.6962       749\n",
            "           4     0.6714    0.7346    0.7016       520\n",
            "           5     0.6922    0.6950    0.6936      1210\n",
            "\n",
            "    accuracy                         0.7146      4699\n",
            "   macro avg     0.7052    0.7006    0.7013      4699\n",
            "weighted avg     0.7123    0.7146    0.7124      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5455    0.2917    0.3801       144\n",
            "           1     0.6458    0.7143    0.6783       245\n",
            "           2     0.5321    0.5391    0.5356       384\n",
            "           3     0.6713    0.5647    0.6134       170\n",
            "           4     0.6822    0.5385    0.6019       299\n",
            "           5     0.5385    0.7165    0.6149       381\n",
            "\n",
            "    accuracy                         0.5878      1623\n",
            "   macro avg     0.6026    0.5608    0.5707      1623\n",
            "weighted avg     0.5942    0.5878    0.5823      1623\n",
            "\n",
            "0.7191809900105 0.7124273713046544 0.49536018190976683 0.5823042545883207\n",
            "Patience counter: 2\n",
            "Epoch: 8, iter 0: loss = 0.6983304619789124\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 8 loss average: 0.628\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6868    0.6649    0.6757       376\n",
            "           1     0.8646    0.8691    0.8668       764\n",
            "           2     0.7549    0.7528    0.7538      1080\n",
            "           3     0.7925    0.7290    0.7594       749\n",
            "           4     0.7452    0.7481    0.7466       520\n",
            "           5     0.7310    0.7727    0.7513      1210\n",
            "\n",
            "    accuracy                         0.7655      4699\n",
            "   macro avg     0.7625    0.7561    0.7589      4699\n",
            "weighted avg     0.7660    0.7655    0.7654      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4684    0.2569    0.3318       144\n",
            "           1     0.6722    0.6612    0.6667       245\n",
            "           2     0.4669    0.6432    0.5411       384\n",
            "           3     0.6710    0.6118    0.6400       170\n",
            "           4     0.6289    0.5385    0.5802       299\n",
            "           5     0.5950    0.5669    0.5806       381\n",
            "\n",
            "    accuracy                         0.5712      1623\n",
            "   macro avg     0.5837    0.5464    0.5567      1623\n",
            "weighted avg     0.5793    0.5712    0.5683      1623\n",
            "\n",
            "0.6278487623979648 0.7653896704848101 0.5080747040805551 0.5683240583177143\n",
            "Patience counter: 3\n",
            "Epoch: 9, iter 0: loss = 0.5877085328102112\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 9 loss average: 0.545\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7864    0.7048    0.7433       376\n",
            "           1     0.8856    0.9018    0.8936       764\n",
            "           2     0.7800    0.7880    0.7840      1080\n",
            "           3     0.7936    0.7650    0.7791       749\n",
            "           4     0.7744    0.8250    0.7989       520\n",
            "           5     0.7765    0.7810    0.7787      1210\n",
            "\n",
            "    accuracy                         0.7985      4699\n",
            "   macro avg     0.7994    0.7943    0.7963      4699\n",
            "weighted avg     0.7983    0.7985    0.7981      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4286    0.4375    0.4330       144\n",
            "           1     0.6624    0.6408    0.6515       245\n",
            "           2     0.5262    0.6016    0.5614       384\n",
            "           3     0.7077    0.5412    0.6133       170\n",
            "           4     0.6364    0.5385    0.5833       299\n",
            "           5     0.5851    0.6404    0.6115       381\n",
            "\n",
            "    accuracy                         0.5841      1623\n",
            "   macro avg     0.5911    0.5667    0.5757      1623\n",
            "weighted avg     0.5912    0.5841    0.5848      1623\n",
            "\n",
            "0.5453726444393396 0.7980716074472533 0.5277345084271783 0.5848396272769968\n",
            "Patience counter: 4\n",
            "Epoch: 10, iter 0: loss = 0.3923491835594177\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 10 loss average: 0.450\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8246    0.7500    0.7855       376\n",
            "           1     0.8957    0.9215    0.9084       764\n",
            "           2     0.8375    0.8157    0.8265      1080\n",
            "           3     0.8479    0.8264    0.8371       749\n",
            "           4     0.8036    0.8654    0.8333       520\n",
            "           5     0.8242    0.8372    0.8307      1210\n",
            "\n",
            "    accuracy                         0.8404      4699\n",
            "   macro avg     0.8389    0.8360    0.8369      4699\n",
            "weighted avg     0.8404    0.8404    0.8400      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4423    0.4792    0.4600       144\n",
            "           1     0.5750    0.7510    0.6513       245\n",
            "           2     0.5075    0.6146    0.5559       384\n",
            "           3     0.6228    0.6118    0.6172       170\n",
            "           4     0.7333    0.3679    0.4900       299\n",
            "           5     0.6192    0.5932    0.6059       381\n",
            "\n",
            "    accuracy                         0.5724      1623\n",
            "   macro avg     0.5834    0.5696    0.5634      1623\n",
            "weighted avg     0.5918    0.5724    0.5678      1623\n",
            "\n",
            "0.4497949415817857 0.8400352921610441 0.49909394671726426 0.5678223425424768\n",
            "Patience counter: 5\n",
            "Epoch: 11, iter 0: loss = 0.305429607629776\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 11 loss average: 0.377\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8444    0.7793    0.8105       376\n",
            "           1     0.9250    0.9045    0.9146       764\n",
            "           2     0.8461    0.8704    0.8581      1080\n",
            "           3     0.8659    0.8531    0.8594       749\n",
            "           4     0.8407    0.8731    0.8566       520\n",
            "           5     0.8512    0.8554    0.8533      1210\n",
            "\n",
            "    accuracy                         0.8623      4699\n",
            "   macro avg     0.8622    0.8559    0.8588      4699\n",
            "weighted avg     0.8626    0.8623    0.8623      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4539    0.4444    0.4491       144\n",
            "           1     0.6246    0.7265    0.6717       245\n",
            "           2     0.5172    0.5885    0.5505       384\n",
            "           3     0.5775    0.6353    0.6050       170\n",
            "           4     0.5915    0.5619    0.5763       299\n",
            "           5     0.6194    0.4698    0.5343       381\n",
            "\n",
            "    accuracy                         0.5687      1623\n",
            "   macro avg     0.5640    0.5711    0.5645      1623\n",
            "weighted avg     0.5718    0.5687    0.5665      1623\n",
            "\n",
            "0.37655449596544105 0.8622745348329184 0.49460122060154527 0.5664873162627407\n",
            "Patience counter: 6\n",
            "Epoch: 12, iter 0: loss = 0.12153703719377518\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 12 loss average: 0.332\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8792    0.8324    0.8552       376\n",
            "           1     0.9160    0.9568    0.9360       764\n",
            "           2     0.8794    0.8713    0.8753      1080\n",
            "           3     0.8889    0.8865    0.8877       749\n",
            "           4     0.8771    0.8923    0.8847       520\n",
            "           5     0.8799    0.8719    0.8759      1210\n",
            "\n",
            "    accuracy                         0.8870      4699\n",
            "   macro avg     0.8868    0.8852    0.8858      4699\n",
            "weighted avg     0.8867    0.8870    0.8867      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4643    0.3611    0.4063       144\n",
            "           1     0.5606    0.6612    0.6067       245\n",
            "           2     0.4827    0.6536    0.5553       384\n",
            "           3     0.5588    0.6706    0.6096       170\n",
            "           4     0.6514    0.4749    0.5493       299\n",
            "           5     0.6321    0.4646    0.5356       381\n",
            "\n",
            "    accuracy                         0.5533      1623\n",
            "   macro avg     0.5583    0.5477    0.5438      1623\n",
            "weighted avg     0.5669    0.5533    0.5498      1623\n",
            "\n",
            "0.33167711133137345 0.8867293302693471 0.4982402441836473 0.5497966451428656\n",
            "Patience counter: 7\n",
            "Epoch: 13, iter 0: loss = 0.31756356358528137\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 13 loss average: 0.285\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8583    0.8378    0.8479       376\n",
            "           1     0.9285    0.9516    0.9399       764\n",
            "           2     0.8911    0.8861    0.8886      1080\n",
            "           3     0.8857    0.8999    0.8927       749\n",
            "           4     0.8806    0.9077    0.8939       520\n",
            "           5     0.9109    0.8868    0.8987      1210\n",
            "\n",
            "    accuracy                         0.8976      4699\n",
            "   macro avg     0.8925    0.8950    0.8936      4699\n",
            "weighted avg     0.8976    0.8976    0.8975      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4296    0.4028    0.4158       144\n",
            "           1     0.5524    0.7102    0.6214       245\n",
            "           2     0.4977    0.5755    0.5338       384\n",
            "           3     0.5459    0.6647    0.5995       170\n",
            "           4     0.7565    0.2910    0.4203       299\n",
            "           5     0.5725    0.6115    0.5914       381\n",
            "\n",
            "    accuracy                         0.5459      1623\n",
            "   macro avg     0.5591    0.5426    0.5304      1623\n",
            "weighted avg     0.5702    0.5459    0.5360      1623\n",
            "\n",
            "0.284907338830332 0.8975150476337007 0.4517722132565979 0.5360413670484923\n",
            "Patience counter: 8\n",
            "Epoch: 14, iter 0: loss = 0.21794818341732025\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 14 loss average: 0.267\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8571    0.8777    0.8673       376\n",
            "           1     0.9287    0.9542    0.9413       764\n",
            "           2     0.9053    0.8935    0.8993      1080\n",
            "           3     0.9032    0.9092    0.9062       749\n",
            "           4     0.9085    0.8788    0.8935       520\n",
            "           5     0.9080    0.9050    0.9065      1210\n",
            "\n",
            "    accuracy                         0.9059      4699\n",
            "   macro avg     0.9018    0.9031    0.9023      4699\n",
            "weighted avg     0.9059    0.9059    0.9059      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5172    0.3125    0.3896       144\n",
            "           1     0.6183    0.6082    0.6132       245\n",
            "           2     0.4938    0.6198    0.5497       384\n",
            "           3     0.6000    0.5824    0.5910       170\n",
            "           4     0.6360    0.5084    0.5651       299\n",
            "           5     0.5575    0.5984    0.5772       381\n",
            "\n",
            "    accuracy                         0.5613      1623\n",
            "   macro avg     0.5705    0.5383    0.5476      1623\n",
            "weighted avg     0.5669    0.5613    0.5587      1623\n",
            "\n",
            "0.26732208657388884 0.9058632259340339 0.50355204294319 0.5586848307411109\n",
            "Patience counter: 9\n",
            "Epoch: 15, iter 0: loss = 0.2552546560764313\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 15 loss average: 0.219\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8828    0.8617    0.8721       376\n",
            "           1     0.9506    0.9568    0.9537       764\n",
            "           2     0.9081    0.9241    0.9160      1080\n",
            "           3     0.9134    0.9292    0.9212       749\n",
            "           4     0.9110    0.9058    0.9084       520\n",
            "           5     0.9359    0.9165    0.9261      1210\n",
            "\n",
            "    accuracy                         0.9213      4699\n",
            "   macro avg     0.9170    0.9157    0.9163      4699\n",
            "weighted avg     0.9213    0.9213    0.9212      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4904    0.3542    0.4113       144\n",
            "           1     0.5606    0.6612    0.6067       245\n",
            "           2     0.5392    0.4661    0.5000       384\n",
            "           3     0.4901    0.7294    0.5863       170\n",
            "           4     0.5866    0.4983    0.5389       299\n",
            "           5     0.5269    0.5407    0.5337       381\n",
            "\n",
            "    accuracy                         0.5367      1623\n",
            "   macro avg     0.5323    0.5417    0.5295      1623\n",
            "weighted avg     0.5388    0.5367    0.5323      1623\n",
            "\n",
            "0.2191789772671958 0.9212149300708168 0.4953676014589014 0.5323492931334353\n",
            "Patience counter: 10\n",
            "Epoch: 16, iter 0: loss = 0.2456420511007309\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 16 loss average: 0.204\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9189    0.9043    0.9115       376\n",
            "           1     0.9603    0.9503    0.9553       764\n",
            "           2     0.9258    0.9241    0.9249      1080\n",
            "           3     0.9069    0.9239    0.9153       749\n",
            "           4     0.9264    0.9442    0.9352       520\n",
            "           5     0.9185    0.9124    0.9154      1210\n",
            "\n",
            "    accuracy                         0.9259      4699\n",
            "   macro avg     0.9261    0.9265    0.9263      4699\n",
            "weighted avg     0.9260    0.9259    0.9260      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5301    0.3056    0.3877       144\n",
            "           1     0.6174    0.6653    0.6405       245\n",
            "           2     0.5011    0.5807    0.5380       384\n",
            "           3     0.5300    0.6765    0.5943       170\n",
            "           4     0.5748    0.5652    0.5700       299\n",
            "           5     0.5750    0.4829    0.5250       381\n",
            "\n",
            "    accuracy                         0.5533      1623\n",
            "   macro avg     0.5547    0.5460    0.5426      1623\n",
            "weighted avg     0.5552    0.5533    0.5489      1623\n",
            "\n",
            "0.20411124530558786 0.9259541734905017 0.4870494955329491 0.5488603478733871\n",
            "Patience counter: 11\n",
            "Done! It took 3.7e+02 secs\n",
            "\n",
            "Current RUN: 3\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.997035276144743\n",
            "Best test f1 weighted\n",
            "0.6019076865603565\n",
            "Best epoch\n",
            "5\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.3 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8317692279815674\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 1 loss average: 1.765\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.2124    0.1073    0.1426       764\n",
            "           2     0.2383    0.1269    0.1656      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.1606    0.0423    0.0670       520\n",
            "           5     0.2524    0.7496    0.3776      1210\n",
            "\n",
            "    accuracy                         0.2443      4699\n",
            "   macro avg     0.1439    0.1710    0.1255      4699\n",
            "weighted avg     0.1721    0.2443    0.1659      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2366    1.0000    0.3827       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.0000    0.0000    0.0000       381\n",
            "\n",
            "    accuracy                         0.2366      1623\n",
            "   macro avg     0.0394    0.1667    0.0638      1623\n",
            "weighted avg     0.0560    0.2366    0.0905      1623\n",
            "\n",
            "1.7648782059550285 0.1658821129578988 0.07909632291642447 0.09053709429197439\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.9874577522277832\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.70it/s]\n",
            "Epoch 2 loss average: 1.682\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.3049    0.2107    0.2492       764\n",
            "           2     0.2503    0.4352    0.3178      1080\n",
            "           3     1.0000    0.0013    0.0027       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2962    0.5612    0.3878      1210\n",
            "\n",
            "    accuracy                         0.2790      4699\n",
            "   macro avg     0.3086    0.2014    0.1596      4699\n",
            "weighted avg     0.3428    0.2790    0.2138      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2316    0.2135    0.2222       384\n",
            "           3     0.9200    0.1353    0.2359       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2934    0.9580    0.4492       381\n",
            "\n",
            "    accuracy                         0.2896      1623\n",
            "   macro avg     0.2408    0.2178    0.1512      1623\n",
            "weighted avg     0.2200    0.2896    0.1827      1623\n",
            "\n",
            "1.6824490825335185 0.2138375716417258 0.15342949993907223 0.18274357394505267\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.2225223779678345\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 3 loss average: 1.509\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.3777    0.3599    0.3686       764\n",
            "           2     0.3546    0.3917    0.3722      1080\n",
            "           3     0.4643    0.3124    0.3735       749\n",
            "           4     0.3899    0.1192    0.1826       520\n",
            "           5     0.3901    0.6818    0.4962      1210\n",
            "\n",
            "    accuracy                         0.3871      4699\n",
            "   macro avg     0.3294    0.3108    0.2989      4699\n",
            "weighted avg     0.3605    0.3871    0.3530      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.8889    0.1306    0.2278       245\n",
            "           2     0.4397    0.6745    0.5324       384\n",
            "           3     0.6490    0.5765    0.6106       170\n",
            "           4     0.4722    0.7391    0.5763       299\n",
            "           5     0.5330    0.5302    0.5316       381\n",
            "\n",
            "    accuracy                         0.5003      1623\n",
            "   macro avg     0.4971    0.4418    0.4131      1623\n",
            "weighted avg     0.5183    0.5003    0.4552      1623\n",
            "\n",
            "1.508825960258643 0.3530059270348498 0.40058314849851234 0.45524932419354397\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.3225781917572021\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 4 loss average: 1.153\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5738    0.0931    0.1602       376\n",
            "           1     0.6923    0.7304    0.7108       764\n",
            "           2     0.4812    0.4861    0.4836      1080\n",
            "           3     0.6844    0.4633    0.5525       749\n",
            "           4     0.5348    0.7096    0.6099       520\n",
            "           5     0.5492    0.7008    0.6158      1210\n",
            "\n",
            "    accuracy                         0.5708      4699\n",
            "   macro avg     0.5860    0.5305    0.5222      4699\n",
            "weighted avg     0.5788    0.5708    0.5537      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2604    0.1736    0.2083       144\n",
            "           1     0.7537    0.6245    0.6830       245\n",
            "           2     0.5942    0.5339    0.5624       384\n",
            "           3     0.7623    0.5471    0.6370       170\n",
            "           4     0.5300    0.6789    0.5953       299\n",
            "           5     0.5549    0.6903    0.6152       381\n",
            "\n",
            "    accuracy                         0.5804      1623\n",
            "   macro avg     0.5759    0.5414    0.5502      1623\n",
            "weighted avg     0.5852    0.5804    0.5755      1623\n",
            "\n",
            "1.1531087619562943 0.5536950035129501 0.4517556637917888 0.5754704555311139\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.1257646083831787\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 5 loss average: 0.945\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5469    0.4495    0.4934       376\n",
            "           1     0.7861    0.7984    0.7922       764\n",
            "           2     0.6057    0.5333    0.5672      1080\n",
            "           3     0.7457    0.5247    0.6160       749\n",
            "           4     0.6168    0.7615    0.6816       520\n",
            "           5     0.6091    0.7521    0.6731      1210\n",
            "\n",
            "    accuracy                         0.6499      4699\n",
            "   macro avg     0.6517    0.6366    0.6372      4699\n",
            "weighted avg     0.6547    0.6499    0.6456      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3801    0.4514    0.4127       144\n",
            "           1     0.7038    0.7469    0.7248       245\n",
            "           2     0.5332    0.6693    0.5935       384\n",
            "           3     0.6792    0.6353    0.6565       170\n",
            "           4     0.6624    0.5184    0.5816       299\n",
            "           5     0.6372    0.5302    0.5788       381\n",
            "\n",
            "    accuracy                         0.5977      1623\n",
            "   macro avg     0.5993    0.5919    0.5913      1623\n",
            "weighted avg     0.6089    0.5977    0.5982      1623\n",
            "\n",
            "0.9450026340782642 0.6455804866514715 0.49179917540365714 0.5982407013133665\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.9249548316001892\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 6 loss average: 0.795\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6532    0.5160    0.5765       376\n",
            "           1     0.8127    0.8063    0.8095       764\n",
            "           2     0.6673    0.6481    0.6576      1080\n",
            "           3     0.7661    0.6515    0.7042       749\n",
            "           4     0.6424    0.7635    0.6977       520\n",
            "           5     0.6866    0.7603    0.7216      1210\n",
            "\n",
            "    accuracy                         0.7055      4699\n",
            "   macro avg     0.7047    0.6910    0.6945      4699\n",
            "weighted avg     0.7078    0.7055    0.7041      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4848    0.3333    0.3951       144\n",
            "           1     0.5807    0.7633    0.6596       245\n",
            "           2     0.5436    0.6328    0.5848       384\n",
            "           3     0.4770    0.6706    0.5575       170\n",
            "           4     0.6352    0.4950    0.5564       299\n",
            "           5     0.6007    0.4462    0.5120       381\n",
            "\n",
            "    accuracy                         0.5607      1623\n",
            "   macro avg     0.5537    0.5569    0.5442      1623\n",
            "weighted avg     0.5673    0.5607    0.5541      1623\n",
            "\n",
            "0.7948712203651667 0.7041366153552919 0.48554033852276046 0.5540914567786847\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.670798659324646\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 7 loss average: 0.695\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6950    0.5878    0.6369       376\n",
            "           1     0.8314    0.8652    0.8480       764\n",
            "           2     0.7107    0.7028    0.7067      1080\n",
            "           3     0.7705    0.6903    0.7282       749\n",
            "           4     0.7030    0.8058    0.7509       520\n",
            "           5     0.7306    0.7554    0.7428      1210\n",
            "\n",
            "    accuracy                         0.7429      4699\n",
            "   macro avg     0.7402    0.7345    0.7356      4699\n",
            "weighted avg     0.7429    0.7429    0.7417      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4188    0.3403    0.3755       144\n",
            "           1     0.6728    0.7469    0.7079       245\n",
            "           2     0.5216    0.6302    0.5708       384\n",
            "           3     0.5670    0.6471    0.6044       170\n",
            "           4     0.6230    0.6355    0.6291       299\n",
            "           5     0.6384    0.4541    0.5307       381\n",
            "\n",
            "    accuracy                         0.5835      1623\n",
            "   macro avg     0.5736    0.5757    0.5697      1623\n",
            "weighted avg     0.5861    0.5835    0.5790      1623\n",
            "\n",
            "0.6952776542554299 0.7416905044015347 0.4921062355639312 0.5790071897534242\n",
            "Patience counter: 2\n",
            "Epoch: 8, iter 0: loss = 0.8014938831329346\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 8 loss average: 0.598\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7147    0.6729    0.6932       376\n",
            "           1     0.8538    0.9018    0.8771       764\n",
            "           2     0.7680    0.7417    0.7546      1080\n",
            "           3     0.8091    0.7583    0.7829       749\n",
            "           4     0.7398    0.8038    0.7705       520\n",
            "           5     0.7728    0.7843    0.7785      1210\n",
            "\n",
            "    accuracy                         0.7827      4699\n",
            "   macro avg     0.7764    0.7771    0.7761      4699\n",
            "weighted avg     0.7823    0.7827    0.7820      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4898    0.3333    0.3967       144\n",
            "           1     0.6894    0.6612    0.6750       245\n",
            "           2     0.4593    0.6901    0.5515       384\n",
            "           3     0.7014    0.5941    0.6433       170\n",
            "           4     0.6830    0.5117    0.5851       299\n",
            "           5     0.6116    0.5538    0.5813       381\n",
            "\n",
            "    accuracy                         0.5792      1623\n",
            "   macro avg     0.6057    0.5574    0.5721      1623\n",
            "weighted avg     0.5991    0.5792    0.5792      1623\n",
            "\n",
            "0.5977802944059173 0.7820347539477371 0.5072641279164032 0.5792020685297606\n",
            "Patience counter: 3\n",
            "Epoch: 9, iter 0: loss = 0.4551053047180176\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 9 loss average: 0.524\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8101    0.6809    0.7399       376\n",
            "           1     0.8714    0.8783    0.8748       764\n",
            "           2     0.7864    0.7944    0.7904      1080\n",
            "           3     0.8458    0.7984    0.8214       749\n",
            "           4     0.7978    0.8500    0.8231       520\n",
            "           5     0.7883    0.8215    0.8045      1210\n",
            "\n",
            "    accuracy                         0.8127      4699\n",
            "   macro avg     0.8167    0.8039    0.8090      4699\n",
            "weighted avg     0.8133    0.8127    0.8123      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5357    0.2083    0.3000       144\n",
            "           1     0.7119    0.6857    0.6985       245\n",
            "           2     0.4882    0.5938    0.5358       384\n",
            "           3     0.5852    0.6059    0.5954       170\n",
            "           4     0.6124    0.6288    0.6205       299\n",
            "           5     0.5748    0.5748    0.5748       381\n",
            "\n",
            "    accuracy                         0.5767      1623\n",
            "   macro avg     0.5847    0.5495    0.5542      1623\n",
            "weighted avg     0.5796    0.5767    0.5704      1623\n",
            "\n",
            "0.5235083969309926 0.8122933780322569 0.48739535611389717 0.5704486186003803\n",
            "Patience counter: 4\n",
            "Epoch: 10, iter 0: loss = 0.4251471757888794\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 10 loss average: 0.447\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7857    0.7314    0.7576       376\n",
            "           1     0.8900    0.9110    0.9004       764\n",
            "           2     0.8316    0.7954    0.8131      1080\n",
            "           3     0.8445    0.8411    0.8428       749\n",
            "           4     0.8004    0.8712    0.8343       520\n",
            "           5     0.8380    0.8463    0.8421      1210\n",
            "\n",
            "    accuracy                         0.8378      4699\n",
            "   macro avg     0.8317    0.8327    0.8317      4699\n",
            "weighted avg     0.8377    0.8378    0.8374      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5052    0.3403    0.4066       144\n",
            "           1     0.5783    0.7837    0.6655       245\n",
            "           2     0.5139    0.6276    0.5651       384\n",
            "           3     0.6012    0.5765    0.5886       170\n",
            "           4     0.6041    0.4950    0.5441       299\n",
            "           5     0.5899    0.4908    0.5358       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5654    0.5523    0.5510      1623\n",
            "weighted avg     0.5664    0.5638    0.5579      1623\n",
            "\n",
            "0.4474780281695227 0.8373857599618854 0.49486093396015546 0.5579102924032047\n",
            "Patience counter: 5\n",
            "Epoch: 11, iter 0: loss = 0.3169758915901184\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 11 loss average: 0.380\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8238    0.8085    0.8161       376\n",
            "           1     0.9005    0.9359    0.9178       764\n",
            "           2     0.8525    0.8509    0.8517      1080\n",
            "           3     0.8698    0.8652    0.8675       749\n",
            "           4     0.8444    0.8769    0.8604       520\n",
            "           5     0.8679    0.8413    0.8544      1210\n",
            "\n",
            "    accuracy                         0.8640      4699\n",
            "   macro avg     0.8598    0.8631    0.8613      4699\n",
            "weighted avg     0.8638    0.8640    0.8638      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6327    0.2153    0.3212       144\n",
            "           1     0.6846    0.6735    0.6790       245\n",
            "           2     0.4551    0.5938    0.5153       384\n",
            "           3     0.4656    0.6765    0.5516       170\n",
            "           4     0.6170    0.5819    0.5990       299\n",
            "           5     0.5644    0.4488    0.5000       381\n",
            "\n",
            "    accuracy                         0.5447      1623\n",
            "   macro avg     0.5699    0.5316    0.5277      1623\n",
            "weighted avg     0.5621    0.5447    0.5384      1623\n",
            "\n",
            "0.38043244555592537 0.8637747887763727 0.46866596602401445 0.5384047622970354\n",
            "Patience counter: 6\n",
            "Epoch: 12, iter 0: loss = 0.24478496611118317\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 12 loss average: 0.332\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8460    0.8617    0.8538       376\n",
            "           1     0.9204    0.9228    0.9216       764\n",
            "           2     0.8687    0.8639    0.8663      1080\n",
            "           3     0.8930    0.8798    0.8863       749\n",
            "           4     0.9033    0.8981    0.9007       520\n",
            "           5     0.8780    0.8860    0.8819      1210\n",
            "\n",
            "    accuracy                         0.8853      4699\n",
            "   macro avg     0.8849    0.8854    0.8851      4699\n",
            "weighted avg     0.8854    0.8853    0.8853      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6167    0.2569    0.3627       144\n",
            "           1     0.6231    0.6816    0.6511       245\n",
            "           2     0.4664    0.5964    0.5234       384\n",
            "           3     0.4912    0.6588    0.5628       170\n",
            "           4     0.6293    0.5452    0.5842       299\n",
            "           5     0.5647    0.4698    0.5129       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5652    0.5348    0.5329      1623\n",
            "weighted avg     0.5591    0.5465    0.5413      1623\n",
            "\n",
            "0.3317635430333515 0.8853084538444758 0.4743107941715453 0.5412939761767076\n",
            "Patience counter: 7\n",
            "Epoch: 13, iter 0: loss = 0.0853661373257637\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 13 loss average: 0.290\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8732    0.8059    0.8382       376\n",
            "           1     0.9272    0.9332    0.9302       764\n",
            "           2     0.8891    0.8833    0.8862      1080\n",
            "           3     0.8863    0.9052    0.8956       749\n",
            "           4     0.8673    0.9173    0.8916       520\n",
            "           5     0.8937    0.8826    0.8881      1210\n",
            "\n",
            "    accuracy                         0.8923      4699\n",
            "   macro avg     0.8895    0.8879    0.8883      4699\n",
            "weighted avg     0.8923    0.8923    0.8921      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4071    0.3958    0.4014       144\n",
            "           1     0.6318    0.6653    0.6481       245\n",
            "           2     0.4966    0.5755    0.5332       384\n",
            "           3     0.5538    0.6059    0.5787       170\n",
            "           4     0.5959    0.4883    0.5368       299\n",
            "           5     0.5645    0.5171    0.5397       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5416    0.5413    0.5396      1623\n",
            "weighted avg     0.5493    0.5465    0.5458      1623\n",
            "\n",
            "0.2899028185444574 0.8921157491697373 0.4949850330400901 0.5457962921129813\n",
            "Patience counter: 8\n",
            "Epoch: 14, iter 0: loss = 0.24851645529270172\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 14 loss average: 0.259\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8389    0.8723    0.8553       376\n",
            "           1     0.9346    0.9346    0.9346       764\n",
            "           2     0.8932    0.9065    0.8998      1080\n",
            "           3     0.9147    0.9025    0.9086       749\n",
            "           4     0.9182    0.8635    0.8900       520\n",
            "           5     0.9049    0.9124    0.9086      1210\n",
            "\n",
            "    accuracy                         0.9044      4699\n",
            "   macro avg     0.9008    0.8986    0.8995      4699\n",
            "weighted avg     0.9048    0.9044    0.9045      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5932    0.2431    0.3448       144\n",
            "           1     0.6349    0.6245    0.6296       245\n",
            "           2     0.4688    0.6250    0.5357       384\n",
            "           3     0.5440    0.6176    0.5785       170\n",
            "           4     0.6015    0.5452    0.5719       299\n",
            "           5     0.5648    0.5144    0.5385       381\n",
            "\n",
            "    accuracy                         0.5496      1623\n",
            "   macro avg     0.5679    0.5283    0.5332      1623\n",
            "weighted avg     0.5598    0.5496    0.5448      1623\n",
            "\n",
            "0.2594022032960008 0.9044863833794704 0.46343519667426436 0.5447545831935061\n",
            "Patience counter: 9\n",
            "Epoch: 15, iter 0: loss = 0.13221698999404907\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 15 loss average: 0.231\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9052    0.8378    0.8702       376\n",
            "           1     0.9504    0.9529    0.9516       764\n",
            "           2     0.9117    0.9176    0.9146      1080\n",
            "           3     0.9048    0.9386    0.9214       749\n",
            "           4     0.8972    0.9231    0.9100       520\n",
            "           5     0.9309    0.9124    0.9215      1210\n",
            "\n",
            "    accuracy                         0.9196      4699\n",
            "   macro avg     0.9167    0.9137    0.9149      4699\n",
            "weighted avg     0.9197    0.9196    0.9194      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5200    0.2708    0.3562       144\n",
            "           1     0.5942    0.6694    0.6296       245\n",
            "           2     0.4454    0.6589    0.5315       384\n",
            "           3     0.5642    0.5941    0.5788       170\n",
            "           4     0.6498    0.4716    0.5465       299\n",
            "           5     0.5877    0.4751    0.5254       381\n",
            "\n",
            "    accuracy                         0.5416      1623\n",
            "   macro avg     0.5602    0.5233    0.5280      1623\n",
            "weighted avg     0.5580    0.5416    0.5370      1623\n",
            "\n",
            "0.23069150373339653 0.9194220451668312 0.47128977334792277 0.5370362382077297\n",
            "Patience counter: 10\n",
            "Epoch: 16, iter 0: loss = 0.27469396591186523\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 16 loss average: 0.208\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9086    0.8989    0.9037       376\n",
            "           1     0.9439    0.9476    0.9458       764\n",
            "           2     0.9197    0.9120    0.9159      1080\n",
            "           3     0.9022    0.9239    0.9129       749\n",
            "           4     0.9387    0.9423    0.9405       520\n",
            "           5     0.9200    0.9124    0.9162      1210\n",
            "\n",
            "    accuracy                         0.9221      4699\n",
            "   macro avg     0.9222    0.9229    0.9225      4699\n",
            "weighted avg     0.9221    0.9221    0.9221      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5062    0.2847    0.3644       144\n",
            "           1     0.6850    0.5592    0.6157       245\n",
            "           2     0.4570    0.5260    0.4891       384\n",
            "           3     0.4605    0.6176    0.5276       170\n",
            "           4     0.6465    0.4649    0.5409       299\n",
            "           5     0.4661    0.5591    0.5084       381\n",
            "\n",
            "    accuracy                         0.5157      1623\n",
            "   macro avg     0.5369    0.5019    0.5077      1623\n",
            "weighted avg     0.5332    0.5157    0.5152      1623\n",
            "\n",
            "0.20770642083759108 0.9220970844703019 0.4565633031403799 0.51524765036916\n",
            "Patience counter: 11\n",
            "Done! It took 3.7e+02 secs\n",
            "\n",
            "Current RUN: 4\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0234883204102516\n",
            "Best test f1 weighted\n",
            "0.5982407013133665\n",
            "Best epoch\n",
            "5\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7695825099945068\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 1 loss average: 1.760\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0909    0.0223    0.0358       764\n",
            "           2     0.2099    0.3648    0.2665      1080\n",
            "           3     0.0797    0.0147    0.0248       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2303    0.4752    0.3102      1210\n",
            "\n",
            "    accuracy                         0.2122      4699\n",
            "   macro avg     0.1018    0.1462    0.1062      4699\n",
            "weighted avg     0.1350    0.2122    0.1509      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2366    1.0000    0.3827       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.0000    0.0000    0.0000       381\n",
            "\n",
            "    accuracy                         0.2366      1623\n",
            "   macro avg     0.0394    0.1667    0.0638      1623\n",
            "weighted avg     0.0560    0.2366    0.0905      1623\n",
            "\n",
            "1.7595442334810893 0.1508975867966471 0.07909632291642447 0.09053709429197439\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.4908658266067505\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.72it/s]\n",
            "Epoch 2 loss average: 1.701\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2867    0.1880    0.2271      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2658    0.8769    0.4080      1210\n",
            "\n",
            "    accuracy                         0.2690      4699\n",
            "   macro avg     0.0921    0.1775    0.1058      4699\n",
            "weighted avg     0.1344    0.2690    0.1572      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2625    0.5052    0.3455       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     1.0000    0.0033    0.0067       299\n",
            "           5     0.3658    0.8478    0.5111       381\n",
            "\n",
            "    accuracy                         0.3192      1623\n",
            "   macro avg     0.2714    0.2261    0.1439      1623\n",
            "weighted avg     0.3322    0.3192    0.2029      1623\n",
            "\n",
            "1.7008535116910934 0.15724899726501743 0.1662717170754514 0.20294914777356443\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.5614982843399048\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.81it/s]\n",
            "Epoch 3 loss average: 1.483\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2500    0.1011    0.1439       376\n",
            "           1     0.3684    0.0458    0.0815       764\n",
            "           2     0.3148    0.3981    0.3516      1080\n",
            "           3     0.9180    0.0748    0.1383       749\n",
            "           4     0.4081    0.4058    0.4069       520\n",
            "           5     0.4027    0.8347    0.5433      1210\n",
            "\n",
            "    accuracy                         0.3788      4699\n",
            "   macro avg     0.4437    0.3100    0.2776      4699\n",
            "weighted avg     0.4474    0.3788    0.3126      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2683    0.5347    0.3573       144\n",
            "           1     0.7421    0.4816    0.5842       245\n",
            "           2     0.4091    0.7500    0.5294       384\n",
            "           3     0.8621    0.2941    0.4386       170\n",
            "           4     0.8696    0.0669    0.1242       299\n",
            "           5     0.5434    0.5591    0.5511       381\n",
            "\n",
            "    accuracy                         0.4720      1623\n",
            "   macro avg     0.6158    0.4477    0.4308      1623\n",
            "weighted avg     0.6107    0.4720    0.4433      1623\n",
            "\n",
            "1.4830858409404755 0.31255031788090715 0.3390132489332987 0.4433386189213262\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.3233824968338013\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 4 loss average: 1.179\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5197    0.2101    0.2992       376\n",
            "           1     0.7266    0.6505    0.6865       764\n",
            "           2     0.4698    0.4824    0.4760      1080\n",
            "           3     0.7444    0.2644    0.3901       749\n",
            "           4     0.5332    0.7577    0.6259       520\n",
            "           5     0.5066    0.7322    0.5989      1210\n",
            "\n",
            "    accuracy                         0.5480      4699\n",
            "   macro avg     0.5834    0.5162    0.5128      4699\n",
            "weighted avg     0.5758    0.5480    0.5306      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7667    0.1597    0.2644       144\n",
            "           1     0.6667    0.7020    0.6839       245\n",
            "           2     0.5235    0.4349    0.4751       384\n",
            "           3     0.6083    0.4294    0.5034       170\n",
            "           4     0.6733    0.6756    0.6745       299\n",
            "           5     0.4899    0.7664    0.5977       381\n",
            "\n",
            "    accuracy                         0.5724      1623\n",
            "   macro avg     0.6214    0.5280    0.5332      1623\n",
            "weighted avg     0.5953    0.5724    0.5564      1623\n",
            "\n",
            "1.1789166803161304 0.5306164048002514 0.3974723079065642 0.5564113681480396\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.1403117179870605\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.75it/s]\n",
            "Epoch 5 loss average: 0.987\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6213    0.3883    0.4779       376\n",
            "           1     0.7265    0.8207    0.7707       764\n",
            "           2     0.5567    0.4361    0.4891      1080\n",
            "           3     0.6689    0.4019    0.5021       749\n",
            "           4     0.5983    0.8077    0.6874       520\n",
            "           5     0.5702    0.7554    0.6498      1210\n",
            "\n",
            "    accuracy                         0.6127      4699\n",
            "   macro avg     0.6237    0.6017    0.5962      4699\n",
            "weighted avg     0.6154    0.6127    0.5994      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6562    0.2917    0.4038       144\n",
            "           1     0.8050    0.6571    0.7236       245\n",
            "           2     0.4360    0.7188    0.5428       384\n",
            "           3     0.6741    0.5353    0.5967       170\n",
            "           4     0.7524    0.5284    0.6208       299\n",
            "           5     0.5748    0.5748    0.5748       381\n",
            "\n",
            "    accuracy                         0.5835      1623\n",
            "   macro avg     0.6498    0.5510    0.5771      1623\n",
            "weighted avg     0.6271    0.5835    0.5853      1623\n",
            "\n",
            "0.9873864476879438 0.599399820094786 0.46215373460398584 0.5852919690690738\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.8473122715950012\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 6 loss average: 0.854\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6973    0.4840    0.5714       376\n",
            "           1     0.7847    0.8351    0.8091       764\n",
            "           2     0.6186    0.5963    0.6073      1080\n",
            "           3     0.7183    0.5514    0.6239       749\n",
            "           4     0.6624    0.8000    0.7247       520\n",
            "           5     0.6466    0.7380    0.6893      1210\n",
            "\n",
            "    accuracy                         0.6780      4699\n",
            "   macro avg     0.6880    0.6675    0.6710      4699\n",
            "weighted avg     0.6799    0.6780    0.6740      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5128    0.1389    0.2186       144\n",
            "           1     0.8085    0.6204    0.7021       245\n",
            "           2     0.4934    0.5859    0.5357       384\n",
            "           3     0.5808    0.6765    0.6250       170\n",
            "           4     0.6565    0.6455    0.6509       299\n",
            "           5     0.5379    0.6325    0.5814       381\n",
            "\n",
            "    accuracy                         0.5829      1623\n",
            "   macro avg     0.5983    0.5500    0.5523      1623\n",
            "weighted avg     0.5923    0.5829    0.5740      1623\n",
            "\n",
            "0.8535127776364485 0.6739901310002719 0.44971288517425045 0.5739978855129063\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.9173468351364136\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.80it/s]\n",
            "Epoch 7 loss average: 0.759\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6526    0.5745    0.6110       376\n",
            "           1     0.8342    0.8691    0.8513       764\n",
            "           2     0.6715    0.6454    0.6582      1080\n",
            "           3     0.7451    0.6088    0.6701       749\n",
            "           4     0.6817    0.7288    0.7045       520\n",
            "           5     0.6750    0.7620    0.7158      1210\n",
            "\n",
            "    accuracy                         0.7095      4699\n",
            "   macro avg     0.7100    0.6981    0.7018      4699\n",
            "weighted avg     0.7102    0.7095    0.7077      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3198    0.4931    0.3880       144\n",
            "           1     0.4893    0.8408    0.6186       245\n",
            "           2     0.4941    0.6589    0.5647       384\n",
            "           3     0.6474    0.5941    0.6196       170\n",
            "           4     0.7609    0.1171    0.2029       299\n",
            "           5     0.6391    0.4462    0.5255       381\n",
            "\n",
            "    accuracy                         0.5151      1623\n",
            "   macro avg     0.5584    0.5250    0.4866      1623\n",
            "weighted avg     0.5772    0.5151    0.4871      1623\n",
            "\n",
            "0.7591316513717175 0.7076686722173242 0.4557716602985058 0.48706594588098856\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.6182345151901245\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 8 loss average: 0.654\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6381    0.7314    0.6815       376\n",
            "           1     0.8348    0.8861    0.8597       764\n",
            "           2     0.7287    0.7361    0.7324      1080\n",
            "           3     0.7765    0.7049    0.7390       749\n",
            "           4     0.7716    0.7212    0.7455       520\n",
            "           5     0.7542    0.7479    0.7510      1210\n",
            "\n",
            "    accuracy                         0.7565      4699\n",
            "   macro avg     0.7506    0.7546    0.7515      4699\n",
            "weighted avg     0.7576    0.7565    0.7563      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5778    0.3611    0.4444       144\n",
            "           1     0.5328    0.7959    0.6383       245\n",
            "           2     0.6054    0.4714    0.5300       384\n",
            "           3     0.6335    0.6000    0.6163       170\n",
            "           4     0.6507    0.4983    0.5644       299\n",
            "           5     0.5460    0.6850    0.6077       381\n",
            "\n",
            "    accuracy                         0.5792      1623\n",
            "   macro avg     0.5910    0.5686    0.5669      1623\n",
            "weighted avg     0.5893    0.5792    0.5724      1623\n",
            "\n",
            "0.6537544118861357 0.7563207069502307 0.4800839574333552 0.57237409744088\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.5407702326774597\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.76it/s]\n",
            "Epoch 9 loss average: 0.574\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7811    0.7021    0.7395       376\n",
            "           1     0.8784    0.8887    0.8835       764\n",
            "           2     0.7582    0.7694    0.7638      1080\n",
            "           3     0.8008    0.7784    0.7894       749\n",
            "           4     0.7609    0.8019    0.7809       520\n",
            "           5     0.7837    0.7876    0.7857      1210\n",
            "\n",
            "    accuracy                         0.7931      4699\n",
            "   macro avg     0.7939    0.7880    0.7905      4699\n",
            "weighted avg     0.7932    0.7931    0.7929      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5205    0.2639    0.3502       144\n",
            "           1     0.6630    0.7306    0.6951       245\n",
            "           2     0.4218    0.7370    0.5365       384\n",
            "           3     0.6185    0.6294    0.6239       170\n",
            "           4     0.7533    0.3779    0.5033       299\n",
            "           5     0.6469    0.4856    0.5547       381\n",
            "\n",
            "    accuracy                         0.5576      1623\n",
            "   macro avg     0.6040    0.5374    0.5440      1623\n",
            "weighted avg     0.6015    0.5576    0.5512      1623\n",
            "\n",
            "0.5738693087672194 0.7929269521124801 0.48071603358489295 0.551244289076161\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.5633642077445984\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 10 loss average: 0.481\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8081    0.7394    0.7722       376\n",
            "           1     0.9010    0.9058    0.9034       764\n",
            "           2     0.8029    0.8185    0.8106      1080\n",
            "           3     0.8377    0.8064    0.8218       749\n",
            "           4     0.7821    0.8423    0.8111       520\n",
            "           5     0.8382    0.8347    0.8364      1210\n",
            "\n",
            "    accuracy                         0.8312      4699\n",
            "   macro avg     0.8284    0.8245    0.8259      4699\n",
            "weighted avg     0.8316    0.8312    0.8311      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5278    0.3958    0.4524       144\n",
            "           1     0.6062    0.7224    0.6592       245\n",
            "           2     0.4625    0.6589    0.5435       384\n",
            "           3     0.5423    0.6412    0.5876       170\n",
            "           4     0.7127    0.4314    0.5375       299\n",
            "           5     0.5850    0.4514    0.5096       381\n",
            "\n",
            "    accuracy                         0.5527      1623\n",
            "   macro avg     0.5727    0.5502    0.5483      1623\n",
            "weighted avg     0.5732    0.5527    0.5484      1623\n",
            "\n",
            "0.4812957566852371 0.8311152824325129 0.47236981005014517 0.5484469672815804\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.5334334969520569\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 11 loss average: 0.411\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7980    0.8298    0.8136       376\n",
            "           1     0.9101    0.9280    0.9190       764\n",
            "           2     0.8460    0.8343    0.8401      1080\n",
            "           3     0.8437    0.8358    0.8397       749\n",
            "           4     0.8566    0.8615    0.8591       520\n",
            "           5     0.8507    0.8430    0.8468      1210\n",
            "\n",
            "    accuracy                         0.8546      4699\n",
            "   macro avg     0.8508    0.8554    0.8530      4699\n",
            "weighted avg     0.8546    0.8546    0.8546      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5341    0.3264    0.4052       144\n",
            "           1     0.6172    0.6449    0.6307       245\n",
            "           2     0.4835    0.6120    0.5402       384\n",
            "           3     0.5317    0.6412    0.5813       170\n",
            "           4     0.6961    0.4749    0.5646       299\n",
            "           5     0.5547    0.5591    0.5569       381\n",
            "\n",
            "    accuracy                         0.5570      1623\n",
            "   macro avg     0.5695    0.5431    0.5465      1623\n",
            "weighted avg     0.5691    0.5570    0.5546      1623\n",
            "\n",
            "0.4107520117734869 0.8545678863152547 0.48126450083704203 0.5546115216316239\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.5031008124351501\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 12 loss average: 0.366\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8506    0.7872    0.8177       376\n",
            "           1     0.9143    0.9359    0.9250       764\n",
            "           2     0.8705    0.8528    0.8616      1080\n",
            "           3     0.8602    0.8625    0.8613       749\n",
            "           4     0.8418    0.8904    0.8654       520\n",
            "           5     0.8736    0.8736    0.8736      1210\n",
            "\n",
            "    accuracy                         0.8721      4699\n",
            "   macro avg     0.8685    0.8670    0.8674      4699\n",
            "weighted avg     0.8720    0.8721    0.8718      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3547    0.5764    0.4392       144\n",
            "           1     0.5932    0.7143    0.6481       245\n",
            "           2     0.5339    0.5339    0.5339       384\n",
            "           3     0.5722    0.6059    0.5886       170\n",
            "           4     0.7517    0.3746    0.5000       299\n",
            "           5     0.5853    0.5853    0.5853       381\n",
            "\n",
            "    accuracy                         0.5551      1623\n",
            "   macro avg     0.5652    0.5650    0.5492      1623\n",
            "weighted avg     0.5831    0.5551    0.5543      1623\n",
            "\n",
            "0.3664831329757969 0.8718359745745513 0.46034685526305746 0.5542769774439522\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.5569824576377869\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.79it/s]\n",
            "Epoch 13 loss average: 0.313\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8522    0.8431    0.8476       376\n",
            "           1     0.9316    0.9267    0.9291       764\n",
            "           2     0.8771    0.8852    0.8811      1080\n",
            "           3     0.8831    0.8879    0.8855       749\n",
            "           4     0.8942    0.8942    0.8942       520\n",
            "           5     0.8953    0.8909    0.8931      1210\n",
            "\n",
            "    accuracy                         0.8915      4699\n",
            "   macro avg     0.8889    0.8880    0.8884      4699\n",
            "weighted avg     0.8915    0.8915    0.8915      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4815    0.3611    0.4127       144\n",
            "           1     0.5971    0.6653    0.6293       245\n",
            "           2     0.5022    0.5938    0.5442       384\n",
            "           3     0.4979    0.6824    0.5757       170\n",
            "           4     0.6602    0.4548    0.5386       299\n",
            "           5     0.5473    0.5013    0.5233       381\n",
            "\n",
            "    accuracy                         0.5459      1623\n",
            "   macro avg     0.5477    0.5431    0.5373      1623\n",
            "weighted avg     0.5539    0.5459    0.5427      1623\n",
            "\n",
            "0.3125957497395575 0.8914782556354109 0.46477664076769265 0.5427335593423713\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.24100537598133087\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 14 loss average: 0.269\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8726    0.8564    0.8644       376\n",
            "           1     0.9433    0.9359    0.9396       764\n",
            "           2     0.9041    0.8991    0.9016      1080\n",
            "           3     0.9003    0.8919    0.8960       749\n",
            "           4     0.8981    0.9154    0.9067       520\n",
            "           5     0.9013    0.9132    0.9072      1210\n",
            "\n",
            "    accuracy                         0.9059      4699\n",
            "   macro avg     0.9033    0.9020    0.9026      4699\n",
            "weighted avg     0.9060    0.9059    0.9059      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5208    0.3472    0.4167       144\n",
            "           1     0.5739    0.6653    0.6163       245\n",
            "           2     0.4941    0.5443    0.5180       384\n",
            "           3     0.4786    0.6588    0.5545       170\n",
            "           4     0.7065    0.4348    0.5383       299\n",
            "           5     0.5398    0.5696    0.5543       381\n",
            "\n",
            "    accuracy                         0.5428      1623\n",
            "   macro avg     0.5523    0.5367    0.5330      1623\n",
            "weighted avg     0.5568    0.5428    0.5399      1623\n",
            "\n",
            "0.2691913841602703 0.9059148575483953 0.46696348249308656 0.539909117894998\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.2841731011867523\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.77it/s]\n",
            "Epoch 15 loss average: 0.238\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8942    0.8989    0.8966       376\n",
            "           1     0.9466    0.9516    0.9491       764\n",
            "           2     0.9056    0.9065    0.9061      1080\n",
            "           3     0.9044    0.9092    0.9068       749\n",
            "           4     0.9162    0.9250    0.9206       520\n",
            "           5     0.9255    0.9132    0.9193      1210\n",
            "\n",
            "    accuracy                         0.9174      4699\n",
            "   macro avg     0.9154    0.9174    0.9164      4699\n",
            "weighted avg     0.9175    0.9174    0.9174      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4717    0.3472    0.4000       144\n",
            "           1     0.5731    0.6082    0.5901       245\n",
            "           2     0.4659    0.6042    0.5261       384\n",
            "           3     0.4855    0.6882    0.5693       170\n",
            "           4     0.6889    0.4147    0.5177       299\n",
            "           5     0.5651    0.5013    0.5313       381\n",
            "\n",
            "    accuracy                         0.5317      1623\n",
            "   macro avg     0.5417    0.5273    0.5224      1623\n",
            "weighted avg     0.5490    0.5317    0.5288      1623\n",
            "\n",
            "0.23800691108529767 0.9174274605167547 0.4790845196530634 0.5287768570578343\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.10771474242210388\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 16 loss average: 0.217\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8932    0.8670    0.8799       376\n",
            "           1     0.9541    0.9529    0.9535       764\n",
            "           2     0.9136    0.9204    0.9170      1080\n",
            "           3     0.9139    0.9212    0.9176       749\n",
            "           4     0.9079    0.9288    0.9183       520\n",
            "           5     0.9273    0.9165    0.9219      1210\n",
            "\n",
            "    accuracy                         0.9215      4699\n",
            "   macro avg     0.9183    0.9178    0.9180      4699\n",
            "weighted avg     0.9215    0.9215    0.9214      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5455    0.2500    0.3429       144\n",
            "           1     0.5762    0.6327    0.6031       245\n",
            "           2     0.4815    0.5078    0.4943       384\n",
            "           3     0.5495    0.5882    0.5682       170\n",
            "           4     0.6825    0.4314    0.5287       299\n",
            "           5     0.4922    0.6614    0.5644       381\n",
            "\n",
            "    accuracy                         0.5342      1623\n",
            "   macro avg     0.5546    0.5119    0.5169      1623\n",
            "weighted avg     0.5481    0.5342    0.5278      1623\n",
            "\n",
            "0.21748784941155463 0.9214384188408917 0.4559216503062402 0.5278159045993739\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.22559010982513428\n",
            "100%|███████████████████████████████████████████| 48/48 [00:17<00:00,  2.78it/s]\n",
            "Epoch 17 loss average: 0.184\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9135    0.8989    0.9062       376\n",
            "           1     0.9621    0.9647    0.9634       764\n",
            "           2     0.9266    0.9352    0.9309      1080\n",
            "           3     0.9226    0.9226    0.9226       749\n",
            "           4     0.9430    0.9538    0.9484       520\n",
            "           5     0.9366    0.9273    0.9319      1210\n",
            "\n",
            "    accuracy                         0.9351      4699\n",
            "   macro avg     0.9341    0.9337    0.9339      4699\n",
            "weighted avg     0.9351    0.9351    0.9351      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3741    0.3819    0.3780       144\n",
            "           1     0.5425    0.6776    0.6025       245\n",
            "           2     0.4671    0.5547    0.5071       384\n",
            "           3     0.5723    0.5824    0.5773       170\n",
            "           4     0.6440    0.4114    0.5020       299\n",
            "           5     0.5771    0.5302    0.5527       381\n",
            "\n",
            "    accuracy                         0.5287      1623\n",
            "   macro avg     0.5295    0.5230    0.5199      1623\n",
            "weighted avg     0.5397    0.5287    0.5272      1623\n",
            "\n",
            "0.1840470974566415 0.9350600042412824 0.47325924679614056 0.527177457614793\n",
            "Patience counter: 11\n",
            "Done! It took 4e+02 secs\n",
            "\n",
            "Current RUN: 5\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0642765536904335\n",
            "Best test f1 weighted\n",
            "0.5739978855129063\n",
            "Best epoch\n",
            "6\n",
            "\n",
            "\n",
            "Average across runs:\n",
            "Best epoch\n",
            "[6, 5, 5, 5, 6]\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0434777483344078\n",
            "Overall test f1 weighted\n",
            "[0.5790948  0.57183808 0.60190769 0.5982407  0.57399789]\n",
            "Best test f1 weighted\n",
            "0.5850158306150286\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
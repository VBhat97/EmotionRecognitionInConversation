{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TL-ERC_IEMOCAP.ipynb",
      "provenance": [],
      "mount_file_id": "1hBfUI0BOqqpzPVNBKidsV51KZNOxdTz9",
      "authorship_tag": "ABX9TyMwYoiQ0knrXXyDJB4PWSgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anitayadav3/EmotionRecognitionInConversation/blob/master/TL_ERC_IEMOCAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pETrP0_ivwDl",
        "outputId": "50840caa-6da3-4545-e5de-1683682f3678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/TL-ERC"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVH4fWXowX40",
        "outputId": "dcdeaff7-7ad0-493e-ed07-6b8718c96885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd bert_model/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx3mnH6rwlns",
        "outputId": "c7ab2234-149e-4efd-96b5-faa8c9addcbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 30.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDXbobLawvGN",
        "outputId": "ea68bd36-a203-46d5-a8fe-3ff8aff24523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 27.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.63)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.63)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.63->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0BfakGoweV3",
        "outputId": "96160381-a6fb-4623-d516-35f51fec0875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/cornell_weights.pkl --data=iemocap --n_epoch=50"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 1.5e+01 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7502583265304565\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  4.99it/s]\n",
            "Epoch 1 loss average: 1.778\n",
            "train\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2211    0.0559    0.0892       376\n",
            "           1     0.0811    0.0838    0.0824       764\n",
            "           2     0.1883    0.1694    0.1784      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0888    0.0288    0.0435       520\n",
            "           5     0.2255    0.4983    0.3105      1210\n",
            "\n",
            "    accuracy                         0.1886      4699\n",
            "   macro avg     0.1341    0.1394    0.1173      4699\n",
            "weighted avg     0.1420    0.1886    0.1463      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.0000    0.0000    0.0000       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2348    1.0000    0.3802       381\n",
            "\n",
            "    accuracy                         0.2348      1623\n",
            "   macro avg     0.0391    0.1667    0.0634      1623\n",
            "weighted avg     0.0551    0.2348    0.0893      1623\n",
            "\n",
            "1.7776562869548798 0.14630391453874672 0.08752898664592536 0.0892614032563339\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.7775299549102783\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  4.85it/s]\n",
            "Epoch 2 loss average: 1.703\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2180    0.2556    0.2353      1080\n",
            "           3     1.0000    0.0053    0.0106       749\n",
            "           4     0.6441    0.0731    0.1313       520\n",
            "           5     0.2891    0.7967    0.4242      1210\n",
            "\n",
            "    accuracy                         0.2728      4699\n",
            "   macro avg     0.3585    0.1884    0.1336      4699\n",
            "weighted avg     0.3552    0.2728    0.1795      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3188    0.5069    0.3914       144\n",
            "           1     0.4154    0.2204    0.2880       245\n",
            "           2     0.7674    0.0859    0.1546       384\n",
            "           3     0.7556    0.4000    0.5231       170\n",
            "           4     0.4605    0.3311    0.3852       299\n",
            "           5     0.3777    0.9081    0.5335       381\n",
            "\n",
            "    accuracy                         0.4147      1623\n",
            "   macro avg     0.5159    0.4088    0.3793      1623\n",
            "weighted avg     0.5252    0.4147    0.3658      1623\n",
            "\n",
            "1.7029668812950451 0.17953089848857318 0.3321853530034148 0.3657785835520084\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.2519915103912354\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 3 loss average: 1.433\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2628    0.2048    0.2302       376\n",
            "           1     0.4573    0.2736    0.3423       764\n",
            "           2     0.3848    0.2954    0.3342      1080\n",
            "           3     0.2312    0.0534    0.0868       749\n",
            "           4     0.3743    0.5038    0.4295       520\n",
            "           5     0.4335    0.8050    0.5635      1210\n",
            "\n",
            "    accuracy                         0.4003      4699\n",
            "   macro avg     0.3573    0.3560    0.3311      4699\n",
            "weighted avg     0.3737    0.4003    0.3574      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.6774    0.5143    0.5847       245\n",
            "           2     0.5203    0.6354    0.5721       384\n",
            "           3     0.8750    0.3294    0.4786       170\n",
            "           4     0.5329    0.8395    0.6519       299\n",
            "           5     0.5150    0.5853    0.5479       381\n",
            "\n",
            "    accuracy                         0.5545      1623\n",
            "   macro avg     0.5201    0.4840    0.4725      1623\n",
            "weighted avg     0.5361    0.5545    0.5225      1623\n",
            "\n",
            "1.4325013384222984 0.3573541406455846 0.40007935420223817 0.5224820476195148\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.055466651916504\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.01it/s]\n",
            "Epoch 4 loss average: 1.200\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7917    0.0505    0.0950       376\n",
            "           1     0.7224    0.5995    0.6552       764\n",
            "           2     0.4722    0.4556    0.4637      1080\n",
            "           3     0.6900    0.3031    0.4212       749\n",
            "           4     0.4633    0.7654    0.5772       520\n",
            "           5     0.5108    0.7645    0.6124      1210\n",
            "\n",
            "    accuracy                         0.5361      4699\n",
            "   macro avg     0.6084    0.4897    0.4708      4699\n",
            "weighted avg     0.5821    0.5361    0.5094      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6053    0.1597    0.2527       144\n",
            "           1     0.7634    0.4082    0.5319       245\n",
            "           2     0.4220    0.6693    0.5176       384\n",
            "           3     0.7212    0.4412    0.5474       170\n",
            "           4     0.6844    0.5151    0.5878       299\n",
            "           5     0.5000    0.6772    0.5753       381\n",
            "\n",
            "    accuracy                         0.5342      1623\n",
            "   macro avg     0.6160    0.4784    0.5021      1623\n",
            "weighted avg     0.5878    0.5342    0.5259      1623\n",
            "\n",
            "1.1995923866828282 0.5094064421950799 0.4263397651060258 0.5258573496787223\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.8964628577232361\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.08it/s]\n",
            "Epoch 5 loss average: 0.995\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5374    0.3059    0.3898       376\n",
            "           1     0.7397    0.7958    0.7667       764\n",
            "           2     0.5795    0.5537    0.5663      1080\n",
            "           3     0.6978    0.4192    0.5238       749\n",
            "           4     0.5963    0.7500    0.6644       520\n",
            "           5     0.5855    0.7388    0.6533      1210\n",
            "\n",
            "    accuracy                         0.6212      4699\n",
            "   macro avg     0.6227    0.5939    0.5940      4699\n",
            "weighted avg     0.6244    0.6212    0.6112      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3592    0.5139    0.4229       144\n",
            "           1     0.6690    0.7837    0.7218       245\n",
            "           2     0.4581    0.7396    0.5657       384\n",
            "           3     0.7674    0.5824    0.6622       170\n",
            "           4     0.6575    0.3211    0.4315       299\n",
            "           5     0.6511    0.4016    0.4968       381\n",
            "\n",
            "    accuracy                         0.5533      1623\n",
            "   macro avg     0.5937    0.5570    0.5501      1623\n",
            "weighted avg     0.5956    0.5533    0.5458      1623\n",
            "\n",
            "0.9946119549373785 0.6112318447070465 0.46644752688439595 0.5457926932502611\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.9833405613899231\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 6 loss average: 0.878\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6361    0.5160    0.5698       376\n",
            "           1     0.8020    0.8220    0.8119       764\n",
            "           2     0.6402    0.6278    0.6339      1080\n",
            "           3     0.7057    0.5154    0.5957       749\n",
            "           4     0.6762    0.7750    0.7222       520\n",
            "           5     0.6281    0.7314    0.6758      1210\n",
            "\n",
            "    accuracy                         0.6755      4699\n",
            "   macro avg     0.6814    0.6646    0.6682      4699\n",
            "weighted avg     0.6775    0.6755    0.6722      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6667    0.1806    0.2842       144\n",
            "           1     0.7432    0.6735    0.7066       245\n",
            "           2     0.4517    0.6693    0.5393       384\n",
            "           3     0.5749    0.7000    0.6313       170\n",
            "           4     0.6833    0.5485    0.6085       299\n",
            "           5     0.5636    0.5118    0.5365       381\n",
            "\n",
            "    accuracy                         0.5705      1623\n",
            "   macro avg     0.6139    0.5473    0.5511      1623\n",
            "weighted avg     0.5966    0.5705    0.5637      1623\n",
            "\n",
            "0.8778051224847635 0.6721948352207532 0.4637647240301542 0.5636569084361374\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.6173402667045593\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 7 loss average: 0.778\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6580    0.5372    0.5915       376\n",
            "           1     0.8015    0.8613    0.8303       764\n",
            "           2     0.6584    0.6676    0.6630      1080\n",
            "           3     0.7395    0.5648    0.6404       749\n",
            "           4     0.7019    0.7288    0.7151       520\n",
            "           5     0.6510    0.7339    0.6900      1210\n",
            "\n",
            "    accuracy                         0.6961      4699\n",
            "   macro avg     0.7017    0.6823    0.6884      4699\n",
            "weighted avg     0.6975    0.6961    0.6936      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3417    0.5694    0.4271       144\n",
            "           1     0.6643    0.7755    0.7156       245\n",
            "           2     0.4628    0.7604    0.5754       384\n",
            "           3     0.7360    0.5412    0.6237       170\n",
            "           4     0.7407    0.2007    0.3158       299\n",
            "           5     0.6654    0.4541    0.5398       381\n",
            "\n",
            "    accuracy                         0.5478      1623\n",
            "   macro avg     0.6018    0.5502    0.5329      1623\n",
            "weighted avg     0.6098    0.5478    0.5323      1623\n",
            "\n",
            "0.777515821158886 0.6935879742606542 0.46748327353416197 0.532275524327965\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.5735077857971191\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 8 loss average: 0.653\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6848    0.6356    0.6593       376\n",
            "           1     0.8704    0.8704    0.8704       764\n",
            "           2     0.7321    0.7389    0.7355      1080\n",
            "           3     0.7645    0.6328    0.6925       749\n",
            "           4     0.7459    0.7962    0.7702       520\n",
            "           5     0.7192    0.7851    0.7507      1210\n",
            "\n",
            "    accuracy                         0.7534      4699\n",
            "   macro avg     0.7528    0.7432    0.7464      4699\n",
            "weighted avg     0.7542    0.7534    0.7522      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5970    0.2778    0.3791       144\n",
            "           1     0.6409    0.6776    0.6587       245\n",
            "           2     0.4557    0.6823    0.5464       384\n",
            "           3     0.6364    0.6588    0.6474       170\n",
            "           4     0.7213    0.4415    0.5477       299\n",
            "           5     0.5923    0.5643    0.5780       381\n",
            "\n",
            "    accuracy                         0.5712      1623\n",
            "   macro avg     0.6073    0.5504    0.5596      1623\n",
            "weighted avg     0.5961    0.5712    0.5667      1623\n",
            "\n",
            "0.6533854256073633 0.7522335699394143 0.4903935902986452 0.5667477865152444\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.6832036375999451\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 9 loss average: 0.564\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7326    0.6995    0.7156       376\n",
            "           1     0.8821    0.8914    0.8867       764\n",
            "           2     0.7871    0.8111    0.7989      1080\n",
            "           3     0.7880    0.7343    0.7602       749\n",
            "           4     0.8012    0.7904    0.7957       520\n",
            "           5     0.7822    0.8041    0.7930      1210\n",
            "\n",
            "    accuracy                         0.7989      4699\n",
            "   macro avg     0.7955    0.7885    0.7917      4699\n",
            "weighted avg     0.7986    0.7989    0.7985      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5385    0.3403    0.4170       144\n",
            "           1     0.5422    0.7347    0.6239       245\n",
            "           2     0.5297    0.5339    0.5318       384\n",
            "           3     0.6711    0.6000    0.6335       170\n",
            "           4     0.6911    0.4415    0.5388       299\n",
            "           5     0.5447    0.6719    0.6016       381\n",
            "\n",
            "    accuracy                         0.5693      1623\n",
            "   macro avg     0.5862    0.5537    0.5578      1623\n",
            "weighted avg     0.5804    0.5693    0.5639      1623\n",
            "\n",
            "0.5639342125505209 0.7984769853945801 0.4758869035757525 0.563854313057363\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.2663924992084503\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 10 loss average: 0.482\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7861    0.7234    0.7535       376\n",
            "           1     0.8982    0.9123    0.9052       764\n",
            "           2     0.8251    0.8037    0.8143      1080\n",
            "           3     0.8011    0.7583    0.7791       749\n",
            "           4     0.8051    0.8500    0.8269       520\n",
            "           5     0.7901    0.8273    0.8082      1210\n",
            "\n",
            "    accuracy                         0.8189      4699\n",
            "   macro avg     0.8176    0.8125    0.8145      4699\n",
            "weighted avg     0.8188    0.8189    0.8184      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4286    0.4375    0.4330       144\n",
            "           1     0.6031    0.6449    0.6233       245\n",
            "           2     0.4758    0.6927    0.5642       384\n",
            "           3     0.6139    0.5706    0.5915       170\n",
            "           4     0.6545    0.4181    0.5102       299\n",
            "           5     0.6111    0.4908    0.5444       381\n",
            "\n",
            "    accuracy                         0.5521      1623\n",
            "   macro avg     0.5645    0.5424    0.5444      1623\n",
            "weighted avg     0.5700    0.5521    0.5497      1623\n",
            "\n",
            "0.4820221948126952 0.8184354161541999 0.4968067199757284 0.5497249540227616\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.11457044631242752\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 11 loss average: 0.423\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7887    0.7447    0.7661       376\n",
            "           1     0.9263    0.9045    0.9152       764\n",
            "           2     0.8310    0.8648    0.8475      1080\n",
            "           3     0.8514    0.7877    0.8183       749\n",
            "           4     0.8252    0.8442    0.8346       520\n",
            "           5     0.8359    0.8628    0.8491      1210\n",
            "\n",
            "    accuracy                         0.8466      4699\n",
            "   macro avg     0.8431    0.8348    0.8385      4699\n",
            "weighted avg     0.8470    0.8466    0.8463      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4062    0.4514    0.4276       144\n",
            "           1     0.5634    0.7796    0.6541       245\n",
            "           2     0.5282    0.5130    0.5205       384\n",
            "           3     0.5000    0.6647    0.5707       170\n",
            "           4     0.5672    0.4515    0.5028       299\n",
            "           5     0.6028    0.4541    0.5180       381\n",
            "\n",
            "    accuracy                         0.5385      1623\n",
            "   macro avg     0.5280    0.5524    0.5323      1623\n",
            "weighted avg     0.5444    0.5385    0.5338      1623\n",
            "\n",
            "0.4232415679531793 0.8463464205467185 0.4618726425875859 0.5338250900797967\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.3407127261161804\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 12 loss average: 0.372\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8179    0.8245    0.8212       376\n",
            "           1     0.9292    0.9110    0.9200       764\n",
            "           2     0.8671    0.8639    0.8655      1080\n",
            "           3     0.8458    0.8571    0.8515       749\n",
            "           4     0.8705    0.8788    0.8746       520\n",
            "           5     0.8629    0.8636    0.8633      1210\n",
            "\n",
            "    accuracy                         0.8689      4699\n",
            "   macro avg     0.8656    0.8665    0.8660      4699\n",
            "weighted avg     0.8692    0.8689    0.8690      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5316    0.2917    0.3767       144\n",
            "           1     0.6079    0.6898    0.6463       245\n",
            "           2     0.4965    0.5547    0.5240       384\n",
            "           3     0.6452    0.5882    0.6154       170\n",
            "           4     0.6395    0.4983    0.5602       299\n",
            "           5     0.5323    0.6273    0.5759       381\n",
            "\n",
            "    accuracy                         0.5619      1623\n",
            "   macro avg     0.5755    0.5417    0.5497      1623\n",
            "weighted avg     0.5668    0.5619    0.5578      1623\n",
            "\n",
            "0.37161985427762073 0.8690197608822564 0.47652862247484634 0.5577995248157757\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.4889599680900574\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 13 loss average: 0.318\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8398    0.8644    0.8519       376\n",
            "           1     0.9258    0.9306    0.9282       764\n",
            "           2     0.8825    0.8972    0.8898      1080\n",
            "           3     0.8843    0.8571    0.8705       749\n",
            "           4     0.9014    0.8788    0.8900       520\n",
            "           5     0.8846    0.8868    0.8857      1210\n",
            "\n",
            "    accuracy                         0.8889      4699\n",
            "   macro avg     0.8864    0.8858    0.8860      4699\n",
            "weighted avg     0.8890    0.8889    0.8889      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4825    0.3819    0.4264       144\n",
            "           1     0.5806    0.7347    0.6486       245\n",
            "           2     0.5396    0.4609    0.4972       384\n",
            "           3     0.5938    0.6706    0.6298       170\n",
            "           4     0.5874    0.5619    0.5744       299\n",
            "           5     0.5573    0.5748    0.5659       381\n",
            "\n",
            "    accuracy                         0.5625      1623\n",
            "   macro avg     0.5569    0.5641    0.5570      1623\n",
            "weighted avg     0.5594    0.5625    0.5580      1623\n",
            "\n",
            "0.31784881294394535 0.8888948767240648 0.4842891206365092 0.5580070386760806\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.19829806685447693\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 14 loss average: 0.273\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8606    0.8537    0.8571       376\n",
            "           1     0.9403    0.9490    0.9446       764\n",
            "           2     0.8961    0.8861    0.8911      1080\n",
            "           3     0.8970    0.8838    0.8904       749\n",
            "           4     0.9034    0.9173    0.9103       520\n",
            "           5     0.9009    0.9091    0.9050      1210\n",
            "\n",
            "    accuracy                         0.9027      4699\n",
            "   macro avg     0.8997    0.8998    0.8997      4699\n",
            "weighted avg     0.9026    0.9027    0.9027      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4946    0.3194    0.3882       144\n",
            "           1     0.6279    0.6612    0.6441       245\n",
            "           2     0.4645    0.6979    0.5578       384\n",
            "           3     0.5000    0.6941    0.5813       170\n",
            "           4     0.6574    0.4749    0.5515       299\n",
            "           5     0.6296    0.4016    0.4904       381\n",
            "\n",
            "    accuracy                         0.5478      1623\n",
            "   macro avg     0.5623    0.5415    0.5355      1623\n",
            "weighted avg     0.5699    0.5478    0.5412      1623\n",
            "\n",
            "0.2730231197395672 0.902661077975686 0.48239294549617023 0.5412375008366893\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.13571445643901825\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  4.99it/s]\n",
            "Epoch 15 loss average: 0.244\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8871    0.8989    0.8930       376\n",
            "           1     0.9446    0.9594    0.9519       764\n",
            "           2     0.9148    0.9046    0.9097      1080\n",
            "           3     0.8925    0.9092    0.9008       749\n",
            "           4     0.9365    0.9077    0.9219       520\n",
            "           5     0.9138    0.9116    0.9127      1210\n",
            "\n",
            "    accuracy                         0.9159      4699\n",
            "   macro avg     0.9149    0.9152    0.9150      4699\n",
            "weighted avg     0.9160    0.9159    0.9159      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3961    0.4236    0.4094       144\n",
            "           1     0.6216    0.6571    0.6389       245\n",
            "           2     0.4610    0.6615    0.5433       384\n",
            "           3     0.5388    0.6529    0.5904       170\n",
            "           4     0.6378    0.3946    0.4876       299\n",
            "           5     0.6306    0.4436    0.5208       381\n",
            "\n",
            "    accuracy                         0.5385      1623\n",
            "   macro avg     0.5477    0.5389    0.5317      1623\n",
            "weighted avg     0.5600    0.5385    0.5352      1623\n",
            "\n",
            "0.24423975489723185 0.9159294344623115 0.4801604982333924 0.5352464272908888\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.3842557668685913\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 16 loss average: 0.223\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8715    0.9016    0.8863       376\n",
            "           1     0.9607    0.9607    0.9607       764\n",
            "           2     0.9078    0.9204    0.9140      1080\n",
            "           3     0.9167    0.9105    0.9136       749\n",
            "           4     0.9270    0.9038    0.9153       520\n",
            "           5     0.9183    0.9107    0.9145      1210\n",
            "\n",
            "    accuracy                         0.9196      4699\n",
            "   macro avg     0.9170    0.9180    0.9174      4699\n",
            "weighted avg     0.9197    0.9196    0.9196      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3529    0.4583    0.3988       144\n",
            "           1     0.6364    0.6857    0.6601       245\n",
            "           2     0.4733    0.6458    0.5463       384\n",
            "           3     0.5020    0.7235    0.5928       170\n",
            "           4     0.6215    0.3679    0.4622       299\n",
            "           5     0.6593    0.3911    0.4909       381\n",
            "\n",
            "    accuracy                         0.5323      1623\n",
            "   macro avg     0.5409    0.5454    0.5252      1623\n",
            "weighted avg     0.5612    0.5323    0.5268      1623\n",
            "\n",
            "0.22323814585494497 0.9195977902599414 0.46666351478946905 0.5267585423063975\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.14514346420764923\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.09it/s]\n",
            "Epoch 17 loss average: 0.198\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9106    0.8936    0.9020       376\n",
            "           1     0.9580    0.9555    0.9567       764\n",
            "           2     0.9254    0.9306    0.9280      1080\n",
            "           3     0.9240    0.9252    0.9246       749\n",
            "           4     0.9294    0.9365    0.9330       520\n",
            "           5     0.9247    0.9231    0.9239      1210\n",
            "\n",
            "    accuracy                         0.9296      4699\n",
            "   macro avg     0.9287    0.9274    0.9280      4699\n",
            "weighted avg     0.9295    0.9296    0.9295      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4029    0.3889    0.3958       144\n",
            "           1     0.5863    0.5959    0.5911       245\n",
            "           2     0.4864    0.5599    0.5206       384\n",
            "           3     0.5455    0.6353    0.5870       170\n",
            "           4     0.6298    0.3813    0.4750       299\n",
            "           5     0.5290    0.5748    0.5509       381\n",
            "\n",
            "    accuracy                         0.5287      1623\n",
            "   macro avg     0.5300    0.5227    0.5201      1623\n",
            "weighted avg     0.5367    0.5287    0.5258      1623\n",
            "\n",
            "0.19815233198460191 0.9295436387738947 0.47680709583769426 0.525833273823871\n",
            "Patience counter: 11\n",
            "Done! It took 2.1e+02 secs\n",
            "\n",
            "Current RUN: 1\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0550428666174412\n",
            "Best test f1 weighted\n",
            "0.5636569084361374\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 5.6 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 2.053934097290039\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 1 loss average: 1.760\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1810    0.1597    0.1697       764\n",
            "           2     0.2645    0.5398    0.3551      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.4259    0.0442    0.0801       520\n",
            "           5     0.2714    0.3769    0.3156      1210\n",
            "\n",
            "    accuracy                         0.2520      4699\n",
            "   macro avg     0.1905    0.1868    0.1534      4699\n",
            "weighted avg     0.2073    0.2520    0.1993      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3725    0.0495    0.0874       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2405    0.9921    0.3871       381\n",
            "\n",
            "    accuracy                         0.2446      1623\n",
            "   macro avg     0.1022    0.1736    0.0791      1623\n",
            "weighted avg     0.1446    0.2446    0.1115      1623\n",
            "\n",
            "1.7599074269334476 0.19932072896928943 0.10710786172956918 0.11153955548610524\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.827050805091858\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.09it/s]\n",
            "Epoch 2 loss average: 1.695\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2414    0.0372    0.0645       376\n",
            "           1     0.0462    0.0039    0.0072       764\n",
            "           2     0.2000    0.1528    0.1732      1080\n",
            "           3     0.7857    0.0294    0.0566       749\n",
            "           4     0.6000    0.0115    0.0226       520\n",
            "           5     0.2793    0.8570    0.4213      1210\n",
            "\n",
            "    accuracy                         0.2654      4699\n",
            "   macro avg     0.3588    0.1820    0.1243      4699\n",
            "weighted avg     0.3363    0.2654    0.1662      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2405    0.2639    0.2517       144\n",
            "           1     0.2482    0.4122    0.3098       245\n",
            "           2     0.4637    0.2161    0.2948       384\n",
            "           3     0.8125    0.3824    0.5200       170\n",
            "           4     0.5704    0.2709    0.3673       299\n",
            "           5     0.4414    0.7612    0.5588       381\n",
            "\n",
            "    accuracy                         0.4054      1623\n",
            "   macro avg     0.4628    0.3844    0.3837      1623\n",
            "weighted avg     0.4623    0.4054    0.3922      1623\n",
            "\n",
            "1.6954733977715175 0.16616737798715667 0.3422675427264715 0.39217021043903505\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.5157707929611206\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 3 loss average: 1.429\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3400    0.0452    0.0798       376\n",
            "           1     0.4462    0.1466    0.2207       764\n",
            "           2     0.3161    0.2148    0.2558      1080\n",
            "           3     0.6862    0.1722    0.2753       749\n",
            "           4     0.4492    0.7654    0.5661       520\n",
            "           5     0.3985    0.8529    0.5432      1210\n",
            "\n",
            "    accuracy                         0.4086      4699\n",
            "   macro avg     0.4394    0.3662    0.3235      4699\n",
            "weighted avg     0.4341    0.4086    0.3475      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.5111    0.0939    0.1586       245\n",
            "           2     0.2324    0.2240    0.2281       384\n",
            "           3     0.7901    0.3765    0.5100       170\n",
            "           4     0.7022    0.5284    0.6031       299\n",
            "           5     0.3792    0.8976    0.5331       381\n",
            "\n",
            "    accuracy                         0.4147      1623\n",
            "   macro avg     0.4358    0.3534    0.3388      1623\n",
            "weighted avg     0.4333    0.4147    0.3676      1623\n",
            "\n",
            "1.4286830003062885 0.34746093998576744 0.358131179439938 0.36758219392031455\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.2476500272750854\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.00it/s]\n",
            "Epoch 4 loss average: 1.202\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2901    0.2500    0.2686       376\n",
            "           1     0.7094    0.5432    0.6153       764\n",
            "           2     0.4700    0.3046    0.3697      1080\n",
            "           3     0.6273    0.4606    0.5312       749\n",
            "           4     0.4750    0.6038    0.5318       520\n",
            "           5     0.4800    0.7455    0.5840      1210\n",
            "\n",
            "    accuracy                         0.5105      4699\n",
            "   macro avg     0.5086    0.4846    0.4834      4699\n",
            "weighted avg     0.5227    0.5105    0.5004      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.6189    0.6694    0.6431       245\n",
            "           2     0.5531    0.5286    0.5406       384\n",
            "           3     0.5924    0.6412    0.6158       170\n",
            "           4     0.5782    0.7793    0.6638       299\n",
            "           5     0.5619    0.5958    0.5783       381\n",
            "\n",
            "    accuracy                         0.5767      1623\n",
            "   macro avg     0.4841    0.5357    0.5070      1623\n",
            "weighted avg     0.5248    0.5767    0.5476      1623\n",
            "\n",
            "1.2024503747622173 0.5003827525796496 0.40887128337876255 0.5475561463010785\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.9812490344047546\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.02it/s]\n",
            "Epoch 5 loss average: 1.014\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3179    0.2367    0.2713       376\n",
            "           1     0.7244    0.8050    0.7626       764\n",
            "           2     0.5516    0.4852    0.5163      1080\n",
            "           3     0.6481    0.5434    0.5911       749\n",
            "           4     0.4974    0.5519    0.5232       520\n",
            "           5     0.5809    0.6793    0.6263      1210\n",
            "\n",
            "    accuracy                         0.5840      4699\n",
            "   macro avg     0.5534    0.5503    0.5485      4699\n",
            "weighted avg     0.5779    0.5840    0.5777      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7059    0.1667    0.2697       144\n",
            "           1     0.6627    0.6898    0.6760       245\n",
            "           2     0.5120    0.5547    0.5325       384\n",
            "           3     0.7664    0.4824    0.5921       170\n",
            "           4     0.6382    0.6254    0.6318       299\n",
            "           5     0.5367    0.7297    0.6185       381\n",
            "\n",
            "    accuracy                         0.5872      1623\n",
            "   macro avg     0.6370    0.5414    0.5534      1623\n",
            "weighted avg     0.6077    0.5872    0.5755      1623\n",
            "\n",
            "1.0143404031793277 0.5777462631091519 0.42808995424530427 0.5755463343227754\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.8312023282051086\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 6 loss average: 0.881\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6848    0.3351    0.4500       376\n",
            "           1     0.7733    0.8482    0.8090       764\n",
            "           2     0.6079    0.5398    0.5718      1080\n",
            "           3     0.7299    0.6168    0.6686       749\n",
            "           4     0.6142    0.8327    0.7069       520\n",
            "           5     0.6428    0.7331    0.6849      1210\n",
            "\n",
            "    accuracy                         0.6680      4699\n",
            "   macro avg     0.6755    0.6509    0.6486      4699\n",
            "weighted avg     0.6701    0.6680    0.6601      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5500    0.3056    0.3929       144\n",
            "           1     0.6230    0.7959    0.6989       245\n",
            "           2     0.4972    0.6953    0.5798       384\n",
            "           3     0.5349    0.6765    0.5974       170\n",
            "           4     0.6903    0.5217    0.5943       299\n",
            "           5     0.6270    0.4147    0.4992       381\n",
            "\n",
            "    accuracy                         0.5761      1623\n",
            "   macro avg     0.5871    0.5683    0.5604      1623\n",
            "weighted avg     0.5909    0.5761    0.5668      1623\n",
            "\n",
            "0.8813550211489201 0.6601470408060431 0.46672697827825177 0.5667910419678683\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.8896002173423767\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.10it/s]\n",
            "Epoch 7 loss average: 0.758\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7288    0.4574    0.5621       376\n",
            "           1     0.8073    0.8442    0.8253       764\n",
            "           2     0.6685    0.6741    0.6713      1080\n",
            "           3     0.7326    0.6876    0.7094       749\n",
            "           4     0.6860    0.8404    0.7554       520\n",
            "           5     0.7101    0.7248    0.7174      1210\n",
            "\n",
            "    accuracy                         0.7180      4699\n",
            "   macro avg     0.7222    0.7048    0.7068      4699\n",
            "weighted avg     0.7188    0.7180    0.7148      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5441    0.2569    0.3491       144\n",
            "           1     0.5985    0.6694    0.6320       245\n",
            "           2     0.5116    0.6328    0.5658       384\n",
            "           3     0.6265    0.6118    0.6190       170\n",
            "           4     0.6789    0.4950    0.5725       299\n",
            "           5     0.5640    0.6247    0.5928       381\n",
            "\n",
            "    accuracy                         0.5755      1623\n",
            "   macro avg     0.5873    0.5484    0.5552      1623\n",
            "weighted avg     0.5828    0.5755    0.5697      1623\n",
            "\n",
            "0.7583440852661928 0.7148413313086786 0.4818380860728058 0.5697051363023268\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.4664978086948395\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 8 loss average: 0.660\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7403    0.6064    0.6667       376\n",
            "           1     0.8297    0.8547    0.8420       764\n",
            "           2     0.7093    0.7093    0.7093      1080\n",
            "           3     0.7991    0.7330    0.7646       749\n",
            "           4     0.7340    0.7962    0.7638       520\n",
            "           5     0.7298    0.7678    0.7483      1210\n",
            "\n",
            "    accuracy                         0.7531      4699\n",
            "   macro avg     0.7570    0.7445    0.7491      4699\n",
            "weighted avg     0.7537    0.7531    0.7524      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3521    0.5208    0.4202       144\n",
            "           1     0.5661    0.6816    0.6185       245\n",
            "           2     0.5327    0.6146    0.5707       384\n",
            "           3     0.5645    0.6176    0.5899       170\n",
            "           4     0.7228    0.2441    0.3650       299\n",
            "           5     0.5636    0.5696    0.5666       381\n",
            "\n",
            "    accuracy                         0.5379      1623\n",
            "   macro avg     0.5503    0.5414    0.5218      1623\n",
            "weighted avg     0.5673    0.5379    0.5277      1623\n",
            "\n",
            "0.6602671602740884 0.7523539195209074 0.47629633265993976 0.5277185575810001\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.608938455581665\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.08it/s]\n",
            "Epoch 9 loss average: 0.578\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6442    0.7128    0.6768       376\n",
            "           1     0.8827    0.8665    0.8745       764\n",
            "           2     0.7706    0.7528    0.7616      1080\n",
            "           3     0.7892    0.7997    0.7944       749\n",
            "           4     0.7805    0.7250    0.7517       520\n",
            "           5     0.7791    0.7959    0.7874      1210\n",
            "\n",
            "    accuracy                         0.7836      4699\n",
            "   macro avg     0.7744    0.7754    0.7744      4699\n",
            "weighted avg     0.7850    0.7836    0.7840      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4286    0.3542    0.3878       144\n",
            "           1     0.6274    0.8041    0.7048       245\n",
            "           2     0.5609    0.5755    0.5681       384\n",
            "           3     0.6319    0.6059    0.6186       170\n",
            "           4     0.5650    0.7124    0.6302       299\n",
            "           5     0.6680    0.4488    0.5369       381\n",
            "\n",
            "    accuracy                         0.5890      1623\n",
            "   macro avg     0.5803    0.5835    0.5744      1623\n",
            "weighted avg     0.5925    0.5890    0.5822      1623\n",
            "\n",
            "0.5782056947549185 0.7839550778052233 0.4814137170947636 0.5821531897718355\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.7296391129493713\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.09it/s]\n",
            "Epoch 10 loss average: 0.506\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7083    0.7234    0.7158       376\n",
            "           1     0.8768    0.8940    0.8853       764\n",
            "           2     0.7849    0.7704    0.7776      1080\n",
            "           3     0.8415    0.8224    0.8319       749\n",
            "           4     0.7984    0.7769    0.7875       520\n",
            "           5     0.7997    0.8182    0.8088      1210\n",
            "\n",
            "    accuracy                         0.8080      4699\n",
            "   macro avg     0.8016    0.8009    0.8011      4699\n",
            "weighted avg     0.8080    0.8080    0.8079      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5213    0.3403    0.4118       144\n",
            "           1     0.5358    0.7633    0.6296       245\n",
            "           2     0.4836    0.6536    0.5559       384\n",
            "           3     0.5810    0.6118    0.5960       170\n",
            "           4     0.6701    0.4348    0.5274       299\n",
            "           5     0.6319    0.4777    0.5441       381\n",
            "\n",
            "    accuracy                         0.5564      1623\n",
            "   macro avg     0.5706    0.5469    0.5441      1623\n",
            "weighted avg     0.5742    0.5564    0.5504      1623\n",
            "\n",
            "0.50584818298618 0.8079448647033574 0.48330542894548273 0.5504218032927103\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.4349319636821747\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 11 loss average: 0.427\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8253    0.7287    0.7740       376\n",
            "           1     0.9004    0.9228    0.9114       764\n",
            "           2     0.8347    0.8463    0.8405      1080\n",
            "           3     0.8540    0.8198    0.8365       749\n",
            "           4     0.8180    0.8558    0.8365       520\n",
            "           5     0.8393    0.8504    0.8448      1210\n",
            "\n",
            "    accuracy                         0.8472      4699\n",
            "   macro avg     0.8453    0.8373    0.8406      4699\n",
            "weighted avg     0.8470    0.8472    0.8467      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4793    0.4028    0.4377       144\n",
            "           1     0.5566    0.7224    0.6288       245\n",
            "           2     0.5042    0.6224    0.5571       384\n",
            "           3     0.5987    0.5529    0.5749       170\n",
            "           4     0.6936    0.4013    0.5085       299\n",
            "           5     0.5816    0.5801    0.5808       381\n",
            "\n",
            "    accuracy                         0.5601      1623\n",
            "   macro avg     0.5690    0.5470    0.5480      1623\n",
            "weighted avg     0.5729    0.5601    0.5558      1623\n",
            "\n",
            "0.42712879413738847 0.8467370877450239 0.4966301987996635 0.5558071931523771\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.18757040798664093\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 12 loss average: 0.372\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8343    0.7899    0.8115       376\n",
            "           1     0.9221    0.9293    0.9257       764\n",
            "           2     0.8807    0.8611    0.8708      1080\n",
            "           3     0.8710    0.8745    0.8728       749\n",
            "           4     0.8519    0.8846    0.8679       520\n",
            "           5     0.8620    0.8727    0.8674      1210\n",
            "\n",
            "    accuracy                         0.8742      4699\n",
            "   macro avg     0.8703    0.8687    0.8693      4699\n",
            "weighted avg     0.8742    0.8742    0.8741      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4221    0.4514    0.4362       144\n",
            "           1     0.6069    0.6490    0.6272       245\n",
            "           2     0.4774    0.6042    0.5333       384\n",
            "           3     0.5450    0.6412    0.5892       170\n",
            "           4     0.7938    0.2575    0.3889       299\n",
            "           5     0.5236    0.5827    0.5516       381\n",
            "\n",
            "    accuracy                         0.5323      1623\n",
            "   macro avg     0.5615    0.5310    0.5211      1623\n",
            "weighted avg     0.5682    0.5323    0.5224      1623\n",
            "\n",
            "0.37151189769307774 0.8740782364000517 0.4709989415490975 0.5224084940519627\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.17224174737930298\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 13 loss average: 0.337\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8280    0.8324    0.8302       376\n",
            "           1     0.9316    0.9267    0.9291       764\n",
            "           2     0.8667    0.8731    0.8699      1080\n",
            "           3     0.8856    0.8892    0.8874       749\n",
            "           4     0.8723    0.8538    0.8630       520\n",
            "           5     0.8812    0.8826    0.8819      1210\n",
            "\n",
            "    accuracy                         0.8815      4699\n",
            "   macro avg     0.8776    0.8763    0.8769      4699\n",
            "weighted avg     0.8815    0.8815    0.8815      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5753    0.2917    0.3871       144\n",
            "           1     0.6385    0.6776    0.6574       245\n",
            "           2     0.4676    0.6016    0.5262       384\n",
            "           3     0.5000    0.6294    0.5573       170\n",
            "           4     0.6857    0.4816    0.5658       299\n",
            "           5     0.5296    0.5171    0.5232       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5661    0.5331    0.5362      1623\n",
            "weighted avg     0.5611    0.5465    0.5435      1623\n",
            "\n",
            "0.33694177082118887 0.8814815176663405 0.4941401581241734 0.5435264416876627\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.28296467661857605\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.08it/s]\n",
            "Epoch 14 loss average: 0.275\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8849    0.8590    0.8718       376\n",
            "           1     0.9277    0.9411    0.9344       764\n",
            "           2     0.8893    0.9000    0.8946      1080\n",
            "           3     0.8988    0.8892    0.8940       749\n",
            "           4     0.8998    0.8981    0.8989       520\n",
            "           5     0.9005    0.8975    0.8990      1210\n",
            "\n",
            "    accuracy                         0.9008      4699\n",
            "   macro avg     0.9002    0.8975    0.8988      4699\n",
            "weighted avg     0.9008    0.9008    0.9008      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5287    0.3194    0.3983       144\n",
            "           1     0.4427    0.8204    0.5751       245\n",
            "           2     0.5189    0.5000    0.5093       384\n",
            "           3     0.5868    0.5765    0.5816       170\n",
            "           4     0.6788    0.4381    0.5325       299\n",
            "           5     0.5938    0.5486    0.5703       381\n",
            "\n",
            "    accuracy                         0.5404      1623\n",
            "   macro avg     0.5583    0.5338    0.5278      1623\n",
            "weighted avg     0.5624    0.5404    0.5355      1623\n",
            "\n",
            "0.2745634650733943 0.9007584892483359 0.476378440059195 0.5355401497011116\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.1728997677564621\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 15 loss average: 0.243\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8914    0.8511    0.8707       376\n",
            "           1     0.9514    0.9490    0.9502       764\n",
            "           2     0.9069    0.9204    0.9136      1080\n",
            "           3     0.9215    0.9092    0.9153       749\n",
            "           4     0.9027    0.9096    0.9061       520\n",
            "           5     0.9106    0.9174    0.9140      1210\n",
            "\n",
            "    accuracy                         0.9157      4699\n",
            "   macro avg     0.9141    0.9094    0.9117      4699\n",
            "weighted avg     0.9157    0.9157    0.9157      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4366    0.4306    0.4336       144\n",
            "           1     0.6007    0.7184    0.6543       245\n",
            "           2     0.5035    0.5625    0.5314       384\n",
            "           3     0.4545    0.7059    0.5530       170\n",
            "           4     0.6717    0.4448    0.5352       299\n",
            "           5     0.5892    0.4593    0.5162       381\n",
            "\n",
            "    accuracy                         0.5434      1623\n",
            "   macro avg     0.5427    0.5536    0.5373      1623\n",
            "weighted avg     0.5582    0.5434    0.5407      1623\n",
            "\n",
            "0.24273248800697425 0.9156616902215666 0.48139242206547667 0.5406617659759935\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.128825381398201\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.02it/s]\n",
            "Epoch 16 loss average: 0.219\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8942    0.8989    0.8966       376\n",
            "           1     0.9480    0.9542    0.9511       764\n",
            "           2     0.9188    0.9222    0.9205      1080\n",
            "           3     0.9069    0.9105    0.9087       749\n",
            "           4     0.9256    0.9327    0.9291       520\n",
            "           5     0.9237    0.9099    0.9167      1210\n",
            "\n",
            "    accuracy                         0.9217      4699\n",
            "   macro avg     0.9195    0.9214    0.9205      4699\n",
            "weighted avg     0.9217    0.9217    0.9217      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4796    0.3264    0.3884       144\n",
            "           1     0.6460    0.5959    0.6200       245\n",
            "           2     0.4398    0.6276    0.5172       384\n",
            "           3     0.5684    0.6353    0.6000       170\n",
            "           4     0.7168    0.4147    0.5254       299\n",
            "           5     0.5335    0.5433    0.5384       381\n",
            "\n",
            "    accuracy                         0.5379      1623\n",
            "   macro avg     0.5640    0.5239    0.5316      1623\n",
            "weighted avg     0.5609    0.5379    0.5364      1623\n",
            "\n",
            "0.2190092847061654 0.9216671670594996 0.47761178063164544 0.536434494894809\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.11039228737354279\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 17 loss average: 0.187\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9178    0.9202    0.9190       376\n",
            "           1     0.9632    0.9594    0.9613       764\n",
            "           2     0.9252    0.9278    0.9265      1080\n",
            "           3     0.9117    0.9239    0.9178       749\n",
            "           4     0.9489    0.9288    0.9388       520\n",
            "           5     0.9281    0.9281    0.9281      1210\n",
            "\n",
            "    accuracy                         0.9319      4699\n",
            "   macro avg     0.9325    0.9314    0.9319      4699\n",
            "weighted avg     0.9320    0.9319    0.9319      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4831    0.2986    0.3691       144\n",
            "           1     0.5747    0.7224    0.6401       245\n",
            "           2     0.4672    0.5755    0.5158       384\n",
            "           3     0.5976    0.5765    0.5868       170\n",
            "           4     0.6780    0.4649    0.5516       299\n",
            "           5     0.5625    0.5669    0.5647       381\n",
            "\n",
            "    accuracy                         0.5508      1623\n",
            "   macro avg     0.5605    0.5341    0.5380      1623\n",
            "weighted avg     0.5597    0.5508    0.5471      1623\n",
            "\n",
            "0.18690373158703247 0.9319359738824751 0.48467705319709237 0.5470564936483471\n",
            "Patience counter: 11\n",
            "Done! It took 2.1e+02 secs\n",
            "\n",
            "Current RUN: 2\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0500206053256989\n",
            "Best test f1 weighted\n",
            "0.5667910419678683\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 5.6 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8487218618392944\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 1 loss average: 1.770\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0549    0.0133    0.0214       376\n",
            "           1     0.0345    0.0013    0.0025       764\n",
            "           2     0.2709    0.2917    0.2809      1080\n",
            "           3     0.1390    0.1108    0.1233       749\n",
            "           4     0.1579    0.0115    0.0215       520\n",
            "           5     0.2499    0.5744    0.3483      1210\n",
            "\n",
            "    accuracy                         0.2352      4699\n",
            "   macro avg     0.1512    0.1672    0.1330      4699\n",
            "weighted avg     0.1762    0.2352    0.1784      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.0000    0.0000    0.0000       384\n",
            "           3     0.8929    0.1471    0.2525       170\n",
            "           4     0.2911    0.2074    0.2422       299\n",
            "           5     0.2554    0.9265    0.4005       381\n",
            "\n",
            "    accuracy                         0.2711      1623\n",
            "   macro avg     0.2399    0.2135    0.1492      1623\n",
            "weighted avg     0.2071    0.2711    0.1651      1623\n",
            "\n",
            "1.7702758635083835 0.17839986013389847 0.15954757787986912 0.16507470274400063\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.7441765069961548\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 2 loss average: 1.677\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.7101    0.0641    0.1176       764\n",
            "           2     0.2642    0.3583    0.3041      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.2361    0.2288    0.2324       520\n",
            "           5     0.3307    0.7264    0.4545      1210\n",
            "\n",
            "    accuracy                         0.3052      4699\n",
            "   macro avg     0.2569    0.2296    0.1848      4699\n",
            "weighted avg     0.2875    0.3052    0.2318      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.2651    0.3592    0.3050       245\n",
            "           2     0.4904    0.1328    0.2090       384\n",
            "           3     0.9524    0.2353    0.3774       170\n",
            "           4     0.4762    0.5686    0.5183       299\n",
            "           5     0.4010    0.8294    0.5406       381\n",
            "\n",
            "    accuracy                         0.4097      1623\n",
            "   macro avg     0.4308    0.3542    0.3251      1623\n",
            "weighted avg     0.4377    0.4097    0.3574      1623\n",
            "\n",
            "1.6770102729399998 0.23178136766971472 0.2957987665063687 0.35742162681469253\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.2830880880355835\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.11it/s]\n",
            "Epoch 3 loss average: 1.362\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2857    0.0266    0.0487       376\n",
            "           1     0.4082    0.4686    0.4363       764\n",
            "           2     0.4134    0.2231    0.2898      1080\n",
            "           3     0.4670    0.3965    0.4289       749\n",
            "           4     0.3764    0.5654    0.4520       520\n",
            "           5     0.4074    0.6017    0.4858      1210\n",
            "\n",
            "    accuracy                         0.4103      4699\n",
            "   macro avg     0.3930    0.3803    0.3569      4699\n",
            "weighted avg     0.4052    0.4103    0.3849      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3455    0.1319    0.1910       144\n",
            "           1     0.8889    0.2939    0.4417       245\n",
            "           2     0.4401    0.6120    0.5120       384\n",
            "           3     0.8696    0.1176    0.2073       170\n",
            "           4     0.6059    0.6221    0.6139       299\n",
            "           5     0.4639    0.7585    0.5757       381\n",
            "\n",
            "    accuracy                         0.5059      1623\n",
            "   macro avg     0.6023    0.4227    0.4236      1623\n",
            "weighted avg     0.5805    0.5059    0.4747      1623\n",
            "\n",
            "1.3619098315636318 0.38492479447387934 0.34350390206835696 0.4746999421132719\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.144295334815979\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 4 loss average: 1.162\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2544    0.1941    0.2202       376\n",
            "           1     0.6835    0.5144    0.5870       764\n",
            "           2     0.4930    0.4213    0.4543      1080\n",
            "           3     0.6395    0.4192    0.5065       749\n",
            "           4     0.4801    0.6019    0.5341       520\n",
            "           5     0.4997    0.7314    0.5938      1210\n",
            "\n",
            "    accuracy                         0.5178      4699\n",
            "   macro avg     0.5083    0.4804    0.4826      4699\n",
            "weighted avg     0.5285    0.5178    0.5102      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4615    0.0417    0.0764       144\n",
            "           1     0.7719    0.5388    0.6346       245\n",
            "           2     0.4772    0.4635    0.4703       384\n",
            "           3     0.6076    0.5647    0.5854       170\n",
            "           4     0.5894    0.6722    0.6281       299\n",
            "           5     0.4921    0.7323    0.5886       381\n",
            "\n",
            "    accuracy                         0.5496      1623\n",
            "   macro avg     0.5666    0.5022    0.4972      1623\n",
            "weighted avg     0.5581    0.5496    0.5291      1623\n",
            "\n",
            "1.1622648586829503 0.5102075500192699 0.40798683082525755 0.5290540531176435\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.1658638715744019\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.02it/s]\n",
            "Epoch 5 loss average: 1.069\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3429    0.2553    0.2927       376\n",
            "           1     0.7189    0.6662    0.6916       764\n",
            "           2     0.4971    0.4824    0.4897      1080\n",
            "           3     0.6815    0.4713    0.5572       749\n",
            "           4     0.4968    0.6000    0.5436       520\n",
            "           5     0.5636    0.7066    0.6271      1210\n",
            "\n",
            "    accuracy                         0.5631      4699\n",
            "   macro avg     0.5501    0.5303    0.5336      4699\n",
            "weighted avg     0.5673    0.5631    0.5588      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0208    0.0408       144\n",
            "           1     0.7328    0.6939    0.7128       245\n",
            "           2     0.3653    0.4167    0.3893       384\n",
            "           3     0.6554    0.5706    0.6101       170\n",
            "           4     0.6941    0.3946    0.5032       299\n",
            "           5     0.4968    0.8241    0.6199       381\n",
            "\n",
            "    accuracy                         0.5311      1623\n",
            "   macro avg     0.6574    0.4868    0.4794      1623\n",
            "weighted avg     0.5989    0.5311    0.5055      1623\n",
            "\n",
            "1.0686803484956424 0.5588424285213884 0.40811733697470554 0.5054615779259657\n",
            "Patience counter: 1\n",
            "Epoch: 6, iter 0: loss = 1.2209773063659668\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 6 loss average: 0.920\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6125    0.4707    0.5323       376\n",
            "           1     0.7747    0.8325    0.8025       764\n",
            "           2     0.5861    0.5389    0.5615      1080\n",
            "           3     0.6861    0.5808    0.6291       749\n",
            "           4     0.5919    0.6192    0.6053       520\n",
            "           5     0.6058    0.7099    0.6537      1210\n",
            "\n",
            "    accuracy                         0.6408      4699\n",
            "   macro avg     0.6428    0.6253    0.6307      4699\n",
            "weighted avg     0.6405    0.6408    0.6377      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5526    0.1458    0.2308       144\n",
            "           1     0.8367    0.5020    0.6276       245\n",
            "           2     0.4080    0.5026    0.4504       384\n",
            "           3     0.5897    0.6765    0.6301       170\n",
            "           4     0.7125    0.3813    0.4967       299\n",
            "           5     0.4754    0.7612    0.5853       381\n",
            "\n",
            "    accuracy                         0.5274      1623\n",
            "   macro avg     0.5958    0.4949    0.5035      1623\n",
            "weighted avg     0.5765    0.5274    0.5167      1623\n",
            "\n",
            "0.9195993238439163 0.6377166040178376 0.4280994219514099 0.5166793843463475\n",
            "Patience counter: 2\n",
            "Epoch: 7, iter 0: loss = 1.0941320657730103\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 7 loss average: 0.820\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7713    0.3856    0.5142       376\n",
            "           1     0.8322    0.7984    0.8150       764\n",
            "           2     0.6310    0.6556    0.6431      1080\n",
            "           3     0.7130    0.6302    0.6690       749\n",
            "           4     0.6420    0.8346    0.7258       520\n",
            "           5     0.6555    0.7140    0.6835      1210\n",
            "\n",
            "    accuracy                         0.6880      4699\n",
            "   macro avg     0.7075    0.6697    0.6751      4699\n",
            "weighted avg     0.6955    0.6880    0.6844      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4762    0.3472    0.4016       144\n",
            "           1     0.6722    0.8204    0.7390       245\n",
            "           2     0.4878    0.7292    0.5846       384\n",
            "           3     0.6420    0.6647    0.6532       170\n",
            "           4     0.7047    0.4548    0.5528       299\n",
            "           5     0.6486    0.4698    0.5449       381\n",
            "\n",
            "    accuracy                         0.5909      1623\n",
            "   macro avg     0.6052    0.5810    0.5793      1623\n",
            "weighted avg     0.6085    0.5909    0.5837      1623\n",
            "\n",
            "0.8202597480267286 0.6844102686970681 0.4644546656059834 0.5836693417802717\n",
            "Patience counter: 0\n",
            "Epoch: 8, iter 0: loss = 0.6977835297584534\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 8 loss average: 0.719\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6378    0.5293    0.5785       376\n",
            "           1     0.8510    0.8743    0.8625       764\n",
            "           2     0.7184    0.6213    0.6663      1080\n",
            "           3     0.7729    0.7223    0.7467       749\n",
            "           4     0.6312    0.7538    0.6871       520\n",
            "           5     0.6978    0.7769    0.7352      1210\n",
            "\n",
            "    accuracy                         0.7259      4699\n",
            "   macro avg     0.7182    0.7130    0.7127      4699\n",
            "weighted avg     0.7273    0.7259    0.7241      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5352    0.2639    0.3535       144\n",
            "           1     0.6655    0.7551    0.7075       245\n",
            "           2     0.5350    0.6562    0.5895       384\n",
            "           3     0.6099    0.6529    0.6307       170\n",
            "           4     0.6523    0.6087    0.6298       299\n",
            "           5     0.5994    0.5381    0.5671       381\n",
            "\n",
            "    accuracy                         0.5995      1623\n",
            "   macro avg     0.5996    0.5792    0.5797      1623\n",
            "weighted avg     0.5993    0.5995    0.5928      1623\n",
            "\n",
            "0.718596801472207 0.724053959808313 0.46811210822563554 0.5928273311134696\n",
            "Patience counter: 0\n",
            "Epoch: 9, iter 0: loss = 0.44130799174308777\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 9 loss average: 0.603\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7398    0.6277    0.6791       376\n",
            "           1     0.8683    0.8887    0.8784       764\n",
            "           2     0.7522    0.7815    0.7666      1080\n",
            "           3     0.8057    0.7583    0.7813       749\n",
            "           4     0.7464    0.7981    0.7714       520\n",
            "           5     0.7704    0.7736    0.7720      1210\n",
            "\n",
            "    accuracy                         0.7827      4699\n",
            "   macro avg     0.7805    0.7713    0.7748      4699\n",
            "weighted avg     0.7827    0.7827    0.7820      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4700    0.3264    0.3852       144\n",
            "           1     0.7488    0.6449    0.6930       245\n",
            "           2     0.4517    0.7057    0.5508       384\n",
            "           3     0.6779    0.5941    0.6332       170\n",
            "           4     0.7213    0.4415    0.5477       299\n",
            "           5     0.6053    0.6037    0.6045       381\n",
            "\n",
            "    accuracy                         0.5786      1623\n",
            "   macro avg     0.6125    0.5527    0.5691      1623\n",
            "weighted avg     0.6076    0.5786    0.5782      1623\n",
            "\n",
            "0.6026133357857665 0.7820228837398248 0.5044291359479061 0.578242192416051\n",
            "Patience counter: 1\n",
            "Epoch: 10, iter 0: loss = 0.33570367097854614\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 10 loss average: 0.524\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8088    0.6862    0.7424       376\n",
            "           1     0.8948    0.8796    0.8871       764\n",
            "           2     0.8119    0.8111    0.8115      1080\n",
            "           3     0.8311    0.8144    0.8227       749\n",
            "           4     0.7824    0.8365    0.8086       520\n",
            "           5     0.8008    0.8339    0.8170      1210\n",
            "\n",
            "    accuracy                         0.8215      4699\n",
            "   macro avg     0.8216    0.8103    0.8149      4699\n",
            "weighted avg     0.8220    0.8215    0.8211      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5275    0.3333    0.4085       144\n",
            "           1     0.6680    0.6571    0.6626       245\n",
            "           2     0.4683    0.6354    0.5392       384\n",
            "           3     0.6483    0.5529    0.5968       170\n",
            "           4     0.7290    0.5217    0.6082       299\n",
            "           5     0.5791    0.6247    0.6010       381\n",
            "\n",
            "    accuracy                         0.5798      1623\n",
            "   macro avg     0.6034    0.5542    0.5694      1623\n",
            "weighted avg     0.5966    0.5798    0.5795      1623\n",
            "\n",
            "0.5239558108150959 0.8211369894720855 0.5040811571984489 0.5794865911167632\n",
            "Patience counter: 2\n",
            "Epoch: 11, iter 0: loss = 0.4953228235244751\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 11 loss average: 0.460\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7936    0.7261    0.7583       376\n",
            "           1     0.8955    0.9084    0.9019       764\n",
            "           2     0.8427    0.8287    0.8357      1080\n",
            "           3     0.8666    0.8238    0.8446       749\n",
            "           4     0.8032    0.8635    0.8323       520\n",
            "           5     0.8276    0.8529    0.8400      1210\n",
            "\n",
            "    accuracy                         0.8427      4699\n",
            "   macro avg     0.8382    0.8339    0.8355      4699\n",
            "weighted avg     0.8429    0.8427    0.8424      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6739    0.2153    0.3263       144\n",
            "           1     0.6667    0.5878    0.6247       245\n",
            "           2     0.4383    0.6849    0.5346       384\n",
            "           3     0.5672    0.6706    0.6146       170\n",
            "           4     0.7122    0.4883    0.5794       299\n",
            "           5     0.5746    0.5354    0.5543       381\n",
            "\n",
            "    accuracy                         0.5558      1623\n",
            "   macro avg     0.6055    0.5304    0.5390      1623\n",
            "weighted avg     0.5896    0.5558    0.5510      1623\n",
            "\n",
            "0.4597794699172179 0.8424238683901587 0.47098379551005204 0.5509719092403832\n",
            "Patience counter: 3\n",
            "Epoch: 12, iter 0: loss = 0.41808614134788513\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 12 loss average: 0.404\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7827    0.7473    0.7646       376\n",
            "           1     0.9229    0.9241    0.9235       764\n",
            "           2     0.8524    0.8611    0.8567      1080\n",
            "           3     0.8584    0.8825    0.8703       749\n",
            "           4     0.8200    0.8500    0.8347       520\n",
            "           5     0.8740    0.8488    0.8612      1210\n",
            "\n",
            "    accuracy                         0.8612      4699\n",
            "   macro avg     0.8518    0.8523    0.8519      4699\n",
            "weighted avg     0.8612    0.8612    0.8611      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7143    0.1736    0.2793       144\n",
            "           1     0.6387    0.6204    0.6294       245\n",
            "           2     0.4824    0.6432    0.5513       384\n",
            "           3     0.5357    0.6176    0.5738       170\n",
            "           4     0.6576    0.5652    0.6079       299\n",
            "           5     0.5636    0.5696    0.5666       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5987    0.5316    0.5347      1623\n",
            "weighted avg     0.5835    0.5638    0.5553      1623\n",
            "\n",
            "0.40386678123225767 0.8611041440767014 0.46853368301305964 0.5553386687246757\n",
            "Patience counter: 4\n",
            "Epoch: 13, iter 0: loss = 0.4518633186817169\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 13 loss average: 0.356\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8227    0.7899    0.8060       376\n",
            "           1     0.9342    0.9476    0.9409       764\n",
            "           2     0.8714    0.8843    0.8778      1080\n",
            "           3     0.8667    0.8678    0.8672       749\n",
            "           4     0.8582    0.8731    0.8656       520\n",
            "           5     0.8889    0.8727    0.8807      1210\n",
            "\n",
            "    accuracy                         0.8802      4699\n",
            "   macro avg     0.8737    0.8726    0.8730      4699\n",
            "weighted avg     0.8800    0.8802    0.8800      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4177    0.4583    0.4371       144\n",
            "           1     0.6667    0.7102    0.6877       245\n",
            "           2     0.4822    0.6719    0.5615       384\n",
            "           3     0.7087    0.5294    0.6061       170\n",
            "           4     0.7405    0.3913    0.5120       299\n",
            "           5     0.5911    0.5958    0.5935       381\n",
            "\n",
            "    accuracy                         0.5742      1623\n",
            "   macro avg     0.6012    0.5595    0.5663      1623\n",
            "weighted avg     0.6012    0.5742    0.5726      1623\n",
            "\n",
            "0.35648594439650577 0.8800185659128816 0.4865800109865206 0.5725725607813659\n",
            "Patience counter: 5\n",
            "Epoch: 14, iter 0: loss = 0.30669355392456055\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 14 loss average: 0.323\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8655    0.7872    0.8245       376\n",
            "           1     0.9241    0.9398    0.9319       764\n",
            "           2     0.8916    0.8991    0.8953      1080\n",
            "           3     0.8793    0.8852    0.8822       749\n",
            "           4     0.8577    0.8923    0.8746       520\n",
            "           5     0.8913    0.8810    0.8861      1210\n",
            "\n",
            "    accuracy                         0.8891      4699\n",
            "   macro avg     0.8849    0.8808    0.8825      4699\n",
            "weighted avg     0.8890    0.8891    0.8889      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5161    0.3333    0.4051       144\n",
            "           1     0.6350    0.6816    0.6575       245\n",
            "           2     0.4807    0.6146    0.5394       384\n",
            "           3     0.6420    0.6118    0.6265       170\n",
            "           4     0.6980    0.4716    0.5629       299\n",
            "           5     0.5607    0.6063    0.5826       381\n",
            "\n",
            "    accuracy                         0.5712      1623\n",
            "   macro avg     0.5887    0.5532    0.5623      1623\n",
            "weighted avg     0.5828    0.5712    0.5689      1623\n",
            "\n",
            "0.32265551170955103 0.8888580570845266 0.4859199450417914 0.5689017393683404\n",
            "Patience counter: 6\n",
            "Epoch: 15, iter 0: loss = 0.2674718499183655\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 15 loss average: 0.274\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9033    0.8697    0.8862       376\n",
            "           1     0.9399    0.9411    0.9405       764\n",
            "           2     0.8959    0.9009    0.8984      1080\n",
            "           3     0.8980    0.9052    0.9016       749\n",
            "           4     0.8942    0.9269    0.9103       520\n",
            "           5     0.9052    0.8917    0.8984      1210\n",
            "\n",
            "    accuracy                         0.9062      4699\n",
            "   macro avg     0.9061    0.9059    0.9059      4699\n",
            "weighted avg     0.9062    0.9062    0.9061      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5867    0.3056    0.4018       144\n",
            "           1     0.5915    0.6857    0.6352       245\n",
            "           2     0.4632    0.6068    0.5254       384\n",
            "           3     0.6047    0.6118    0.6082       170\n",
            "           4     0.7427    0.4247    0.5404       299\n",
            "           5     0.5574    0.6115    0.5832       381\n",
            "\n",
            "    accuracy                         0.5601      1623\n",
            "   macro avg     0.5910    0.5410    0.5490      1623\n",
            "weighted avg     0.5820    0.5601    0.5560      1623\n",
            "\n",
            "0.27361386572010815 0.9061014924220548 0.4751574402918192 0.5560119464881321\n",
            "Patience counter: 7\n",
            "Epoch: 16, iter 0: loss = 0.29227763414382935\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 16 loss average: 0.235\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8965    0.8750    0.8856       376\n",
            "           1     0.9494    0.9581    0.9537       764\n",
            "           2     0.9104    0.9028    0.9066      1080\n",
            "           3     0.9096    0.9399    0.9245       749\n",
            "           4     0.9143    0.9231    0.9187       520\n",
            "           5     0.9353    0.9207    0.9279      1210\n",
            "\n",
            "    accuracy                         0.9223      4699\n",
            "   macro avg     0.9192    0.9199    0.9195      4699\n",
            "weighted avg     0.9223    0.9223    0.9223      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5122    0.2917    0.3717       144\n",
            "           1     0.6792    0.6653    0.6722       245\n",
            "           2     0.4586    0.6484    0.5372       384\n",
            "           3     0.6000    0.5824    0.5910       170\n",
            "           4     0.6794    0.4749    0.5591       299\n",
            "           5     0.5573    0.5617    0.5595       381\n",
            "\n",
            "    accuracy                         0.5601      1623\n",
            "   macro avg     0.5811    0.5374    0.5484      1623\n",
            "weighted avg     0.5753    0.5601    0.5578      1623\n",
            "\n",
            "0.23473993580167493 0.9222577752588307 0.4760261830137514 0.5577878465618276\n",
            "Patience counter: 8\n",
            "Epoch: 17, iter 0: loss = 0.30395713448524475\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 17 loss average: 0.226\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9224    0.8856    0.9037       376\n",
            "           1     0.9517    0.9542    0.9529       764\n",
            "           2     0.9059    0.9093    0.9076      1080\n",
            "           3     0.9052    0.9306    0.9177       749\n",
            "           4     0.9317    0.9442    0.9379       520\n",
            "           5     0.9194    0.9050    0.9121      1210\n",
            "\n",
            "    accuracy                         0.9208      4699\n",
            "   macro avg     0.9227    0.9215    0.9220      4699\n",
            "weighted avg     0.9209    0.9208    0.9208      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4692    0.4236    0.4453       144\n",
            "           1     0.5747    0.6122    0.5929       245\n",
            "           2     0.5068    0.4844    0.4953       384\n",
            "           3     0.6643    0.5471    0.6000       170\n",
            "           4     0.7273    0.4281    0.5389       299\n",
            "           5     0.4900    0.7060    0.5785       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5720    0.5336    0.5418      1623\n",
            "weighted avg     0.5669    0.5465    0.5441      1623\n",
            "\n",
            "0.22646623461817703 0.9207822643700713 0.46503553116803426 0.544137896635743\n",
            "Patience counter: 9\n",
            "Epoch: 18, iter 0: loss = 0.25707653164863586\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 18 loss average: 0.191\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9249    0.9176    0.9212       376\n",
            "           1     0.9622    0.9673    0.9648       764\n",
            "           2     0.9169    0.9398    0.9282      1080\n",
            "           3     0.9319    0.9319    0.9319       749\n",
            "           4     0.9384    0.9077    0.9228       520\n",
            "           5     0.9391    0.9306    0.9348      1210\n",
            "\n",
            "    accuracy                         0.9353      4699\n",
            "   macro avg     0.9356    0.9325    0.9340      4699\n",
            "weighted avg     0.9354    0.9353    0.9353      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4587    0.3472    0.3953       144\n",
            "           1     0.7315    0.4449    0.5533       245\n",
            "           2     0.4231    0.6953    0.5261       384\n",
            "           3     0.5192    0.6353    0.5714       170\n",
            "           4     0.6885    0.4214    0.5228       299\n",
            "           5     0.5598    0.5039    0.5304       381\n",
            "\n",
            "    accuracy                         0.5250      1623\n",
            "   macro avg     0.5635    0.5080    0.5166      1623\n",
            "weighted avg     0.5639    0.5250    0.5237      1623\n",
            "\n",
            "0.1914191679097712 0.9352855126871668 0.4872120333129163 0.5237491337689052\n",
            "Patience counter: 10\n",
            "Epoch: 19, iter 0: loss = 0.1376751810312271\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 19 loss average: 0.170\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9392    0.9043    0.9214       376\n",
            "           1     0.9608    0.9634    0.9621       764\n",
            "           2     0.9442    0.9407    0.9425      1080\n",
            "           3     0.9155    0.9399    0.9275       749\n",
            "           4     0.9235    0.9519    0.9375       520\n",
            "           5     0.9462    0.9306    0.9383      1210\n",
            "\n",
            "    accuracy                         0.9400      4699\n",
            "   macro avg     0.9383    0.9385    0.9382      4699\n",
            "weighted avg     0.9402    0.9400    0.9400      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4297    0.3819    0.4044       144\n",
            "           1     0.6185    0.6816    0.6485       245\n",
            "           2     0.4510    0.6719    0.5397       384\n",
            "           3     0.6667    0.5647    0.6115       170\n",
            "           4     0.6782    0.4582    0.5469       299\n",
            "           5     0.6059    0.4882    0.5407       381\n",
            "\n",
            "    accuracy                         0.5539      1623\n",
            "   macro avg     0.5750    0.5411    0.5486      1623\n",
            "weighted avg     0.5752    0.5539    0.5532      1623\n",
            "\n",
            "0.16988797873879471 0.9399831275123551 0.47080836445443075 0.5532174399055934\n",
            "Patience counter: 11\n",
            "Done! It took 2.4e+02 secs\n",
            "\n",
            "Current RUN: 3\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.9891167655587196\n",
            "Best test f1 weighted\n",
            "0.5928273311134696\n",
            "Best epoch\n",
            "8\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 5.6 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8142807483673096\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 1 loss average: 1.778\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0217    0.0013    0.0025       764\n",
            "           2     0.2613    0.1769    0.2109      1080\n",
            "           3     0.2214    0.1575    0.1841       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2727    0.7636    0.4019      1210\n",
            "\n",
            "    accuracy                         0.2626      4699\n",
            "   macro avg     0.1295    0.1832    0.1332      4699\n",
            "weighted avg     0.1691    0.2626    0.1817      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3737    0.0964    0.1532       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2474    0.9895    0.3958       381\n",
            "\n",
            "    accuracy                         0.2551      1623\n",
            "   macro avg     0.1035    0.1810    0.0915      1623\n",
            "weighted avg     0.1465    0.2551    0.1292      1623\n",
            "\n",
            "1.7779634073376656 0.1817177027997488 0.11689862466793474 0.12916346157525938\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.7906081676483154\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 2 loss average: 1.692\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.2543    0.2683    0.2611       764\n",
            "           2     0.2704    0.3657    0.3109      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2870    0.5769    0.3833      1210\n",
            "\n",
            "    accuracy                         0.2762      4699\n",
            "   macro avg     0.1353    0.2018    0.1592      4699\n",
            "weighted avg     0.1774    0.2762    0.2126      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.2556    0.6490    0.3668       245\n",
            "           2     0.3200    0.0208    0.0391       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3617    0.9265    0.5203       381\n",
            "\n",
            "    accuracy                         0.3204      1623\n",
            "   macro avg     0.1562    0.2661    0.1544      1623\n",
            "weighted avg     0.1992    0.3204    0.1868      1623\n",
            "\n",
            "1.691600315272808 0.2126174400911135 0.1399131489224945 0.18675580559963267\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.232207179069519\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.10it/s]\n",
            "Epoch 3 loss average: 1.482\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7500    0.0239    0.0464       376\n",
            "           1     0.3592    0.4175    0.3862       764\n",
            "           2     0.3102    0.4343    0.3619      1080\n",
            "           3     0.4490    0.0294    0.0551       749\n",
            "           4     0.2833    0.0327    0.0586       520\n",
            "           5     0.3907    0.7033    0.5024      1210\n",
            "\n",
            "    accuracy                         0.3590      4699\n",
            "   macro avg     0.4237    0.2735    0.2351      4699\n",
            "weighted avg     0.3932    0.3590    0.2943      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2561    0.2917    0.2727       144\n",
            "           1     0.8417    0.4122    0.5534       245\n",
            "           2     0.4963    0.7031    0.5819       384\n",
            "           3     1.0000    0.1000    0.1818       170\n",
            "           4     0.5421    0.6455    0.5893       299\n",
            "           5     0.4882    0.5407    0.5131       381\n",
            "\n",
            "    accuracy                         0.5108      1623\n",
            "   macro avg     0.6041    0.4489    0.4487      1623\n",
            "weighted avg     0.5864    0.5108    0.4935      1623\n",
            "\n",
            "1.481551967561245 0.2943116555579724 0.35703432107023914 0.4934723709225316\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.1864104270935059\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.07it/s]\n",
            "Epoch 4 loss average: 1.210\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4868    0.0984    0.1637       376\n",
            "           1     0.5972    0.6597    0.6269       764\n",
            "           2     0.4097    0.3676    0.3875      1080\n",
            "           3     0.6477    0.3805    0.4794       749\n",
            "           4     0.5067    0.5846    0.5429       520\n",
            "           5     0.5068    0.7413    0.6020      1210\n",
            "\n",
            "    accuracy                         0.5159      4699\n",
            "   macro avg     0.5258    0.4720    0.4671      4699\n",
            "weighted avg     0.5200    0.5159    0.4956      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0208    0.0408       144\n",
            "           1     0.7643    0.4898    0.5970       245\n",
            "           2     0.5000    0.6745    0.5743       384\n",
            "           3     0.7604    0.4294    0.5489       170\n",
            "           4     0.5550    0.7090    0.6226       299\n",
            "           5     0.5225    0.6404    0.5755       381\n",
            "\n",
            "    accuracy                         0.5613      1623\n",
            "   macro avg     0.6837    0.4940    0.4932      1623\n",
            "weighted avg     0.6269    0.5613    0.5369      1623\n",
            "\n",
            "1.2096981840829055 0.49559010047308605 0.43310121219462666 0.536903268118937\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.0279649496078491\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.09it/s]\n",
            "Epoch 5 loss average: 0.993\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4778    0.2580    0.3351       376\n",
            "           1     0.7158    0.8207    0.7646       764\n",
            "           2     0.5379    0.5120    0.5247      1080\n",
            "           3     0.7321    0.4633    0.5675       749\n",
            "           4     0.5914    0.7654    0.6672       520\n",
            "           5     0.5903    0.7050    0.6426      1210\n",
            "\n",
            "    accuracy                         0.6118      4699\n",
            "   macro avg     0.6075    0.5874    0.5836      4699\n",
            "weighted avg     0.6124    0.6118    0.6015      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6296    0.1181    0.1988       144\n",
            "           1     0.5088    0.8286    0.6304       245\n",
            "           2     0.5654    0.5964    0.5805       384\n",
            "           3     0.7885    0.4824    0.5985       170\n",
            "           4     0.6370    0.6221    0.6294       299\n",
            "           5     0.5909    0.6142    0.6023       381\n",
            "\n",
            "    accuracy                         0.5860      1623\n",
            "   macro avg     0.6200    0.5436    0.5400      1623\n",
            "weighted avg     0.6051    0.5860    0.5702      1623\n",
            "\n",
            "0.9926336035132408 0.6014654691182877 0.4450666699312597 0.5701974982364149\n",
            "Patience counter: 1\n",
            "Epoch: 6, iter 0: loss = 0.6461880803108215\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 6 loss average: 0.884\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4644    0.3298    0.3857       376\n",
            "           1     0.7803    0.8508    0.8140       764\n",
            "           2     0.6243    0.6417    0.6329      1080\n",
            "           3     0.6763    0.5300    0.5943       749\n",
            "           4     0.5563    0.6462    0.5979       520\n",
            "           5     0.6495    0.6967    0.6722      1210\n",
            "\n",
            "    accuracy                         0.6476      4699\n",
            "   macro avg     0.6252    0.6159    0.6162      4699\n",
            "weighted avg     0.6441    0.6476    0.6427      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3410    0.5139    0.4100       144\n",
            "           1     0.5861    0.7918    0.6736       245\n",
            "           2     0.5633    0.5911    0.5769       384\n",
            "           3     0.8241    0.5235    0.6403       170\n",
            "           4     0.6090    0.3177    0.4176       299\n",
            "           5     0.5956    0.6378    0.6160       381\n",
            "\n",
            "    accuracy                         0.5681      1623\n",
            "   macro avg     0.5865    0.5627    0.5557      1623\n",
            "weighted avg     0.5903    0.5681    0.5631      1623\n",
            "\n",
            "0.8838580083101988 0.6426671874062551 0.4748044199411181 0.5631428852211137\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.8091514706611633\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.01it/s]\n",
            "Epoch 7 loss average: 0.763\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6799    0.5479    0.6068       376\n",
            "           1     0.8125    0.8508    0.8312       764\n",
            "           2     0.6670    0.6472    0.6570      1080\n",
            "           3     0.7714    0.6128    0.6830       749\n",
            "           4     0.6787    0.7962    0.7327       520\n",
            "           5     0.6627    0.7355    0.6972      1210\n",
            "\n",
            "    accuracy                         0.7061      4699\n",
            "   macro avg     0.7120    0.6984    0.7013      4699\n",
            "weighted avg     0.7085    0.7061    0.7042      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4255    0.4167    0.4211       144\n",
            "           1     0.6840    0.6980    0.6909       245\n",
            "           2     0.5514    0.6146    0.5813       384\n",
            "           3     0.7556    0.6000    0.6689       170\n",
            "           4     0.6240    0.5050    0.5582       299\n",
            "           5     0.5808    0.6509    0.6139       381\n",
            "\n",
            "    accuracy                         0.5964      1623\n",
            "   macro avg     0.6035    0.5809    0.5890      1623\n",
            "weighted avg     0.6019    0.5964    0.5962      1623\n",
            "\n",
            "0.7630273258934418 0.7041821993469953 0.49604596859225586 0.5961870992106308\n",
            "Patience counter: 0\n",
            "Epoch: 8, iter 0: loss = 0.6772075891494751\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 8 loss average: 0.645\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7325    0.6117    0.6667       376\n",
            "           1     0.8528    0.8796    0.8660       764\n",
            "           2     0.7396    0.7389    0.7392      1080\n",
            "           3     0.7713    0.6889    0.7278       749\n",
            "           4     0.7445    0.8404    0.7895       520\n",
            "           5     0.7298    0.7612    0.7451      1210\n",
            "\n",
            "    accuracy                         0.7606      4699\n",
            "   macro avg     0.7617    0.7534    0.7557      4699\n",
            "weighted avg     0.7605    0.7606    0.7593      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5946    0.3056    0.4037       144\n",
            "           1     0.6853    0.6490    0.6667       245\n",
            "           2     0.4909    0.6328    0.5529       384\n",
            "           3     0.7600    0.4471    0.5630       170\n",
            "           4     0.7273    0.4548    0.5597       299\n",
            "           5     0.5009    0.7034    0.5852       381\n",
            "\n",
            "    accuracy                         0.5705      1623\n",
            "   macro avg     0.6265    0.5321    0.5552      1623\n",
            "weighted avg     0.6035    0.5705    0.5667      1623\n",
            "\n",
            "0.6453841446588436 0.7592962772142308 0.47041762772741536 0.566706263589111\n",
            "Patience counter: 1\n",
            "Epoch: 9, iter 0: loss = 0.49805206060409546\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 9 loss average: 0.549\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7610    0.7367    0.7486       376\n",
            "           1     0.8661    0.8887    0.8773       764\n",
            "           2     0.7895    0.7917    0.7906      1080\n",
            "           3     0.8326    0.7303    0.7781       749\n",
            "           4     0.7955    0.8231    0.8091       520\n",
            "           5     0.7745    0.8149    0.7942      1210\n",
            "\n",
            "    accuracy                         0.8027      4699\n",
            "   macro avg     0.8032    0.7976    0.7996      4699\n",
            "weighted avg     0.8033    0.8027    0.8023      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4583    0.4583    0.4583       144\n",
            "           1     0.5326    0.8327    0.6497       245\n",
            "           2     0.5119    0.6172    0.5596       384\n",
            "           3     0.6642    0.5235    0.5855       170\n",
            "           4     0.7152    0.3779    0.4945       299\n",
            "           5     0.6188    0.5538    0.5845       381\n",
            "\n",
            "    accuracy                         0.5669      1623\n",
            "   macro avg     0.5835    0.5606    0.5554      1623\n",
            "weighted avg     0.5888    0.5669    0.5608      1623\n",
            "\n",
            "0.549009476788342 0.8023040575544549 0.4678965128105188 0.5607889431679128\n",
            "Patience counter: 2\n",
            "Epoch: 10, iter 0: loss = 0.599281907081604\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 10 loss average: 0.493\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7699    0.6676    0.7151       376\n",
            "           1     0.8833    0.9018    0.8925       764\n",
            "           2     0.8297    0.8120    0.8208      1080\n",
            "           3     0.8391    0.7730    0.8047       749\n",
            "           4     0.7856    0.8596    0.8209       520\n",
            "           5     0.8019    0.8463    0.8235      1210\n",
            "\n",
            "    accuracy                         0.8229      4699\n",
            "   macro avg     0.8183    0.8101    0.8129      4699\n",
            "weighted avg     0.8231    0.8229    0.8221      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6322    0.3819    0.4762       144\n",
            "           1     0.6007    0.7429    0.6642       245\n",
            "           2     0.5022    0.5833    0.5398       384\n",
            "           3     0.5193    0.7118    0.6005       170\n",
            "           4     0.7424    0.4916    0.5915       299\n",
            "           5     0.5309    0.4961    0.5129       381\n",
            "\n",
            "    accuracy                         0.5656      1623\n",
            "   macro avg     0.5880    0.5679    0.5642      1623\n",
            "weighted avg     0.5814    0.5656    0.5625      1623\n",
            "\n",
            "0.4933321876451373 0.8221358445859792 0.4735303796859166 0.5625045333320037\n",
            "Patience counter: 3\n",
            "Epoch: 11, iter 0: loss = 0.39130306243896484\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 11 loss average: 0.434\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7455    0.7713    0.7582       376\n",
            "           1     0.8778    0.9215    0.8991       764\n",
            "           2     0.8513    0.8537    0.8525      1080\n",
            "           3     0.8531    0.8451    0.8491       749\n",
            "           4     0.8242    0.7846    0.8039       520\n",
            "           5     0.8577    0.8421    0.8499      1210\n",
            "\n",
            "    accuracy                         0.8461      4699\n",
            "   macro avg     0.8350    0.8364    0.8355      4699\n",
            "weighted avg     0.8461    0.8461    0.8459      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4419    0.2639    0.3304       144\n",
            "           1     0.7561    0.5061    0.6064       245\n",
            "           2     0.5325    0.5339    0.5332       384\n",
            "           3     0.5967    0.6353    0.6154       170\n",
            "           4     0.5565    0.6421    0.5963       299\n",
            "           5     0.5152    0.6247    0.5647       381\n",
            "\n",
            "    accuracy                         0.5576      1623\n",
            "   macro avg     0.5665    0.5343    0.5410      1623\n",
            "weighted avg     0.5653    0.5576    0.5539      1623\n",
            "\n",
            "0.4337154927973946 0.8459416509525902 0.4943729418244959 0.5538547488806763\n",
            "Patience counter: 4\n",
            "Epoch: 12, iter 0: loss = 0.3375178575515747\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.08it/s]\n",
            "Epoch 12 loss average: 0.371\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8095    0.7686    0.7885       376\n",
            "           1     0.9056    0.9162    0.9109       764\n",
            "           2     0.8526    0.8676    0.8600      1080\n",
            "           3     0.8911    0.8518    0.8710       749\n",
            "           4     0.8290    0.8673    0.8477       520\n",
            "           5     0.8612    0.8612    0.8612      1210\n",
            "\n",
            "    accuracy                         0.8634      4699\n",
            "   macro avg     0.8582    0.8555    0.8566      4699\n",
            "weighted avg     0.8635    0.8634    0.8633      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4597    0.3958    0.4254       144\n",
            "           1     0.6316    0.6857    0.6575       245\n",
            "           2     0.4886    0.6719    0.5658       384\n",
            "           3     0.5434    0.7000    0.6118       170\n",
            "           4     0.6791    0.4247    0.5226       299\n",
            "           5     0.6054    0.4751    0.5324       381\n",
            "\n",
            "    accuracy                         0.5607      1623\n",
            "   macro avg     0.5680    0.5589    0.5526      1623\n",
            "weighted avg     0.5759    0.5607    0.5562      1623\n",
            "\n",
            "0.3713784671078126 0.8632518356082008 0.47382427118524695 0.5562027250311141\n",
            "Patience counter: 5\n",
            "Epoch: 13, iter 0: loss = 0.2078472524881363\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 13 loss average: 0.317\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8584    0.7899    0.8227       376\n",
            "           1     0.9251    0.9372    0.9311       764\n",
            "           2     0.8937    0.8870    0.8903      1080\n",
            "           3     0.8966    0.8678    0.8820       749\n",
            "           4     0.8603    0.9000    0.8797       520\n",
            "           5     0.8821    0.9025    0.8922      1210\n",
            "\n",
            "    accuracy                         0.8898      4699\n",
            "   macro avg     0.8860    0.8807    0.8830      4699\n",
            "weighted avg     0.8897    0.8898    0.8895      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4094    0.4236    0.4164       144\n",
            "           1     0.5799    0.7551    0.6560       245\n",
            "           2     0.4534    0.6849    0.5456       384\n",
            "           3     0.6742    0.5235    0.5894       170\n",
            "           4     0.7073    0.2910    0.4123       299\n",
            "           5     0.6281    0.5276    0.5735       381\n",
            "\n",
            "    accuracy                         0.5459      1623\n",
            "   macro avg     0.5754    0.5343    0.5322      1623\n",
            "weighted avg     0.5795    0.5459    0.5374      1623\n",
            "\n",
            "0.3173846947029233 0.8895048991787721 0.45273037706356206 0.5373916998742617\n",
            "Patience counter: 6\n",
            "Epoch: 14, iter 0: loss = 0.4903028607368469\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.02it/s]\n",
            "Epoch 14 loss average: 0.289\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8246    0.8378    0.8311       376\n",
            "           1     0.9303    0.9437    0.9370       764\n",
            "           2     0.9006    0.9065    0.9036      1080\n",
            "           3     0.8966    0.8919    0.8942       749\n",
            "           4     0.8839    0.8635    0.8735       520\n",
            "           5     0.9018    0.8959    0.8988      1210\n",
            "\n",
            "    accuracy                         0.8972      4699\n",
            "   macro avg     0.8897    0.8899    0.8897      4699\n",
            "weighted avg     0.8972    0.8972    0.8972      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4853    0.2292    0.3113       144\n",
            "           1     0.5939    0.7102    0.6468       245\n",
            "           2     0.5215    0.5365    0.5289       384\n",
            "           3     0.6800    0.6000    0.6375       170\n",
            "           4     0.5859    0.5819    0.5839       299\n",
            "           5     0.5548    0.6115    0.5818       381\n",
            "\n",
            "    accuracy                         0.5681      1623\n",
            "   macro avg     0.5702    0.5449    0.5484      1623\n",
            "weighted avg     0.5655    0.5681    0.5613      1623\n",
            "\n",
            "0.2891905716775606 0.8971729947644799 0.4664166409021404 0.5613133030991402\n",
            "Patience counter: 7\n",
            "Epoch: 15, iter 0: loss = 0.0919908806681633\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 15 loss average: 0.252\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8803    0.8218    0.8501       376\n",
            "           1     0.9198    0.9607    0.9398       764\n",
            "           2     0.9050    0.9000    0.9025      1080\n",
            "           3     0.8983    0.9079    0.9031       749\n",
            "           4     0.8868    0.9192    0.9027       520\n",
            "           5     0.9119    0.8893    0.9004      1210\n",
            "\n",
            "    accuracy                         0.9042      4699\n",
            "   macro avg     0.9004    0.8998    0.8998      4699\n",
            "weighted avg     0.9041    0.9042    0.9040      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4430    0.2431    0.3139       144\n",
            "           1     0.6565    0.6163    0.6358       245\n",
            "           2     0.4747    0.6120    0.5347       384\n",
            "           3     0.6000    0.6353    0.6171       170\n",
            "           4     0.5621    0.5452    0.5535       299\n",
            "           5     0.5673    0.5197    0.5425       381\n",
            "\n",
            "    accuracy                         0.5484      1623\n",
            "   macro avg     0.5506    0.5286    0.5329      1623\n",
            "weighted avg     0.5503    0.5484    0.5443      1623\n",
            "\n",
            "0.25187192717567086 0.9039528561843928 0.4823532991484344 0.5442876443406132\n",
            "Patience counter: 8\n",
            "Epoch: 16, iter 0: loss = 0.19969309866428375\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.02it/s]\n",
            "Epoch 16 loss average: 0.210\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8913    0.8723    0.8817       376\n",
            "           1     0.9565    0.9490    0.9527       764\n",
            "           2     0.9296    0.9287    0.9291      1080\n",
            "           3     0.9102    0.9332    0.9216       749\n",
            "           4     0.9084    0.9154    0.9119       520\n",
            "           5     0.9276    0.9215    0.9245      1210\n",
            "\n",
            "    accuracy                         0.9249      4699\n",
            "   macro avg     0.9206    0.9200    0.9203      4699\n",
            "weighted avg     0.9249    0.9249    0.9249      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4947    0.3264    0.3933       144\n",
            "           1     0.5444    0.7755    0.6397       245\n",
            "           2     0.5105    0.5703    0.5387       384\n",
            "           3     0.7164    0.5647    0.6316       170\n",
            "           4     0.6320    0.4883    0.5509       299\n",
            "           5     0.5974    0.6037    0.6005       381\n",
            "\n",
            "    accuracy                         0.5718      1623\n",
            "   macro avg     0.5826    0.5548    0.5591      1623\n",
            "weighted avg     0.5786    0.5718    0.5676      1623\n",
            "\n",
            "0.20986121233242253 0.9248710729836888 0.4705615584172532 0.567558638889587\n",
            "Patience counter: 9\n",
            "Epoch: 17, iter 0: loss = 0.1359449326992035\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 17 loss average: 0.180\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9187    0.9016    0.9101       376\n",
            "           1     0.9560    0.9673    0.9616       764\n",
            "           2     0.9333    0.9463    0.9398      1080\n",
            "           3     0.9387    0.9199    0.9292       749\n",
            "           4     0.9349    0.9385    0.9367       520\n",
            "           5     0.9370    0.9339    0.9354      1210\n",
            "\n",
            "    accuracy                         0.9379      4699\n",
            "   macro avg     0.9364    0.9346    0.9355      4699\n",
            "weighted avg     0.9378    0.9379    0.9378      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4155    0.4097    0.4126       144\n",
            "           1     0.5948    0.6531    0.6226       245\n",
            "           2     0.4938    0.5182    0.5057       384\n",
            "           3     0.6620    0.5529    0.6026       170\n",
            "           4     0.5885    0.4783    0.5277       299\n",
            "           5     0.5236    0.5827    0.5516       381\n",
            "\n",
            "    accuracy                         0.5404      1623\n",
            "   macro avg     0.5464    0.5325    0.5371      1623\n",
            "weighted avg     0.5441    0.5404    0.5400      1623\n",
            "\n",
            "0.17982724704779685 0.9377980167251556 0.4653788631518855 0.540043050018773\n",
            "Patience counter: 10\n",
            "Epoch: 18, iter 0: loss = 0.06395397335290909\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "Epoch 18 loss average: 0.163\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9064    0.9016    0.9040       376\n",
            "           1     0.9573    0.9673    0.9622       764\n",
            "           2     0.9317    0.9472    0.9394      1080\n",
            "           3     0.9432    0.9306    0.9368       749\n",
            "           4     0.9480    0.9462    0.9471       520\n",
            "           5     0.9465    0.9364    0.9414      1210\n",
            "\n",
            "    accuracy                         0.9413      4699\n",
            "   macro avg     0.9388    0.9382    0.9385      4699\n",
            "weighted avg     0.9413    0.9413    0.9412      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4615    0.2917    0.3574       144\n",
            "           1     0.6077    0.6449    0.6257       245\n",
            "           2     0.4921    0.5677    0.5272       384\n",
            "           3     0.5215    0.6412    0.5752       170\n",
            "           4     0.5992    0.5151    0.5540       299\n",
            "           5     0.5427    0.5171    0.5296       381\n",
            "\n",
            "    accuracy                         0.5410      1623\n",
            "   macro avg     0.5375    0.5296    0.5282      1623\n",
            "weighted avg     0.5415    0.5410    0.5375      1623\n",
            "\n",
            "0.16275951828962812 0.9412380088960522 0.4716915451741982 0.5375289803319945\n",
            "Patience counter: 11\n",
            "Done! It took 2.3e+02 secs\n",
            "\n",
            "Current RUN: 4\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0128306075930595\n",
            "Best test f1 weighted\n",
            "0.5961870992106308\n",
            "Best epoch\n",
            "7\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 50,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/drive/My Drive/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 5.6 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8817942142486572\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  4.87it/s]\n",
            "Epoch 1 loss average: 1.760\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0220    0.0053    0.0086       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2374    0.1046    0.1452      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2560    0.8727    0.3959      1210\n",
            "\n",
            "    accuracy                         0.2492      4699\n",
            "   macro avg     0.0859    0.1638    0.0916      4699\n",
            "weighted avg     0.1222    0.2492    0.1360      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3643    0.1224    0.1832       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2463    0.9659    0.3925       381\n",
            "\n",
            "    accuracy                         0.2557      1623\n",
            "   macro avg     0.1018    0.1814    0.0960      1623\n",
            "weighted avg     0.1440    0.2557    0.1355      1623\n",
            "\n",
            "1.7602965881427128 0.13600651611766337 0.11868590454004195 0.13550078441122318\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 2.2779738903045654\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 2 loss average: 1.720\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2977    0.2009    0.2399      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0909    0.0019    0.0038       520\n",
            "           5     0.2743    0.8975    0.4202      1210\n",
            "\n",
            "    accuracy                         0.2775      4699\n",
            "   macro avg     0.1105    0.1834    0.1106      4699\n",
            "weighted avg     0.1491    0.2775    0.1638      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3079    0.6927    0.4263       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.8261    0.0635    0.1180       299\n",
            "           5     0.3709    0.7165    0.4888       381\n",
            "\n",
            "    accuracy                         0.3438      1623\n",
            "   macro avg     0.2508    0.2455    0.1722      1623\n",
            "weighted avg     0.3121    0.3438    0.2373      1623\n",
            "\n",
            "1.7202712818980217 0.16375863288744372 0.1666570996649747 0.23734711604252576\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.5929768085479736\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 3 loss average: 1.474\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4444    0.0213    0.0406       376\n",
            "           1     0.4419    0.1741    0.2498       764\n",
            "           2     0.3909    0.5028    0.4399      1080\n",
            "           3     0.4461    0.2043    0.2802       749\n",
            "           4     0.5157    0.4115    0.4578       520\n",
            "           5     0.3717    0.6860    0.4821      1210\n",
            "\n",
            "    accuracy                         0.4003      4699\n",
            "   macro avg     0.4351    0.3333    0.3251      4699\n",
            "weighted avg     0.4211    0.4003    0.3644      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2931    0.1181    0.1683       144\n",
            "           1     0.6531    0.5224    0.5805       245\n",
            "           2     0.4359    0.5755    0.4961       384\n",
            "           3     0.8261    0.3353    0.4770       170\n",
            "           4     0.6471    0.4783    0.5500       299\n",
            "           5     0.4825    0.7244    0.5792       381\n",
            "\n",
            "    accuracy                         0.5188      1623\n",
            "   macro avg     0.5563    0.4590    0.4752      1623\n",
            "weighted avg     0.5467    0.5188    0.5072      1623\n",
            "\n",
            "1.4738574847579002 0.36442551661305705 0.38970390306507613 0.507192513791758\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.0677651166915894\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 4 loss average: 1.184\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3952    0.2606    0.3141       376\n",
            "           1     0.6671    0.6427    0.6547       764\n",
            "           2     0.4888    0.4833    0.4860      1080\n",
            "           3     0.6699    0.4606    0.5459       749\n",
            "           4     0.5758    0.6212    0.5976       520\n",
            "           5     0.5111    0.6636    0.5775      1210\n",
            "\n",
            "    accuracy                         0.5495      4699\n",
            "   macro avg     0.5513    0.5220    0.5293      4699\n",
            "weighted avg     0.5545    0.5495    0.5451      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4167    0.0694    0.1190       144\n",
            "           1     0.6137    0.8041    0.6961       245\n",
            "           2     0.5754    0.4271    0.4903       384\n",
            "           3     0.6389    0.2706    0.3802       170\n",
            "           4     0.3836    0.9097    0.5397       299\n",
            "           5     0.6981    0.3885    0.4992       381\n",
            "\n",
            "    accuracy                         0.5157      1623\n",
            "   macro avg     0.5544    0.4782    0.4541      1623\n",
            "weighted avg     0.5672    0.5157    0.4881      1623\n",
            "\n",
            "1.1844604735573132 0.5451298383734403 0.37706184520105823 0.4880662657212521\n",
            "Patience counter: 1\n",
            "Epoch: 5, iter 0: loss = 1.040434718132019\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 5 loss average: 1.028\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6348    0.3005    0.4079       376\n",
            "           1     0.6689    0.7932    0.7257       764\n",
            "           2     0.5362    0.5481    0.5421      1080\n",
            "           3     0.7255    0.4446    0.5513       749\n",
            "           4     0.5714    0.7154    0.6354       520\n",
            "           5     0.5496    0.6364    0.5898      1210\n",
            "\n",
            "    accuracy                         0.5929      4699\n",
            "   macro avg     0.6144    0.5730    0.5754      4699\n",
            "weighted avg     0.6032    0.5929    0.5853      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4615    0.2083    0.2871       144\n",
            "           1     0.5540    0.8163    0.6601       245\n",
            "           2     0.6177    0.4714    0.5347       384\n",
            "           3     0.6154    0.6588    0.6364       170\n",
            "           4     0.5910    0.7057    0.6433       299\n",
            "           5     0.5945    0.5696    0.5818       381\n",
            "\n",
            "    accuracy                         0.5860      1623\n",
            "   macro avg     0.5724    0.5717    0.5572      1623\n",
            "weighted avg     0.5836    0.5860    0.5734      1623\n",
            "\n",
            "1.0283149133125942 0.5853057896439169 0.4213305959954844 0.5733615309893668\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.9174162745475769\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 6 loss average: 0.866\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6283    0.4495    0.5240       376\n",
            "           1     0.7711    0.8377    0.8030       764\n",
            "           2     0.6322    0.5824    0.6063      1080\n",
            "           3     0.7438    0.6048    0.6672       749\n",
            "           4     0.6141    0.7712    0.6837       520\n",
            "           5     0.6359    0.7058    0.6690      1210\n",
            "\n",
            "    accuracy                         0.6695      4699\n",
            "   macro avg     0.6709    0.6586    0.6589      4699\n",
            "weighted avg     0.6712    0.6695    0.6661      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5636    0.2153    0.3116       144\n",
            "           1     0.7705    0.5755    0.6589       245\n",
            "           2     0.4846    0.5729    0.5251       384\n",
            "           3     0.5864    0.6588    0.6205       170\n",
            "           4     0.6640    0.5552    0.6047       299\n",
            "           5     0.4980    0.6404    0.5603       381\n",
            "\n",
            "    accuracy                         0.5632      1623\n",
            "   macro avg     0.5945    0.5364    0.5468      1623\n",
            "weighted avg     0.5816    0.5632    0.5593      1623\n",
            "\n",
            "0.865806524331371 0.6661091262023668 0.45872168813114644 0.5592595461569073\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 1.1112539768218994\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  4.97it/s]\n",
            "Epoch 7 loss average: 0.764\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7000    0.5957    0.6437       376\n",
            "           1     0.7848    0.8259    0.8048       764\n",
            "           2     0.6853    0.6815    0.6834      1080\n",
            "           3     0.7368    0.6689    0.7012       749\n",
            "           4     0.7138    0.7673    0.7396       520\n",
            "           5     0.7029    0.7331    0.7176      1210\n",
            "\n",
            "    accuracy                         0.7189      4699\n",
            "   macro avg     0.7206    0.7121    0.7151      4699\n",
            "weighted avg     0.7185    0.7189    0.7178      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3922    0.1389    0.2051       144\n",
            "           1     0.7887    0.4571    0.5788       245\n",
            "           2     0.5346    0.6432    0.5839       384\n",
            "           3     0.6296    0.6000    0.6145       170\n",
            "           4     0.5504    0.8395    0.6649       299\n",
            "           5     0.5657    0.5197    0.5417       381\n",
            "\n",
            "    accuracy                         0.5730      1623\n",
            "   macro avg     0.5769    0.5331    0.5315      1623\n",
            "weighted avg     0.5805    0.5730    0.5578      1623\n",
            "\n",
            "0.7642816783239444 0.7178308084474204 0.45389110071227473 0.5577535696781262\n",
            "Patience counter: 2\n",
            "Epoch: 8, iter 0: loss = 0.5953792333602905\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.04it/s]\n",
            "Epoch 8 loss average: 0.661\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7126    0.6596    0.6851       376\n",
            "           1     0.8302    0.8770    0.8530       764\n",
            "           2     0.7333    0.7130    0.7230      1080\n",
            "           3     0.7779    0.7156    0.7455       749\n",
            "           4     0.7310    0.7942    0.7613       520\n",
            "           5     0.7460    0.7645    0.7551      1210\n",
            "\n",
            "    accuracy                         0.7580      4699\n",
            "   macro avg     0.7552    0.7540    0.7538      4699\n",
            "weighted avg     0.7575    0.7580    0.7572      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4000    0.3889    0.3944       144\n",
            "           1     0.5393    0.7837    0.6389       245\n",
            "           2     0.5021    0.6250    0.5568       384\n",
            "           3     0.6124    0.6412    0.6264       170\n",
            "           4     0.7080    0.2676    0.3883       299\n",
            "           5     0.5866    0.5512    0.5683       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5581    0.5429    0.5289      1623\n",
            "weighted avg     0.5680    0.5465    0.5338      1623\n",
            "\n",
            "0.6610218988110622 0.757183746532444 0.43824811233380967 0.5337663321956787\n",
            "Patience counter: 3\n",
            "Epoch: 9, iter 0: loss = 0.4474712312221527\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 9 loss average: 0.575\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7515    0.6596    0.7025       376\n",
            "           1     0.8352    0.8691    0.8518       764\n",
            "           2     0.7863    0.7870    0.7867      1080\n",
            "           3     0.8254    0.7824    0.8033       749\n",
            "           4     0.7701    0.8115    0.7903       520\n",
            "           5     0.7911    0.8074    0.7992      1210\n",
            "\n",
            "    accuracy                         0.7974      4699\n",
            "   macro avg     0.7933    0.7862    0.7890      4699\n",
            "weighted avg     0.7971    0.7974    0.7968      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4158    0.5486    0.4731       144\n",
            "           1     0.5727    0.7714    0.6574       245\n",
            "           2     0.4799    0.5599    0.5168       384\n",
            "           3     0.6273    0.5941    0.6103       170\n",
            "           4     0.7477    0.2776    0.4049       299\n",
            "           5     0.5483    0.5512    0.5497       381\n",
            "\n",
            "    accuracy                         0.5404      1623\n",
            "   macro avg     0.5653    0.5505    0.5354      1623\n",
            "weighted avg     0.5691    0.5404    0.5311      1623\n",
            "\n",
            "0.5750125413760543 0.796802016685157 0.4513414413522902 0.5310518733433292\n",
            "Patience counter: 4\n",
            "Epoch: 10, iter 0: loss = 0.7848185896873474\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.01it/s]\n",
            "Epoch 10 loss average: 0.499\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7207    0.7686    0.7439       376\n",
            "           1     0.8870    0.8940    0.8905       764\n",
            "           2     0.8131    0.8019    0.8075      1080\n",
            "           3     0.8280    0.8224    0.8252       749\n",
            "           4     0.8217    0.7712    0.7956       520\n",
            "           5     0.8221    0.8364    0.8292      1210\n",
            "\n",
            "    accuracy                         0.8229      4699\n",
            "   macro avg     0.8154    0.8157    0.8153      4699\n",
            "weighted avg     0.8234    0.8229    0.8230      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5811    0.2986    0.3945       144\n",
            "           1     0.5487    0.7592    0.6370       245\n",
            "           2     0.5139    0.5781    0.5441       384\n",
            "           3     0.6556    0.5824    0.6168       170\n",
            "           4     0.6600    0.5518    0.6011       299\n",
            "           5     0.5756    0.5696    0.5726       381\n",
            "\n",
            "    accuracy                         0.5742      1623\n",
            "   macro avg     0.5891    0.5566    0.5610      1623\n",
            "weighted avg     0.5814    0.5742    0.5696      1623\n",
            "\n",
            "0.4989188062027097 0.8229778173760195 0.4549221310309437 0.5696499491864765\n",
            "Patience counter: 5\n",
            "Epoch: 11, iter 0: loss = 0.8998795747756958\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.05it/s]\n",
            "Epoch 11 loss average: 0.440\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8098    0.7021    0.7521       376\n",
            "           1     0.8794    0.9162    0.8974       764\n",
            "           2     0.8442    0.8176    0.8307      1080\n",
            "           3     0.8554    0.8451    0.8502       749\n",
            "           4     0.7886    0.8750    0.8295       520\n",
            "           5     0.8311    0.8339    0.8325      1210\n",
            "\n",
            "    accuracy                         0.8393      4699\n",
            "   macro avg     0.8347    0.8317    0.8321      4699\n",
            "weighted avg     0.8394    0.8393    0.8387      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4692    0.4236    0.4453       144\n",
            "           1     0.6444    0.7102    0.6757       245\n",
            "           2     0.4782    0.6562    0.5532       384\n",
            "           3     0.5567    0.6353    0.5934       170\n",
            "           4     0.7207    0.4314    0.5397       299\n",
            "           5     0.5882    0.4987    0.5398       381\n",
            "\n",
            "    accuracy                         0.5632      1623\n",
            "   macro avg     0.5762    0.5592    0.5579      1623\n",
            "weighted avg     0.5812    0.5632    0.5607      1623\n",
            "\n",
            "0.4397815448852877 0.8387071909190076 0.4729285984880071 0.5607092558864085\n",
            "Patience counter: 6\n",
            "Epoch: 12, iter 0: loss = 0.2838682532310486\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 12 loss average: 0.384\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8150    0.8085    0.8117       376\n",
            "           1     0.8912    0.9110    0.9010       764\n",
            "           2     0.8565    0.8620    0.8593      1080\n",
            "           3     0.8669    0.8611    0.8640       749\n",
            "           4     0.8544    0.8462    0.8502       520\n",
            "           5     0.8766    0.8686    0.8726      1210\n",
            "\n",
            "    accuracy                         0.8655      4699\n",
            "   macro avg     0.8601    0.8596    0.8598      4699\n",
            "weighted avg     0.8654    0.8655    0.8654      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4639    0.3125    0.3734       144\n",
            "           1     0.6594    0.6163    0.6371       245\n",
            "           2     0.5168    0.6016    0.5560       384\n",
            "           3     0.4977    0.6353    0.5581       170\n",
            "           4     0.6383    0.6020    0.6196       299\n",
            "           5     0.5413    0.4987    0.5191       381\n",
            "\n",
            "    accuracy                         0.5576      1623\n",
            "   macro avg     0.5529    0.5444    0.5439      1623\n",
            "weighted avg     0.5598    0.5576    0.5553      1623\n",
            "\n",
            "0.38399634246403974 0.8654259967203995 0.4692671774943593 0.5553282392063421\n",
            "Patience counter: 7\n",
            "Epoch: 13, iter 0: loss = 0.2384532392024994\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.02it/s]\n",
            "Epoch 13 loss average: 0.322\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8750    0.8005    0.8361       376\n",
            "           1     0.9229    0.9398    0.9313       764\n",
            "           2     0.8923    0.8824    0.8873      1080\n",
            "           3     0.8895    0.8919    0.8907       749\n",
            "           4     0.8561    0.9154    0.8848       520\n",
            "           5     0.8952    0.8893    0.8922      1210\n",
            "\n",
            "    accuracy                         0.8921      4699\n",
            "   macro avg     0.8885    0.8865    0.8871      4699\n",
            "weighted avg     0.8922    0.8921    0.8919      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3713    0.4306    0.3987       144\n",
            "           1     0.7020    0.5673    0.6275       245\n",
            "           2     0.4752    0.6745    0.5576       384\n",
            "           3     0.5436    0.6235    0.5808       170\n",
            "           4     0.6562    0.4214    0.5132       299\n",
            "           5     0.5890    0.5039    0.5431       381\n",
            "\n",
            "    accuracy                         0.5447      1623\n",
            "   macro avg     0.5562    0.5369    0.5368      1623\n",
            "weighted avg     0.5674    0.5447    0.5449      1623\n",
            "\n",
            "0.32178775127977133 0.8918781652924598 0.46677143094046875 0.5449232259939999\n",
            "Patience counter: 8\n",
            "Epoch: 14, iter 0: loss = 0.3299606144428253\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 14 loss average: 0.292\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8873    0.8378    0.8618       376\n",
            "           1     0.9276    0.9385    0.9330       764\n",
            "           2     0.8903    0.8870    0.8887      1080\n",
            "           3     0.8852    0.8852    0.8852       749\n",
            "           4     0.8977    0.9115    0.9046       520\n",
            "           5     0.8892    0.8950    0.8921      1210\n",
            "\n",
            "    accuracy                         0.8959      4699\n",
            "   macro avg     0.8962    0.8925    0.8942      4699\n",
            "weighted avg     0.8958    0.8959    0.8958      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4155    0.4097    0.4126       144\n",
            "           1     0.6910    0.5020    0.5816       245\n",
            "           2     0.4422    0.6979    0.5414       384\n",
            "           3     0.5893    0.5824    0.5858       170\n",
            "           4     0.7634    0.3344    0.4651       299\n",
            "           5     0.5452    0.5696    0.5571       381\n",
            "\n",
            "    accuracy                         0.5336      1623\n",
            "   macro avg     0.5744    0.5160    0.5239      1623\n",
            "weighted avg     0.5762    0.5336    0.5303      1623\n",
            "\n",
            "0.29185950321455795 0.8958164471041468 0.4668927944171645 0.5303252588227829\n",
            "Patience counter: 9\n",
            "Epoch: 15, iter 0: loss = 0.08794139325618744\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.06it/s]\n",
            "Epoch 15 loss average: 0.247\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8509    0.8803    0.8654       376\n",
            "           1     0.9464    0.9476    0.9470       764\n",
            "           2     0.9125    0.9074    0.9099      1080\n",
            "           3     0.9163    0.9065    0.9114       749\n",
            "           4     0.9191    0.8962    0.9075       520\n",
            "           5     0.9043    0.9140    0.9092      1210\n",
            "\n",
            "    accuracy                         0.9121      4699\n",
            "   macro avg     0.9083    0.9087    0.9084      4699\n",
            "weighted avg     0.9123    0.9121    0.9122      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4845    0.3264    0.3900       144\n",
            "           1     0.5529    0.6612    0.6022       245\n",
            "           2     0.4679    0.6458    0.5427       384\n",
            "           3     0.5579    0.6235    0.5889       170\n",
            "           4     0.7111    0.4281    0.5344       299\n",
            "           5     0.5556    0.4856    0.5182       381\n",
            "\n",
            "    accuracy                         0.5397      1623\n",
            "   macro avg     0.5550    0.5284    0.5294      1623\n",
            "weighted avg     0.5570    0.5397    0.5357      1623\n",
            "\n",
            "0.24700093129649758 0.9121656206469421 0.4570087077873603 0.5357025460380921\n",
            "Patience counter: 10\n",
            "Epoch: 16, iter 0: loss = 0.08467011153697968\n",
            "100%|███████████████████████████████████████████| 48/48 [00:09<00:00,  5.09it/s]\n",
            "Epoch 16 loss average: 0.206\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9162    0.9016    0.9088       376\n",
            "           1     0.9496    0.9620    0.9558       764\n",
            "           2     0.9302    0.9130    0.9215      1080\n",
            "           3     0.9225    0.9212    0.9218       749\n",
            "           4     0.9371    0.9462    0.9416       520\n",
            "           5     0.9116    0.9207    0.9161      1210\n",
            "\n",
            "    accuracy                         0.9270      4699\n",
            "   macro avg     0.9279    0.9274    0.9276      4699\n",
            "weighted avg     0.9270    0.9270    0.9270      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5089    0.3958    0.4453       144\n",
            "           1     0.6062    0.5592    0.5817       245\n",
            "           2     0.4779    0.6198    0.5397       384\n",
            "           3     0.5044    0.6765    0.5779       170\n",
            "           4     0.7254    0.4682    0.5691       299\n",
            "           5     0.4973    0.4777    0.4873       381\n",
            "\n",
            "    accuracy                         0.5354      1623\n",
            "   macro avg     0.5533    0.5329    0.5335      1623\n",
            "weighted avg     0.5529    0.5354    0.5348      1623\n",
            "\n",
            "0.20592734749273708 0.9269573750750393 0.4625497009584693 0.53478007433536\n",
            "Patience counter: 11\n",
            "Done! It took 2e+02 secs\n",
            "\n",
            "Current RUN: 5\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0595774091780186\n",
            "Best test f1 weighted\n",
            "0.5733615309893668\n",
            "Best epoch\n",
            "5\n",
            "\n",
            "\n",
            "Average across runs:\n",
            "Best epoch\n",
            "[6, 6, 8, 7, 5]\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0333176508545876\n",
            "Overall test f1 weighted\n",
            "[0.56365691 0.56679104 0.59282733 0.5961871  0.57336153]\n",
            "Best test f1 weighted\n",
            "0.5785647823434946\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
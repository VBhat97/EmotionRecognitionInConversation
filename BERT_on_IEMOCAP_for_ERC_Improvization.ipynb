{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_on_IEMOCAP_for_ERC_Improvization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO99DbIGNc+BZmzjlRGsfZe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anitayadav3/EmotionRecognitionInConversation/blob/master/BERT_on_IEMOCAP_for_ERC_Improvization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbKXIVqMGym"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam  \n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "import pickle\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils.np_utils import to_categorical  \n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNTNsf6SMN0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5e03f8-c3e9-4d10-9541-26920be6f9da"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 22.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 17.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 16.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 13.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 12.9MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 14.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 13.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 14.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 14.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r72yzfDkMSVU"
      },
      "source": [
        "import tokenization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAuiS6t4MW_4"
      },
      "source": [
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R5uSnu5MZQ3"
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    clf_output1 = tf.reshape(clf_output,[1,clf_output.shape[1],1])\n",
        "    text_layer = Conv1D(128, 100, activation='relu', input_shape=(clf_output.shape[1],1))(clf_output1)\n",
        "    text_layer = Conv1D(128, 90, activation='relu')(text_layer)\n",
        "    text_layer = MaxPooling1D(3)(text_layer)\n",
        "    text_layer = Flatten()(text_layer)\n",
        "    dense_layer1 = Dense(100,activation='relu')(text_layer)\n",
        "    dense_layer2 = Dense(100,activation='relu')(dense_layer1)\n",
        "    dense_layer3 = Dense(100,activation='relu')(dense_layer2)\n",
        "    out = Dense(6, activation='softmax')(dense_layer3)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=2e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0lDyjgKQrbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81219de3-82e4-4b62-bcd4-559130648431"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPBiw3jdQqXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4587a8-d45e-4edd-e8fd-a5aa52214809"
      },
      "source": [
        "%%time\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 41.8 s, sys: 12.1 s, total: 53.9 s\n",
            "Wall time: 2min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-g0HXbfQuZa"
      },
      "source": [
        "with open('/content/gdrive/My Drive/iemocap/train/sentences.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "with open('/content/gdrive/My Drive/iemocap/train/labels.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f)\n",
        "with open('/content/gdrive/My Drive/iemocap/test/sentences.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "with open('/content/gdrive/My Drive/iemocap/test/labels.pkl', 'rb') as f:\n",
        "    test_labels = pickle.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTVssce7QwPd"
      },
      "source": [
        "def preprocessing(data,labels):\n",
        "  processed_data=[]\n",
        "  processed_label=[]\n",
        "  for i in range(0,len(data)):\n",
        "    for j in range(0,len(data[i])):\n",
        "      intermediate_data=[]\n",
        "      intermediate_label=[]\n",
        "      for k in range(0,len(data[i][j])):\n",
        "        text=data[i][j][k]\n",
        "        if text != '<eos>'and text!='<pad>':\n",
        "          intermediate_data.append(text)\n",
        "      processed_data.append(intermediate_data)\n",
        "  for i in labels:\n",
        "    for j in i:\n",
        "      processed_label.append(j)\n",
        "  return processed_data,processed_label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKUpRX1cQw1y"
      },
      "source": [
        "processed_data,processed_label = preprocessing(data,labels)\n",
        "test_processed_data,test_processed_label = preprocessing(test_data,test_labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DcF9IPt-2EQ"
      },
      "source": [
        "for i in range(0,len(processed_data)):\n",
        "  processed_data[i]= ' '.join(processed_data[i])\n",
        "for i in range(0,len(test_processed_data)):\n",
        "  test_processed_data[i]=' '.join(test_processed_data[i])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf4C0QIsRPiS"
      },
      "source": [
        "processed_data=np.asarray(processed_data)\n",
        "test_processed_data=np.asarray(test_processed_data)\n",
        "Y=to_categorical(processed_label, num_classes=6)\n",
        "test_Y=to_categorical(test_processed_label, num_classes=6)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiTdbVgdRTAi"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8sAlpdtRW6n"
      },
      "source": [
        "t0 = time.time()\n",
        "train_input = bert_encode(processed_data, tokenizer, max_len=160)\n",
        "test_input = bert_encode(test_processed_data, tokenizer, max_len=160)\n",
        "train_labels = Y\n",
        "test_labels = test_Y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYSK8AjpRYPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63d672d-f521-4887-e2d1-b4db4b726c02"
      },
      "source": [
        "model = build_model(bert_layer, max_len=161)\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 161)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 161)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 161)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape (TensorFlow [(1, 1024, 1)]       0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (1, 925, 128)        12928       tf_op_layer_Reshape[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (1, 836, 128)        1474688     conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (1, 278, 128)        0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (1, 35584)           0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (1, 100)             3558500     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (1, 100)             10100       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (1, 100)             10100       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (1, 6)               606         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 340,208,811\n",
            "Trainable params: 340,208,810\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99O-dUk_Ra5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836b3784-3852-4a57-f90e-477c07d820dd"
      },
      "source": [
        "train_history = model.fit(\n",
        "    train_input, train_labels,\n",
        "    epochs=3,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_word_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_word_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_mask:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_mask:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"segment_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"segment_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_word_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_word_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_mask:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"input_mask:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"segment_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 161) for input Tensor(\"segment_ids:0\", shape=(None, 161), dtype=int32), but it was called on an input with incompatible shape (1, 160).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 236/4699 [>.............................] - ETA: 12:34 - loss: 1.7774 - accuracy: 0.2669"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1yXEXJpRcdF"
      },
      "source": [
        "y_pred=model.predict(test_input, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifq6GS_ylKXA"
      },
      "source": [
        "y_pred1=np.argmax(y_pred,axis=1)\n",
        "test_processed_label=np.asarray(test_processed_label)\n",
        "t1 = time.time()\n",
        "total = t1-t0\n",
        "print(\"Total Execution time (Training + Testing): \" + str(total))\n",
        "print(\"Accuracy : \" + str(accuracy_score(test_processed_label, y_pred1)))\n",
        "print(\"Weighted F1-score : \" + str(f1_score(test_processed_label, y_pred1, average='weighted')))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}